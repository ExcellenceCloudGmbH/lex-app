2024-11-29 13:46:56.318 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=0
2024-11-29 13:46:56.319 | DEBUG    | metagpt.roles.role:_observe:443 - David(DataInterpreter) observed: ['user: \n    style:\n    {\n  ...']
2024-11-29 13:46:56.319 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=0
2024-11-29 13:46:56.320 | DEBUG    | metagpt.roles.role:_react:474 - David(DataInterpreter): self.rc.state=0, will do WriteAnalysisCode
2024-11-29 13:46:56.326 | INFO     | metagpt.tools.tool_recommend:recall_tools:194 - Recalled tools: 
['lex_app_setup', 'scrape_web_playwright', 'OneHotEncode', 'TargetMeanEncoder', 'KFoldTargetMeanEncoder', 'GPTvGenerator', 'RobustScale', 'email_login_imap', 'MinMaxScale', 'StandardScale', 'GeneralSelection', 'CatCount', 'SplitBins', 'SDEngine', 'PolynomialExpansion', 'VarianceBasedSelection', 'FillMissingValue', 'CatCross', 'LabelEncode', 'GroupStat']; Scores: [31.3294, 16.7347, 16.1477, 13.403, 13.0201, 9.335, 9.0479, 8.7756, 7.8372, 7.483, 7.1347, 6.0755, 5.4092, 4.7279, 4.0137, 3.9754, 3.7634, 2.1085, 1.5426, 0.3836]
2024-11-29 13:46:56.385 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '\n## User Requirement:\n\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    \n\n## Task\nRecommend up to 5 tools from \'Available Tools\' that can help solve the \'User Requirement\'. \n\n## Available Tools:\n{\'lex_app_setup\': \'Initializes the Django environment and sets up necessary paths for the project. This function performs the following tasks: 1. Defines the paths to the application and project root directories. - `LEX_APP_PACKAGE_ROOT`: The parent directory of the `lex_app` package. - `PROJECT_ROOT_DIR`: The current working directory of the project. 2. Adds the `LEX_APP_PACKAGE_ROOT` to the system path (`sys.path`) to ensure that modules from the parent package are accessible. 3. Sets the required environment variables: - `DJANGO_SETTINGS_MODULE`: Specifies the settings module for the Django project. - `PROJECT_ROOT`: The root directory of the project. - `LEX_APP_PACKAGE_ROOT`: The root directory of the application package. 4. Calls `django.setup()` to initialize the Django framework, making it possible to use Django ORM, models, and other features. Note: This script must be run in a Django-compatible environment, and the Django settings file must be properly configured.\', \'scrape_web_playwright\': \'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \', \'OneHotEncode\': \'Apply one-hot encoding to specified categorical columns, the original columns will be dropped.\', \'TargetMeanEncoder\': \'Encode a categorical column by the mean of the label column, and adds the result as a new feature.\', \'KFoldTargetMeanEncoder\': \'Add a new feature to the DataFrame by k-fold mean encoding of a categorical column using the label column.\', \'GPTvGenerator\': \'Class for generating webpage code from a given webpage screenshot. This class provides methods to generate webpages including all code (HTML, CSS, and JavaScript) based on an image. It utilizes a vision model to analyze the layout from an image and generate webpage codes accordingly.\', \'RobustScale\': \'Apply the RobustScaler to scale features using statistics that are robust to outliers.\', \'email_login_imap\': \'Use imap_tools package to log in to your email (the email that supports IMAP protocol) to verify and return the account object. \', \'MinMaxScale\': \'Transform features by scaling each feature to a range, which is (0, 1).\', \'StandardScale\': \'Standardize features by removing the mean and scaling to unit variance.\', \'GeneralSelection\': \'Drop all nan feats and feats with only one unique value.\', \'CatCount\': \'Add value counts of a categorical column as new feature.\', \'SplitBins\': \'Inplace binning of continuous data into intervals, returning integer-encoded bin identifiers directly.\', \'SDEngine\': \'Generate image using stable diffusion model. This class provides methods to interact with a stable diffusion service to generate images based on text inputs.\', \'PolynomialExpansion\': \'Add polynomial and interaction features from selected numeric columns to input DataFrame.\', \'VarianceBasedSelection\': \'Select features based on variance and remove features with low variance.\', \'FillMissingValue\': \'Completing missing values with simple strategies.\', \'CatCross\': \'Add pairwise crossed features and convert them to numerical features.\', \'LabelEncode\': \'Apply label encoding to specified categorical columns in-place.\', \'GroupStat\': "Aggregate specified column in a DataFrame grouped by another column, adding new features named \'<agg_col>_<agg_func>_by_<group_col>\'."}\n\n## Tool Selection and Instructions:\n- Select tools most relevant to completing the \'User Requirement\'.\n- If you believe that no tools are suitable, indicate with an empty list.\n- Only list the names of the tools, not the full schema of each tool.\n- Ensure selected tools are listed in \'Available Tools\'.\n- Output a json list of tool names:\n```json\n["tool_name1", "tool_name2", ...]\n```\n'}]
2024-11-29 13:46:57.688 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:00.785 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:00.786 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 1489, completion_tokens: 22
2024-11-29 13:47:00.786 | INFO     | metagpt.tools.tool_recommend:recommend_tools:100 - Recommended tools: 
['lex_app_setup', 'FillMissingValue', 'GroupStat', 'LabelEncode', 'GeneralSelection']
2024-11-29 13:47:00.786 | INFO     | metagpt.roles.di.data_interpreter:_write_code:155 - ready to WriteAnalysisCode
2024-11-29 13:47:00.787 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': "As a data scientist, you need to help user to achieve their goal step by step in a continuous Jupyter notebook. Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function."}, {'role': 'user', 'content': '\n# User Requirement\n\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    \n\n# Plan Status\n\n\n# Tool Info\n\n## Capabilities\n- You can utilize pre-defined tools in any code lines from \'Available Tools\' in the form of Python class or function.\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n\n## Available Tools:\nEach tool is described in JSON format. When you call a tool, import the tool from its path first.\n{\'lex_app_setup\': {\'type\': \'function\', \'description\': \'Initializes the Django environment and sets up necessary paths for the project. This function performs the following tasks: 1. Defines the paths to the application and project root directories. - `LEX_APP_PACKAGE_ROOT`: The parent directory of the `lex_app` package. - `PROJECT_ROOT_DIR`: The current working directory of the project. 2. Adds the `LEX_APP_PACKAGE_ROOT` to the system path (`sys.path`) to ensure that modules from the parent package are accessible. 3. Sets the required environment variables: - `DJANGO_SETTINGS_MODULE`: Specifies the settings module for the Django project. - `PROJECT_ROOT`: The root directory of the project. - `LEX_APP_PACKAGE_ROOT`: The root directory of the application package. 4. Calls `django.setup()` to initialize the Django framework, making it possible to use Django ORM, models, and other features. Note: This script must be run in a Django-compatible environment, and the Django settings file must be properly configured.\', \'signature\': \'()\', \'parameters\': \'\', \'tool_path\': \'metagpt/tools/libs/__init__.py\'}, \'FillMissingValue\': {\'type\': \'class\', \'description\': \'Completing missing values with simple strategies.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': \'(self, features: \\\'list\\\', strategy: "Literal[\\\'mean\\\', \\\'median\\\', \\\'most_frequent\\\', \\\'constant\\\']" = \\\'mean\\\', fill_value=None)\', \'parameters\': \'Args: features (list): Columns to be processed. strategy (Literal["mean", "median", "most_frequent", "constant"], optional): The imputation strategy, notice \\\'mean\\\' and \\\'median\\\' can only be used for numeric features. Defaults to \\\'mean\\\'. fill_value (int, optional): Fill_value is used to replace all occurrences of missing_values. Defaults to None.\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/data_preprocess.py\'}, \'GroupStat\': {\'type\': \'class\', \'description\': "Aggregate specified column in a DataFrame grouped by another column, adding new features named \'<agg_col>_<agg_func>_by_<group_col>\'.", \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': "(self, group_col: \'str\', agg_col: \'str\', agg_funcs: \'list\')", \'parameters\': "Args: group_col (str): Column used for grouping. agg_col (str): Column on which aggregation is performed. agg_funcs (list): List of aggregation functions to apply, such as [\'mean\', \'std\']. Each function must be supported by pandas."}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/feature_engineering.py\'}, \'LabelEncode\': {\'type\': \'class\', \'description\': \'Apply label encoding to specified categorical columns in-place.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': "(self, features: \'list\')", \'parameters\': \'Args: features (list): Categorical columns to be label encoded.\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/data_preprocess.py\'}, \'GeneralSelection\': {\'type\': \'class\', \'description\': \'Drop all nan feats and feats with only one unique value.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. See help(type(self)) for accurate signature.\', \'signature\': "(self, label_col: \'str\')", \'parameters\': \'\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/feature_engineering.py\'}}\n\n\n# Constraints\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n- Always prioritize using pre-defined tools for the same functionality.\n\n# Output\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n```python\nyour code\n```\n'}, {'role': 'user', 'content': '\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    '}]
2024-11-29 13:47:09.761 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:09.764 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:09.765 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.026 | Max budget: $10.000 | Current cost: $0.026, prompt_tokens: 3096, completion_tokens: 707
2024-11-29 13:47:14.850 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=-1
2024-11-29 13:47:14.851 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n   Given the projec...']
2024-11-29 13:47:14.851 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:47:14.851 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:47:14.851 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n   Given the project overview and the files with explanations:\n\n[START INSTRUCTIONS]\n\n1. Use the project overview and files provided as a requirements specification for drafting an object design.\n2. Identify and represent relationships between entity classes based on the project description and file content, treating each input file as a potential entity model.\n3. **Incorporate Foreign Key relationships** wherever logical dependencies exist between classes. Analyze each entity to identify connections between them, and create Foreign Key fields to represent these relationships, ensuring a normalized database schema.\n4. Treat input files as sources for entity classes, which hold data and should be structured as models. **Extract these models and their relationships based on the project description, file content, and explanations.**\n5. In every upload model, there must be a mandatory file field, and in every report model, there must be an OPTIONAL file field.\n6. Create models to handle data uploads for each corresponding entity class. Ensure that these upload models can process data according to the columns and types found in the input files.\n7. **Analyze the columns in each input file. Create a unique input model for each file that has a unique set of columns. Only if multiple files have an identical set of columns should they be grouped into a shared model.**\n8. Represent the file structure in JSON format to indicate where each entity class will reside in the project directory.\n9. Separate models logically into main models, upload models, and report models to reflect the structure and purpose of each class.\n10. For report models, draw inspiration from the output files and define models or logic that consolidate data from input models to generate reports.\n11. **Translate any non-English terms to English** for all model and field names. Use standardized software engineering conventions for names, ensuring consistency.\n12. Use capital case for Model and Folder names (e.g., `ModelClass`, `Folder1`) and camel case for field names (e.g., `fieldName`).\n13. Choose fields based on file content, and ensure all model names and field names are in English.\n14. Provide descriptions for each class to clarify its purpose.\n15. Return a simple, parsable JSON object.\n16. Normalize the database schema by minimizing redundancy and using Foreign Key relationships wherever logical dependencies exist between classes.\n17. Examine the columns and data in each file context to inform model structures, **ensuring a distinct input model for each unique set of columns, grouping only identical structures in shared models**.\n18. Ensure report models can use data from input models to generate consolidated reports.\n19. **Return only the JSON object that encapsulates these requirements.**\n20. Exclude ```json tags; write the JSON object directly.\n21. Example folder names: ("Inputs", "Reports", "Uploads,.. etc this is just an example it could be different) CapitalCase, no spaces, no underscores.\n22. Report class should be able to generate the same output as the output files provided in the file context (Don\'t add fields that are not needed in the Reports).\n\n[END INSTRUCTIONS]\n\nContext:\n\nProject Overview:\n"The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\\n\\n"\n\nFiles with Explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback. \n    '}]
2024-11-29 13:47:22.150 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:22.151 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:22.152 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.020 | Max budget: $10.000 | Current cost: $0.020, prompt_tokens: 1909, completion_tokens: 700
2024-11-29 13:47:22.152 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:47:22.153 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        Transform t...']
2024-11-29 13:47:22.153 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:47:22.153 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:47:22.154 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        Transform the json given to the same structure as the given json format:\n        1. Don\'t include project details and overview, only include files and folders, classes and explanations.\n\n        Given Json:\n        {\n  "Inputs": {\n    "Windparks": {\n      "description": "Windparks data",\n      "fields": {\n        "windpark": "string",\n        "windparkOperator": "string",\n        "generationMioKwhPerYear": "float"\n      }\n    },\n    "WindparkOperators": {\n      "description": "Windpark Operators data",\n      "fields": {\n        "windparkOperator": "string",\n        "numberOfWindparks": "integer",\n        "year": "integer",\n        "investmentMMEur": "float",\n        "employees": "integer",\n        "areaM2": "float"\n      }\n    },\n    "USCashflows": {\n      "description": "US Cashflows data",\n      "fields": {\n        "id": "integer",\n        "windparkOperator": "string",\n        "year": "integer",\n        "cashflow": "float"\n      }\n    },\n    "ChinaCashflows": {\n      "description": "China Cashflows data",\n      "fields": {\n        "id": "integer",\n        "windparkOperator": "string",\n        "windparkName": "string",\n        "date": "datetime",\n        "year": "integer",\n        "cashflow": "float"\n      }\n    },\n    "GermanyCashflows": {\n      "description": "Germany Cashflows data",\n      "fields": {\n        "id": "integer",\n        "windparkOperator": "string",\n        "windpark": "string",\n        "year": "integer",\n        "quarter": "string",\n        "cashflow": "float"\n      }\n    }\n  },\n  "Uploads": {\n    "WindparksUpload": {\n      "description": "Upload model for Windparks data",\n      "fields": {\n        "file": "file"\n      }\n    },\n    "WindparkOperatorsUpload": {\n      "description": "Upload model for Windpark Operators data",\n      "fields": {\n        "file": "file"\n      }\n    },\n    "USCashflowsUpload": {\n      "description": "Upload model for US Cashflows data",\n      "fields": {\n        "file": "file"\n      }\n    },\n    "ChinaCashflowsUpload": {\n      "description": "Upload model for China Cashflows data",\n      "fields": {\n        "file": "file"\n      }\n    },\n    "GermanyCashflowsUpload": {\n      "description": "Upload model for Germany Cashflows data",\n      "fields": {\n        "file": "file"\n      }\n    }\n  },\n  "Reports": {\n    "CashflowReport": {\n      "description": "Consolidated Cashflow Report",\n      "fields": {\n        "windparkOperator": "string",\n        "cashflowSum": "float",\n        "file": "file (optional)"\n      }\n    }\n  },\n  "Relationships": {\n    "Windparks": {\n      "windparkOperator": "WindparkOperators.windparkOperator"\n    },\n    "USCashflows": {\n      "windparkOperator": "WindparkOperators.windparkOperator"\n    },\n    "ChinaCashflows": {\n      "windparkOperator": "WindparkOperators.windparkOperator"\n    },\n    "GermanyCashflows": {\n      "windparkOperator": "WindparkOperators.windparkOperator"\n    }\n  }\n}\n\n        Specified Format:\n        \n        {\n          "Folder1": {\n            "class1": "Explanation 1"\n            "class2": "Explanation 2"\n          },\n          "Folder2": {\n            "class3": "Explanation 3"\n            "class4": "Explanation 4"\n          }\n        } \n        \n\n\n        **Outputs**:\n            output only the json without ```json:\n\n        json =  \n        '}]
2024-11-29 13:47:24.812 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:24.815 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:47:24.822 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 863, completion_tokens: 242
2024-11-29 13:47:24.822 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:47:53.504 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \nFile content and th...']
2024-11-29 13:47:53.504 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:47:53.505 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:47:53.505 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\nFile content and their explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nProject Structure:\n{\'Inputs\': {\'Windparks\': {\'fields\': {\'windpark\': \'string\', \'windparkOperator\': \'string\', \'generationMioKwhPerYear\': \'float\'}, \'description\': \'Windparks data\'}, \'USCashflows\': {\'fields\': {\'id\': \'integer\', \'year\': \'integer\', \'cashflow\': \'float\', \'windparkOperator\': \'string\'}, \'description\': \'US Cashflows data\'}, \'ChinaCashflows\': {\'fields\': {\'id\': \'integer\', \'date\': \'datetime\', \'year\': \'integer\', \'cashflow\': \'float\', \'windparkName\': \'string\', \'windparkOperator\': \'string\'}, \'description\': \'China Cashflows data\'}, \'GermanyCashflows\': {\'fields\': {\'id\': \'integer\', \'year\': \'integer\', \'quarter\': \'string\', \'cashflow\': \'float\', \'windpark\': \'string\', \'windparkOperator\': \'string\'}, \'description\': \'Germany Cashflows data\'}, \'WindparkOperators\': {\'fields\': {\'year\': \'integer\', \'areaM2\': \'float\', \'employees\': \'integer\', \'investmentMMEur\': \'float\', \'windparkOperator\': \'string\', \'numberOfWindparks\': \'integer\'}, \'description\': \'Windpark Operators data\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'file (optional)\', \'cashflowSum\': \'float\', \'windparkOperator\': \'string\'}, \'description\': \'Consolidated Cashflow Report\'}}, \'Uploads\': {\'WindparksUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Windparks data\'}, \'USCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for US Cashflows data\'}, \'ChinaCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for China Cashflows data\'}, \'GermanyCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Germany Cashflows data\'}, \'WindparkOperatorsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Windpark Operators data\'}}, \'Relationships\': {\'Windparks\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'USCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'ChinaCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'GermanyCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}}}\n\nCurrent Project Functionalities:\nThis is the first time for this query, there is no project functionality.\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback.\n\n[START INSTRUCTIONS]\n\nGiven the project structure, summarize the main functionalities of the project, listing each model with a high-level description of its role and key interactions.\n1. Provide a basic pseudocode outline for each model’s calculate method (Upload and Report Model basically), mentioning:\n    - Primary steps for data processing, report generation, and any conditional logic.\n    - Any logging actions.\n    - Simplified placeholders for complex calculations, with notes on intended outcomes.\n    - Avoid specific implementation details; instead, focus on the sequence and purpose of steps in each model.\n2. Present this information in markdown format without code snippets. \n\n[STOP INSTRUCTIONS]\nStart of markdown:\n    '}]
2024-11-29 13:48:05.657 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:48:05.660 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:48:05.662 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.025 | Max budget: $10.000 | Current cost: $0.025, prompt_tokens: 1895, completion_tokens: 1014
2024-11-29 13:48:05.663 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:48:05.859 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \nFile content and th...']
2024-11-29 13:48:05.859 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:48:05.860 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:48:05.860 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\nFile content and their explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nProject Structure:\n{\'Inputs\': {\'Windparks\': {\'fields\': {\'windpark\': \'string\', \'windparkOperator\': \'string\', \'generationMioKwhPerYear\': \'float\'}, \'description\': \'Windparks data\'}, \'USCashflows\': {\'fields\': {\'id\': \'integer\', \'year\': \'integer\', \'cashflow\': \'float\', \'windparkOperator\': \'string\'}, \'description\': \'US Cashflows data\'}, \'ChinaCashflows\': {\'fields\': {\'id\': \'integer\', \'date\': \'datetime\', \'year\': \'integer\', \'cashflow\': \'float\', \'windparkName\': \'string\', \'windparkOperator\': \'string\'}, \'description\': \'China Cashflows data\'}, \'GermanyCashflows\': {\'fields\': {\'id\': \'integer\', \'year\': \'integer\', \'quarter\': \'string\', \'cashflow\': \'float\', \'windpark\': \'string\', \'windparkOperator\': \'string\'}, \'description\': \'Germany Cashflows data\'}, \'WindparkOperators\': {\'fields\': {\'year\': \'integer\', \'areaM2\': \'float\', \'employees\': \'integer\', \'investmentMMEur\': \'float\', \'windparkOperator\': \'string\', \'numberOfWindparks\': \'integer\'}, \'description\': \'Windpark Operators data\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'file (optional)\', \'cashflowSum\': \'float\', \'windparkOperator\': \'string\'}, \'description\': \'Consolidated Cashflow Report\'}}, \'Uploads\': {\'WindparksUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Windparks data\'}, \'USCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for US Cashflows data\'}, \'ChinaCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for China Cashflows data\'}, \'GermanyCashflowsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Germany Cashflows data\'}, \'WindparkOperatorsUpload\': {\'fields\': {\'file\': \'file\'}, \'description\': \'Upload model for Windpark Operators data\'}}, \'Relationships\': {\'Windparks\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'USCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'ChinaCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}, \'GermanyCashflows\': {\'windparkOperator\': \'WindparkOperators.windparkOperator\'}}}\n\nCurrent Project Functionalities:\nThis is the first time for this query, there is no project functionality.\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback.\n\n[START INSTRUCTIONS]\n\nGiven the project structure, summarize the main functionalities of the project, listing each model with a high-level description of its role and key interactions.\n1. Provide a basic pseudocode outline for each model’s calculate method (Upload and Report Model basically), mentioning:\n    - Primary steps for data processing, report generation, and any conditional logic.\n    - Any logging actions.\n    - Simplified placeholders for complex calculations, with notes on intended outcomes.\n    - Avoid specific implementation details; instead, focus on the sequence and purpose of steps in each model.\n2. Present this information in markdown format without code snippets. \n\n[STOP INSTRUCTIONS]\nStart of markdown:\n    '}]
2024-11-29 13:48:19.096 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:48:19.098 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:48:19.099 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.024 | Max budget: $10.000 | Current cost: $0.024, prompt_tokens: 1895, completion_tokens: 948
2024-11-29 13:48:19.100 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:49:20.779 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=0
2024-11-29 13:49:20.780 | DEBUG    | metagpt.roles.role:_observe:443 - David(DataInterpreter) observed: ['user: \n    style:\n    {\n  ...']
2024-11-29 13:49:20.780 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=0
2024-11-29 13:49:20.780 | DEBUG    | metagpt.roles.role:_react:474 - David(DataInterpreter): self.rc.state=0, will do WriteAnalysisCode
2024-11-29 13:49:20.783 | INFO     | metagpt.tools.tool_recommend:recall_tools:194 - Recalled tools: 
['lex_app_setup', 'scrape_web_playwright', 'OneHotEncode', 'TargetMeanEncoder', 'KFoldTargetMeanEncoder', 'GPTvGenerator', 'RobustScale', 'email_login_imap', 'MinMaxScale', 'StandardScale', 'GeneralSelection', 'CatCount', 'SplitBins', 'SDEngine', 'PolynomialExpansion', 'VarianceBasedSelection', 'FillMissingValue', 'CatCross', 'LabelEncode', 'GroupStat']; Scores: [31.3294, 16.7347, 16.1477, 13.403, 13.0201, 9.335, 9.0479, 8.7756, 7.8372, 7.483, 7.1347, 6.0755, 5.4092, 4.7279, 4.0137, 3.9754, 3.7634, 2.1085, 1.5426, 0.3836]
2024-11-29 13:49:20.826 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '\n## User Requirement:\n\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    \n\n## Task\nRecommend up to 5 tools from \'Available Tools\' that can help solve the \'User Requirement\'. \n\n## Available Tools:\n{\'lex_app_setup\': \'Initializes the Django environment and sets up necessary paths for the project. This function performs the following tasks: 1. Defines the paths to the application and project root directories. - `LEX_APP_PACKAGE_ROOT`: The parent directory of the `lex_app` package. - `PROJECT_ROOT_DIR`: The current working directory of the project. 2. Adds the `LEX_APP_PACKAGE_ROOT` to the system path (`sys.path`) to ensure that modules from the parent package are accessible. 3. Sets the required environment variables: - `DJANGO_SETTINGS_MODULE`: Specifies the settings module for the Django project. - `PROJECT_ROOT`: The root directory of the project. - `LEX_APP_PACKAGE_ROOT`: The root directory of the application package. 4. Calls `django.setup()` to initialize the Django framework, making it possible to use Django ORM, models, and other features. Note: This script must be run in a Django-compatible environment, and the Django settings file must be properly configured.\', \'scrape_web_playwright\': \'Asynchronously Scrape and save the HTML structure and inner text content of a web page using Playwright. \', \'OneHotEncode\': \'Apply one-hot encoding to specified categorical columns, the original columns will be dropped.\', \'TargetMeanEncoder\': \'Encode a categorical column by the mean of the label column, and adds the result as a new feature.\', \'KFoldTargetMeanEncoder\': \'Add a new feature to the DataFrame by k-fold mean encoding of a categorical column using the label column.\', \'GPTvGenerator\': \'Class for generating webpage code from a given webpage screenshot. This class provides methods to generate webpages including all code (HTML, CSS, and JavaScript) based on an image. It utilizes a vision model to analyze the layout from an image and generate webpage codes accordingly.\', \'RobustScale\': \'Apply the RobustScaler to scale features using statistics that are robust to outliers.\', \'email_login_imap\': \'Use imap_tools package to log in to your email (the email that supports IMAP protocol) to verify and return the account object. \', \'MinMaxScale\': \'Transform features by scaling each feature to a range, which is (0, 1).\', \'StandardScale\': \'Standardize features by removing the mean and scaling to unit variance.\', \'GeneralSelection\': \'Drop all nan feats and feats with only one unique value.\', \'CatCount\': \'Add value counts of a categorical column as new feature.\', \'SplitBins\': \'Inplace binning of continuous data into intervals, returning integer-encoded bin identifiers directly.\', \'SDEngine\': \'Generate image using stable diffusion model. This class provides methods to interact with a stable diffusion service to generate images based on text inputs.\', \'PolynomialExpansion\': \'Add polynomial and interaction features from selected numeric columns to input DataFrame.\', \'VarianceBasedSelection\': \'Select features based on variance and remove features with low variance.\', \'FillMissingValue\': \'Completing missing values with simple strategies.\', \'CatCross\': \'Add pairwise crossed features and convert them to numerical features.\', \'LabelEncode\': \'Apply label encoding to specified categorical columns in-place.\', \'GroupStat\': "Aggregate specified column in a DataFrame grouped by another column, adding new features named \'<agg_col>_<agg_func>_by_<group_col>\'."}\n\n## Tool Selection and Instructions:\n- Select tools most relevant to completing the \'User Requirement\'.\n- If you believe that no tools are suitable, indicate with an empty list.\n- Only list the names of the tools, not the full schema of each tool.\n- Ensure selected tools are listed in \'Available Tools\'.\n- Output a json list of tool names:\n```json\n["tool_name1", "tool_name2", ...]\n```\n'}]
2024-11-29 13:49:22.068 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:22.495 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:22.495 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 1489, completion_tokens: 22
2024-11-29 13:49:22.496 | INFO     | metagpt.tools.tool_recommend:recommend_tools:100 - Recommended tools: 
['lex_app_setup', 'FillMissingValue', 'GroupStat', 'LabelEncode', 'GeneralSelection']
2024-11-29 13:49:22.496 | INFO     | metagpt.roles.di.data_interpreter:_write_code:155 - ready to WriteAnalysisCode
2024-11-29 13:49:22.496 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': "As a data scientist, you need to help user to achieve their goal step by step in a continuous Jupyter notebook. Since it is a notebook environment, don't use asyncio.run. Instead, use await if you need to call an async function."}, {'role': 'user', 'content': '\n# User Requirement\n\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    \n\n# Plan Status\n\n\n# Tool Info\n\n## Capabilities\n- You can utilize pre-defined tools in any code lines from \'Available Tools\' in the form of Python class or function.\n- You can freely combine the use of any other public packages, like sklearn, numpy, pandas, etc..\n\n## Available Tools:\nEach tool is described in JSON format. When you call a tool, import the tool from its path first.\n{\'lex_app_setup\': {\'type\': \'function\', \'description\': \'Initializes the Django environment and sets up necessary paths for the project. This function performs the following tasks: 1. Defines the paths to the application and project root directories. - `LEX_APP_PACKAGE_ROOT`: The parent directory of the `lex_app` package. - `PROJECT_ROOT_DIR`: The current working directory of the project. 2. Adds the `LEX_APP_PACKAGE_ROOT` to the system path (`sys.path`) to ensure that modules from the parent package are accessible. 3. Sets the required environment variables: - `DJANGO_SETTINGS_MODULE`: Specifies the settings module for the Django project. - `PROJECT_ROOT`: The root directory of the project. - `LEX_APP_PACKAGE_ROOT`: The root directory of the application package. 4. Calls `django.setup()` to initialize the Django framework, making it possible to use Django ORM, models, and other features. Note: This script must be run in a Django-compatible environment, and the Django settings file must be properly configured.\', \'signature\': \'()\', \'parameters\': \'\', \'tool_path\': \'metagpt/tools/libs/__init__.py\'}, \'FillMissingValue\': {\'type\': \'class\', \'description\': \'Completing missing values with simple strategies.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': \'(self, features: \\\'list\\\', strategy: "Literal[\\\'mean\\\', \\\'median\\\', \\\'most_frequent\\\', \\\'constant\\\']" = \\\'mean\\\', fill_value=None)\', \'parameters\': \'Args: features (list): Columns to be processed. strategy (Literal["mean", "median", "most_frequent", "constant"], optional): The imputation strategy, notice \\\'mean\\\' and \\\'median\\\' can only be used for numeric features. Defaults to \\\'mean\\\'. fill_value (int, optional): Fill_value is used to replace all occurrences of missing_values. Defaults to None.\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/data_preprocess.py\'}, \'GroupStat\': {\'type\': \'class\', \'description\': "Aggregate specified column in a DataFrame grouped by another column, adding new features named \'<agg_col>_<agg_func>_by_<group_col>\'.", \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': "(self, group_col: \'str\', agg_col: \'str\', agg_funcs: \'list\')", \'parameters\': "Args: group_col (str): Column used for grouping. agg_col (str): Column on which aggregation is performed. agg_funcs (list): List of aggregation functions to apply, such as [\'mean\', \'std\']. Each function must be supported by pandas."}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/feature_engineering.py\'}, \'LabelEncode\': {\'type\': \'class\', \'description\': \'Apply label encoding to specified categorical columns in-place.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. \', \'signature\': "(self, features: \'list\')", \'parameters\': \'Args: features (list): Categorical columns to be label encoded.\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/data_preprocess.py\'}, \'GeneralSelection\': {\'type\': \'class\', \'description\': \'Drop all nan feats and feats with only one unique value.\', \'methods\': {\'__init__\': {\'type\': \'function\', \'description\': \'Initialize self. See help(type(self)) for accurate signature.\', \'signature\': "(self, label_col: \'str\')", \'parameters\': \'\'}, \'fit\': {\'type\': \'function\', \'description\': \'Fit a model to be used in subsequent transform. \', \'signature\': "(self, df: \'pd.DataFrame\')", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame.\'}, \'fit_transform\': {\'type\': \'function\', \'description\': \'Fit and transform the input DataFrame. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}, \'transform\': {\'type\': \'function\', \'description\': \'Transform the input DataFrame with the fitted model. \', \'signature\': "(self, df: \'pd.DataFrame\') -> \'pd.DataFrame\'", \'parameters\': \'Args: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame.\'}}, \'tool_path\': \'metagpt/tools/libs/feature_engineering.py\'}}\n\n\n# Constraints\n- Take on Current Task if it is in Plan Status, otherwise, tackle User Requirement directly.\n- Ensure the output new code is executable in the same Jupyter notebook as the previous executed code.\n- Always prioritize using pre-defined tools for the same functionality.\n\n# Output\nWhile some concise thoughts are helpful, code is absolutely required. Always output one and only one code block in your response. Output code in the following format:\n```python\nyour code\n```\n'}, {'role': 'user', 'content': '\n    style:\n    {\n        "components": [\n            {\n                "name": "",\n                "type": "",\n                "explanation": "",\n                "path": "",\n                "columns": [\n                ],\n                "data_samples": [\n                    {\n                    },\n                    {\n                    },\n                    {\n                    },\n                ],\n                row_count: 0\n            }\n        ]\n    }\n    \n    \n    Files:\n    [{\'name\': \'input_files/Windparks.xlsx\', \'explanation\': \'Windparks\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx\'}, {\'name\': \'input_files/Windparkbetreiber_Übersicht.xlsx\', \'explanation\': \'Windpark Operators\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_Übersicht.xlsx\'}, {\'name\': \'input_files/US_CF_Report_20-22.xlsx\', \'explanation\': \'US Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/C_CF_Report_20-22.xlsx\', \'explanation\': \'China Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx\'}, {\'name\': \'input_files/DE_CF_Report_20-22.xlsx\', \'explanation\': \'Germany Cashflows\', \'type\': \'Input\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx\'}, {\'name\': \'output_files/Cashflow_Report.xlsx\', \'explanation\': \'Cashflow Report Example\', \'type\': \'Output\', \'path\': \'/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx\'}]\n    \n    Given the files with explanations, analyze the content of the files and understand the components of the project and their relationships. \n    [START INSTRUCTIONS]\n    1. Open the file and get the content\n    2. Transform the every Timestamp to the form or format of YYYY-MM-DD HH:MM:SS or None\n    3. Timestamp should be parsable from a json prespective\n    4. Name should be without file extensions\n    6. output of the json should be similar to the `style` defined above but without whitespaces\n    7. The json should have no whitespaces\n    8. Output no whitespace or any null bytes, only the json with no \'\'\' json\n    9. The json output is kind of large, somtimes the output would be cut off, so do something about that\n    [STOP INSTRUCTIONS]\n    \n    [START OUTPUTING JSON] \n    '}]
2024-11-29 13:49:32.394 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:32.398 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:32.399 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.026 | Max budget: $10.000 | Current cost: $0.026, prompt_tokens: 3096, completion_tokens: 686
2024-11-29 13:49:37.190 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[WriteAnalysisCode], state=-1
2024-11-29 13:49:37.190 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n   Given the projec...']
2024-11-29 13:49:37.191 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:49:37.191 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:49:37.191 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n   Given the project overview and the files with explanations:\n\n[START INSTRUCTIONS]\n\n1. Use the project overview and files provided as a requirements specification for drafting an object design.\n2. Identify and represent relationships between entity classes based on the project description and file content, treating each input file as a potential entity model.\n3. **Incorporate Foreign Key relationships** wherever logical dependencies exist between classes. Analyze each entity to identify connections between them, and create Foreign Key fields to represent these relationships, ensuring a normalized database schema.\n4. Treat input files as sources for entity classes, which hold data and should be structured as models. **Extract these models and their relationships based on the project description, file content, and explanations.**\n5. In every upload model, there must be a mandatory file field, and in every report model, there must be an OPTIONAL file field.\n6. Create models to handle data uploads for each corresponding entity class. Ensure that these upload models can process data according to the columns and types found in the input files.\n7. **Analyze the columns in each input file. Create a unique input model for each file that has a unique set of columns. Only if multiple files have an identical set of columns should they be grouped into a shared model.**\n8. Represent the file structure in JSON format to indicate where each entity class will reside in the project directory.\n9. Separate models logically into main models, upload models, and report models to reflect the structure and purpose of each class.\n10. For report models, draw inspiration from the output files and define models or logic that consolidate data from input models to generate reports.\n11. **Translate any non-English terms to English** for all model and field names. Use standardized software engineering conventions for names, ensuring consistency.\n12. Use capital case for Model and Folder names (e.g., `ModelClass`, `Folder1`) and camel case for field names (e.g., `fieldName`).\n13. Choose fields based on file content, and ensure all model names and field names are in English.\n14. Provide descriptions for each class to clarify its purpose.\n15. Return a simple, parsable JSON object.\n16. Normalize the database schema by minimizing redundancy and using Foreign Key relationships wherever logical dependencies exist between classes.\n17. Examine the columns and data in each file context to inform model structures, **ensuring a distinct input model for each unique set of columns, grouping only identical structures in shared models**.\n18. Ensure report models can use data from input models to generate consolidated reports.\n19. **Return only the JSON object that encapsulates these requirements.**\n20. Exclude ```json tags; write the JSON object directly.\n21. Example folder names: ("Inputs", "Reports", "Uploads,.. etc this is just an example it could be different) CapitalCase, no spaces, no underscores.\n22. Report class should be able to generate the same output as the output files provided in the file context (Don\'t add fields that are not needed in the Reports).\n23. Relationships should not be represented as classes but rather as fields\n\n[END INSTRUCTIONS]\n\nContext:\n\nProject Overview:\n"The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\\n\\n"\n\nFiles with Explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback. \n    '}]
2024-11-29 13:49:45.709 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:45.711 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:45.712 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.021 | Max budget: $10.000 | Current cost: $0.021, prompt_tokens: 1923, completion_tokens: 776
2024-11-29 13:49:45.712 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:49:45.713 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        Transform t...']
2024-11-29 13:49:45.713 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:49:45.714 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:49:45.715 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        Transform the json given to the same structure as the given json format:\n        1. Don\'t include project details and overview, only include files and folders, classes and explanations.\n\n        Given Json:\n        {\n  "Folders": {\n    "Inputs": {\n      "Windpark": {\n        "description": "Represents individual windparks and their annual energy production.",\n        "fields": {\n          "windpark": "String",\n          "windparkOperator": "String",\n          "productionMioKwhPerYear": "Float"\n        }\n      },\n      "WindparkOperatorOverview": {\n        "description": "Represents windpark operators, their investments, and other details.",\n        "fields": {\n          "windparkOperator": "String",\n          "numberOfWindparks": "Integer",\n          "year": "Integer",\n          "investmentMMEur": "Float",\n          "employees": "Integer",\n          "areaM2": "Float"\n        }\n      },\n      "USCashflowReport": {\n        "description": "Represents cashflow data for US windpark operators.",\n        "fields": {\n          "id": "Integer",\n          "windparkOperator": "String",\n          "year": "Integer",\n          "cashflow": "Float"\n        }\n      },\n      "ChinaCashflowReport": {\n        "description": "Represents cashflow data for Chinese windpark operators.",\n        "fields": {\n          "id": "Integer",\n          "windparkOperator": "String",\n          "windparkName": "String",\n          "date": "DateTime",\n          "year": "Integer",\n          "cashflow": "Float"\n        }\n      },\n      "GermanyCashflowReport": {\n        "description": "Represents cashflow data for German windpark operators.",\n        "fields": {\n          "id": "Integer",\n          "windparkOperator": "String",\n          "windpark": "String",\n          "year": "Integer",\n          "quarter": "String",\n          "cashflow": "Float"\n        }\n      }\n    },\n    "Uploads": {\n      "WindparkUpload": {\n        "description": "Handles data uploads for windparks.",\n        "fields": {\n          "file": "File"\n        }\n      },\n      "WindparkOperatorOverviewUpload": {\n        "description": "Handles data uploads for windpark operators.",\n        "fields": {\n          "file": "File"\n        }\n      },\n      "USCashflowReportUpload": {\n        "description": "Handles data uploads for US cashflow reports.",\n        "fields": {\n          "file": "File"\n        }\n      },\n      "ChinaCashflowReportUpload": {\n        "description": "Handles data uploads for China cashflow reports.",\n        "fields": {\n          "file": "File"\n        }\n      },\n      "GermanyCashflowReportUpload": {\n        "description": "Handles data uploads for Germany cashflow reports.",\n        "fields": {\n          "file": "File"\n        }\n      }\n    },\n    "Reports": {\n      "CashflowReport": {\n        "description": "Generates consolidated cashflow reports.",\n        "fields": {\n          "windparkOperator": "String",\n          "cashflowSum": "Float",\n          "file": "File (Optional)"\n        }\n      }\n    }\n  },\n  "Relationships": {\n    "Windpark": {\n      "windparkOperator": "ForeignKey(WindparkOperatorOverview.windparkOperator)"\n    },\n    "USCashflowReport": {\n      "windparkOperator": "ForeignKey(WindparkOperatorOverview.windparkOperator)"\n    },\n    "ChinaCashflowReport": {\n      "windparkOperator": "ForeignKey(WindparkOperatorOverview.windparkOperator)"\n    },\n    "GermanyCashflowReport": {\n      "windparkOperator": "ForeignKey(WindparkOperatorOverview.windparkOperator)",\n      "windpark": "ForeignKey(Windpark.windpark)"\n    }\n  }\n}\n\n        Specified Format:\n        \n        {\n          "Folder1": {\n            "class1": "Explanation 1"\n            "class2": "Explanation 2"\n          },\n          "Folder2": {\n            "class3": "Explanation 3"\n            "class4": "Explanation 4"\n          }\n        } \n        \n\n\n        **Outputs**:\n            output only the json without ```json:\n\n        json =  \n        '}]
2024-11-29 13:49:48.146 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:48.147 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:49:48.147 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 939, completion_tokens: 220
2024-11-29 13:49:48.147 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:50:00.830 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n    1. Given a json...']
2024-11-29 13:50:00.830 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:50:00.831 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:50:00.831 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n    1. Given a json that has all entities and relationships you will create react json object exactly like given below.\n    2. As field types, be careful to use correct names from builtin Django model fields.\n    3. Models relationships are mandatory, you should use ForeignKey for relationships.\n    4. No ```json\n    5. Use double quotes for keys and values\n    6. Do not include anything apart from the fields such as descriptions or methods.\n    7. Do not write otherFields, instead write every field explicitly.\n\n    Project Structure: \n    {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n\n    Example:\n    \n    {\n    "Folder1": {\n      "Class1": {\n        "id": "AutoField (Primary Key)",\n        "class_to_another_class": "ForeignKey (to self)",\n        "name": "TextField",\n        "date": "DateTimeField",\n        ...otherfields\n      },\n      "Class2": {\n        "id": "AutoField (Primary Key)",\n        "period": "ForeignKey (to Period)",\n        "class2": "TextField",\n        "number_of_class5": "IntegerField",\n        "year": "IntegerField",\n        "investment": "FloatField",\n        "employees": "IntegerField",\n        "area": "FloatField",\n        ...otherfields\n      },\n      },\n        "Folder2": {\n      "report_cashflow_expample": {\n        "id": "AutoField (Primary Key)",\n        "upload_id": "IntegerField",\n        "class_4": "ForeignKey (to Class4)",\n        "class1_name": "ForeignKey (to Class1)",\n        "date": "DateTimeField",\n        "year": "IntegerField",\n        "cashflow": "FloatField",\n        ...otherfields\n      },\n      },\n      "Folder3": {\n      "Downloads": {\n        "id": "AutoField (Primary Key)",\n        "report": "XLSXField",\n        ...otherfields\n      },\n      },\n      ...otherfoldersandclasses\n    } \n    \n    \n    Current Project Model Structure:\n    This is the first time for this query, there is no project model structure.\n    \n    User Feedback:\n    This is the first time for this query, there is no user feedback.\n\n    **Output**:\n    Return only the json without ```json:\n    json =  \n    '}]
2024-11-29 13:50:06.706 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:06.707 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:06.708 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.014 | Max budget: $10.000 | Current cost: $0.014, prompt_tokens: 1128, completion_tokens: 578
2024-11-29 13:50:06.709 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:50:06.773 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n    old structure: ...']
2024-11-29 13:50:06.773 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:50:06.774 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:50:06.774 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n    old structure: \n    {\'Inputs\': {\'Windpark\': \'Represents individual windparks and their annual energy production.\', \'USCashflowReport\': \'Represents cashflow data for US windpark operators.\', \'ChinaCashflowReport\': \'Represents cashflow data for Chinese windpark operators.\', \'GermanyCashflowReport\': \'Represents cashflow data for German windpark operators.\', \'WindparkOperatorOverview\': \'Represents windpark operators, their investments, and other details.\'}, \'Reports\': {\'CashflowReport\': \'Generates consolidated cashflow reports.\'}, \'Uploads\': {\'WindparkUpload\': \'Handles data uploads for windparks.\', \'USCashflowReportUpload\': \'Handles data uploads for US cashflow reports.\', \'ChinaCashflowReportUpload\': \'Handles data uploads for China cashflow reports.\', \'GermanyCashflowReportUpload\': \'Handles data uploads for Germany cashflow reports.\', \'WindparkOperatorOverviewUpload\': \'Handles data uploads for windpark operators.\'}}\n\n    Example:\n    \n    {\n        "class_name1": "path/to/class1.py",\n        "class_name2": "path2/to2/class2.py",\n        ...\n    } \n     \n    \n    updated models and their names:\n    {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n\n    1. Give back a json that holds the files structure of the new models and their fields, according to the old file structure and the example.\n    2. Don\'t include anything else just classname as key and path as value.\n    3. replace the \'path/to/\' with the path from the model_fields and old structure, find the class path and replace it\n    \n    **Output**:\n    Return only the json without ```json:\n    json =  \n    '}]
2024-11-29 13:50:09.273 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:09.274 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:09.274 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 865, completion_tokens: 218
2024-11-29 13:50:09.275 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:50:13.589 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \nFile content and th...']
2024-11-29 13:50:13.590 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:50:13.590 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:50:13.590 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\nFile content and their explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nProject Structure:\n{\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n\nCurrent Project Functionalities:\nThis is the first time for this query, there is no project functionality.\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback.\n\n[START INSTRUCTIONS]\n\nGiven the project structure, summarize the main functionalities of the project, listing each model with a high-level description of its role and key interactions.\n1. Provide a basic pseudocode outline for each model’s calculate method (Upload and Report Model basically), mentioning:\n    - Primary steps for data processing, report generation, and any conditional logic.\n    - Any logging actions.\n    - Simplified placeholders for complex calculations, with notes on intended outcomes.\n    - Avoid specific implementation details; instead, focus on the sequence and purpose of steps in each model.\n2. Present this information in markdown format without code snippets. \n\n[STOP INSTRUCTIONS]\nStart of markdown:\n    '}]
2024-11-29 13:50:28.030 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:28.032 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:28.034 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.026 | Max budget: $10.000 | Current cost: $0.026, prompt_tokens: 1979, completion_tokens: 1049
2024-11-29 13:50:28.034 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:50:34.095 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n    1. Given a json...']
2024-11-29 13:50:34.095 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:50:34.095 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:50:34.095 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n    1. Given a json that has all entities and relationships you will create react json object exactly like given below.\n    2. As field types, be careful to use correct names from builtin Django model fields.\n    3. Models relationships are mandatory, you should use ForeignKey for relationships.\n    4. No ```json\n    5. Use double quotes for keys and values\n    6. Do not include anything apart from the fields such as descriptions or methods.\n    7. Do not write otherFields, instead write every field explicitly.\n\n    Project Structure: \n    {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n\n    Example:\n    \n    {\n    "Folder1": {\n      "Class1": {\n        "id": "AutoField (Primary Key)",\n        "class_to_another_class": "ForeignKey (to self)",\n        "name": "TextField",\n        "date": "DateTimeField",\n        ...otherfields\n      },\n      "Class2": {\n        "id": "AutoField (Primary Key)",\n        "period": "ForeignKey (to Period)",\n        "class2": "TextField",\n        "number_of_class5": "IntegerField",\n        "year": "IntegerField",\n        "investment": "FloatField",\n        "employees": "IntegerField",\n        "area": "FloatField",\n        ...otherfields\n      },\n      },\n        "Folder2": {\n      "report_cashflow_expample": {\n        "id": "AutoField (Primary Key)",\n        "upload_id": "IntegerField",\n        "class_4": "ForeignKey (to Class4)",\n        "class1_name": "ForeignKey (to Class1)",\n        "date": "DateTimeField",\n        "year": "IntegerField",\n        "cashflow": "FloatField",\n        ...otherfields\n      },\n      },\n      "Folder3": {\n      "Downloads": {\n        "id": "AutoField (Primary Key)",\n        "report": "XLSXField",\n        ...otherfields\n      },\n      },\n      ...otherfoldersandclasses\n    } \n    \n    \n    Current Project Model Structure:\n    This is the first time for this query, there is no project model structure.\n    \n    User Feedback:\n    This is the first time for this query, there is no user feedback.\n\n    **Output**:\n    Return only the json without ```json:\n    json =  \n    '}]
2024-11-29 13:50:42.652 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:42.654 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:42.655 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.014 | Max budget: $10.000 | Current cost: $0.014, prompt_tokens: 1128, completion_tokens: 578
2024-11-29 13:50:42.656 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:50:42.739 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n    old structure: ...']
2024-11-29 13:50:42.739 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:50:42.740 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:50:42.740 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n    old structure: \n    {\'Inputs\': {\'Windpark\': \'Represents individual windparks and their annual energy production.\', \'USCashflowReport\': \'Represents cashflow data for US windpark operators.\', \'ChinaCashflowReport\': \'Represents cashflow data for Chinese windpark operators.\', \'GermanyCashflowReport\': \'Represents cashflow data for German windpark operators.\', \'WindparkOperatorOverview\': \'Represents windpark operators, their investments, and other details.\'}, \'Reports\': {\'CashflowReport\': \'Generates consolidated cashflow reports.\'}, \'Uploads\': {\'WindparkUpload\': \'Handles data uploads for windparks.\', \'USCashflowReportUpload\': \'Handles data uploads for US cashflow reports.\', \'ChinaCashflowReportUpload\': \'Handles data uploads for China cashflow reports.\', \'GermanyCashflowReportUpload\': \'Handles data uploads for Germany cashflow reports.\', \'WindparkOperatorOverviewUpload\': \'Handles data uploads for windpark operators.\'}}\n\n    Example:\n    \n    {\n        "class_name1": "path/to/class1.py",\n        "class_name2": "path2/to2/class2.py",\n        ...\n    } \n     \n    \n    updated models and their names:\n    {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n\n    1. Give back a json that holds the files structure of the new models and their fields, according to the old file structure and the example.\n    2. Don\'t include anything else just classname as key and path as value.\n    3. replace the \'path/to/\' with the path from the model_fields and old structure, find the class path and replace it\n    \n    **Output**:\n    Return only the json without ```json:\n    json =  \n    '}]
2024-11-29 13:50:45.724 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:45.725 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:50:45.726 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.008 | Max budget: $10.000 | Current cost: $0.008, prompt_tokens: 865, completion_tokens: 218
2024-11-29 13:50:45.726 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:51:33.445 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n   Lex_app context:...']
2024-11-29 13:51:33.445 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:51:33.445 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:51:33.445 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '\nYou are Josh, a highly capable and analytical AI Software Engineer. Your task is to provide an accurate, detailed, and comprehensive description of the business logic for the given project. You will use the provided project context, structure, models, and any user feedback to inform your response. \n**Incorporate a clear chain-of-thought** in your reasoning to ensure transparency and comprehensiveness in your explanations. When describing the `calculate()` methods, provide **detailed and well-documented pseudo-code** that outlines each step of the calculation process.\n    '}, {'role': 'user', 'content': '\n   Lex_app context:\n{\'CalculationModel\': {\'Import Path\': \'from lex.lex_app.lex_models.CalculationModel import CalculationModel\', \'source code\': "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50, choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract = True\\n    \\n    # This is the only thing that should be implemented, DON\'T USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE, on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self, \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct type\\n          # update_calculation_status(self)\\n          self.calculate()\\n          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n      except Exception as e:\\n        self.is_calculated = self.ERROR\\n        raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n", \'LexModel\': {\'Import Path\': \'from lex.lex_app.lex_models.LexModel import LexModel\', \'source code\': \'class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True, blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True, editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n  def update_edited_by(self):\\n    context = context_id.get()\\n    if context and hasattr(context[\\\'request_obj\\\'], \\\'auth\\\'):\\n      self.edited_by = f"{context[\\\'request_obj\\\'].auth[\\\'name\\\']} ({context[\\\'request_obj\\\'].auth[\\\'sub\\\']})"\\n    else:\\n      self.edited_by = \\\'Initial Data Upload\\\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n    context = context_id.get()\\n    if context and hasattr(context[\\\'request_obj\\\'], \\\'auth\\\'):\\n      self.created_by = f"{context[\\\'request_obj\\\'].auth[\\\'name\\\']} ({context[\\\'request_obj\\\'].auth[\\\'sub\\\']})"\\n    else:\\n      self.created_by = \\\'Initial Data Upload\\\'\\n\'}}, \'LexLogger\': {\'Import Path\': \'from lex.lex_app.LexLogger.LexLogger import LexLogger\', \'source code\': \'@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n      self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n      self.kwargs = {**{key: value for key, value in kwargs.items()\\n        if key != "flushing" and key != "level" and key not in self.kwargs.keys()}, **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self, text: str, level: int = 1):\\n                                              if 1 <= level <= 6:\\n                                                self.content.append(f"{\\\'#\\\' * level} {text}\\\\n")\\n                                              return self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n                                      """Add a paragraph."""\\n                                        self.content.append(f"{text}\\\\n\\\\n")\\n                                        return self._check_flush()\\n    \\n    def sleep(self, seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self, text, color="black"):\\n      self.content.append(f"<span style=\\\'color:{color}\\\'>{text}</span>\\\\n\\\\n")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text: str):\\n                                 """Add bold text."""\\n                                   self.content.append(f"**{text}** ")\\n                                   return self._check_flush()\\n    \\n    def add_table(self, data: dict):\\n                                  """Add a table from a dictionary. Keys are the headers, values are lists of column data."""\\n                                    headers = list(data.keys())\\n                                    rows = list(zip(*data.values()))\\n                                \\n                                # Add header row\\n                                    self.content.append(f"| {\\\' | \\\'.join(headers)} |\\\\n")\\n                                    self.content.append(f"|{\\\'|\\\'.join([\\\' --- \\\' for _ in headers])}|\\\\n")\\n                                \\n                                # Add rows\\n                                    for row in rows:\\n                                      self.content.append(f"| {\\\' | \\\'.join(map(str, row))} |\\\\n")\\n                                    self.content.append("\\\\n")\\n                                    return self._check_flush()\\n    \\n    def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n        return self.add_paragraph("No data available")._check_flush()\\n      \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n      # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number of columns dynamically from the first row of the data\\n        num_columns = len(data[0])\\n        columns = [f"Column {i + 1}" for i in range(num_columns)]\\n        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return self.add_paragraph("Invalid data format")._check_flush()\\n    \\n    def add_italic(self, text: str):\\n                                   """Add italic text."""\\n                                     self.content.append(f"*{text}*")\\n                                     return self._check_flush()\\n    \\n    def add_link(self, text: str, url: str):\\n                                           """Add a link."""\\n                                             self.content.append(f"[{text}]({url})")\\n                                             return self._check_flush()\\n    \\n    def add_list(self, items: list, ordered: bool = False):\\n                                                 """Add a list, either ordered (numbered) or unordered (bullets)."""\\n                                                   if ordered:\\n                                                     self.content.extend([f"{i + 1}. {item}" for i, item in enumerate(items)])\\n                                                   else:\\n                                                     self.content.extend([f"- {item}" for item in items])\\n                                                   self.content.append("\\\\n")\\n                                                   return self._check_flush()\\n    \\n    def add_code_block(self, code: str, language: str = ""):\\n                                                      """Add a code block with optional language syntax highlighting."""\\n                                                        self.content.append(f"```{language}\\\\n{code}\\\\n```\\\\n")\\n                                                        return self._check_flush()\\n    \\n    def add_horizontal_rule(self):\\n        """Add a horizontal rule."""\\n          self.content.append("---\\\\n")\\n          return self._check_flush()\\n    \\n    def add_blockquote(self, text: str):\\n                                       """Add blockquote."""\\n                                         self.content.append(f"> {text}\\\\n\\\\n")\\n                                         return self._check_flush()\\n    \\n    def add_image(self, alt_text: str, url: str):\\n                                                """Add an image."""\\n                                                  self.content.append(f"![{alt_text}]({url})\\\\n\\\\n")\\n                                                  return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n                           message = self.__str__()\\n                           if not message:\\n                             return\\n                           self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n                           if self.content is self.parts:\\n                             self.parts = []\\n                             self.det = []\\n                             self.content = self.parts\\n                           else:\\n                             self.parts = []\\n                             self.det = []\\n                             self.content = self.det\\n                           \\n                           return self\\n    \\n    def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n        """Return the entire Markdown text as a string."""\\n          return "".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return "".join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]: value for key, value in self.kwargs.items() if key != "flushing"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'details\\\']: self.details_to_str(),\\n                                                                 WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'message\\\']: self.__str__(), WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'level\\\']: \\\'INFO\\\'}\\n  \\n  def is_valid_markdown(self, message: str) -> bool:\\n                                         try:\\n                                           self.parser(message)\\n                                           return True\\n                                         except Exception as e:\\n                                           print(e)\\n                                           return False\\n  \\n  def __init__(self):\\n    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE, "VERBOSE")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\\\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\\\')\\n    formatter = logging.Formatter(\\\'%(message)s\\\')\\n    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self, message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n    message=message,\\n    message_type=kwargs.get(\\\'message_type\\\', "Progress"),\\n    trigger_name=kwargs.get(\\\'trigger_name\\\', None),\\n    is_notification=kwargs.get(\\\'is_notification\\\', False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \\\'log_id\\\': obj.id, \\\'calculation_id\\\': obj.calculationId, \\\'class_name\\\': obj.calculation_record, "trigger_name": obj.trigger_name, "method": obj.method})\\n  \\n  def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level, flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self, message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n\'}, \'XLSXField\': {\'Import Path\': \'from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\', \'source code\': \'class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format = \\\'#,##0.00 ;[Red]-#,##0.00 ;_-* "-"??_-\\\'\\n  cell_format_without_color = \\\'#,##0.00 ;-#,##0.00 ;_-* "-"??_-\\\'\\n  boolean_format = \\\'[Green]"TRUE";[Red]"FALSE";[Red]"FALSE";[Red]"FALSE"\\\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self, sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)  # Adjust the row number to the row where you want to start splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(".")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1, amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows, index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1, column=column_num)  # Adjust the row number to the row where you want to start splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(".")  # Split the entry at every "."\\n        first_row = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num, value=split_value)\\n          # Bold the cell and add outside borders\\n          cell_to_format = sheet.cell(row=row_num, column=column_num)\\n          cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\\\'thin\\\'),\\n          bottom=Side(border_style=\\\'thin\\\'),\\n          left=Side(border_style=\\\'thin\\\'),\\n          right=Side(border_style=\\\'thin\\\'))\\n  \\n  def create_pivotable_row(self, sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value = ""\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num, column=column_num)\\n        if cell.value:\\n          concatenated_value += cell.value + " "\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num, value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self, path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={}, index=True, ranges_of_pivot_concatenation={\\\'default\\\': None}):\\n                                                                                                                                                                                  """\\n        :param path: file_path including file_name; if relative, will be saved under self.to+path\\n        :param data_frames: list of dataframes that will be inserted into an excel tab each\\n        :param sheet_names: list of sheet names corresponding to the data_frames\\n        :rtype: None\\n        """\\n    if sheet_names is None:\\n        sheet_names = [\\\'Sheet\\\']\\n    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\\\'xlsxwriter\\\')\\n    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames, sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n                df = df.append(pd.Series(), ignore_index=True)\\n            if index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer, sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]  # pull worksheet object\\n            worksheet_comment = comments[sheet_name] if sheet_name in comments else None\\n            cell_formats = {}\\n            for format in formats:\\n                cell_formats[format] = writer.book.add_format({\\\'num_format\\\': formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n                for idx, col in enumerate(index_frame):  # loop through all columns\\n                    series = index_frame[col]\\n                    max_len = max((\\n                        series.astype(str).map(len).max(),  # len of largest item\\n                        len(str(series.name))  # len of column name/header\\n                    )) + 1  # adding a little extra space\\n                    if is_datetime(series):\\n                        max_len = 22\\n                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\n            for idx, col in enumerate(df):  # loop through all columns\\n                series = df[col]\\n                if worksheet_comment is not None:\\n                    comment = worksheet_comment[col] if col in worksheet_comment else None\\n                    if comment is not None:\\n                        worksheet.write_comment(0, idx + idx_length, comment)\\n\\n                max_len = max((\\n                    series.astype(str).map(len).max(),  # len of largest item\\n                    len(str(series.name))  # len of column name/header\\n                )) + 1  # adding a little extra space\\n                worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  # set column width\\n                # set Cell format\\n                if col in formats:\\n                    worksheet.set_column(idx + idx_length, idx + idx_length, max_len, cell_format=cell_formats[col])\\n                elif is_datetime(df[col]):\\n                    pass\\n                else:\\n                    worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n                                         cell_format=writer.book.add_format({\\\'num_format\\\': XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns) > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path, content=File(excel_file), save=False)\\n\\n    return excel_file\\n\', \'Example Usage\': \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames, sheet_names)\\n\'}, \'LexLogLevel\': {\'Import Path\': \'from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\', \'source code\': \'class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n\'}, \'PrcoessAdminTest\': {\'Import Path\': \'from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\', \'source code\': \'class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase class is a custom test case class designed to facilitate testing in a Django application. It extends from unittest.TestCase and automates the setup and teardown of test data based on a JSON configuration. This class reads a JSON structure defining actions like create, update, and delete on Django models and executes these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase` to create a custom test case class that automates test data setup.\\n\\n    def replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes parameters by replacing special tagged values with actual objects or parsed data.\\n       # * Functionality:\\n       #   - Iterates over each key-value pair in object_parameters.\\n       #   - If the value is a string and starts with:\\n       #     "tag:": Replaces it with an object from self.tagged_objects.\\n       #     "datetime:": Parses it into a datetime object.\\n       #   - Updates the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n          value: str = object_parameters[key]\\n          if isinstance(value, str):\\n              parsed_value = value\\n              if value.startswith("tag:"):\\n                  # Replace \\\'tag:<tag_name>\\\' with the actual object from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace("tag:", "")]\\n              elif value.startswith("datetime:"):\\n                  # Parse \\\'datetime:<date_string>\\\' into a datetime object\\n                  parsed_value = dateutil.parser.parse(value.replace("datetime:", ""))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\n\\n    # test_path: An attribute that specifies the path to the test data JSON file. If not provided, it defaults to test_data.json in the same directory as the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n       # Purpose: Sets up the test environment by performing actions defined in the test data.\\n       # Functionality:\\n       #   Retrieves all models from the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp and an empty dictionary for tagged objects.\\n       #   Loads test data using get_test_data().\\n       #   Iterates over each action in the test data and performs create, update, or delete operations.\\n       #     Create:\\n       #       Processes parameters and creates a new object.\\n       #       Stores the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n       #       Processes filter parameters to find the object.\\n       #       Updates the object\\\'s attributes and saves it.\\n       #     Delete:\\n       #       Deletes objects matching the filter parameters.\\n\\n        from datetime import datetime\\n\\n        # Retrieve all models from the specified Django app\\n        generic_app_models = {\\n            f"{model.__name__}": model for model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n        self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n        # Iterate over each action in the test data\\n        for object in test_data:\\n            klass = generic_app_models[object[\\\'class\\\']]  # Get the model class\\n            action = object[\\\'action\\\']\\n            tag = object.get(\\\'tag\\\', \\\'instance\\\')  # Default tag if not provided\\n\\n            if action == \\\'create\\\':\\n                # Replace tagged parameters\\n                object[\\\'parameters\\\'] = self.replace_tagged_parameters(object[\\\'parameters\\\'])\\n                # Create an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\\\'parameters\\\'])\\n                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(), f"{object[\\\'class\\\']}_{action}")\\n                # Save the object to the database\\n                self.tagged_objects[tag].save()\\n\\n            elif action == \\\'update\\\':\\n                # Replace tagged filter parameters\\n                object[\\\'filter_parameters\\\'] = self.replace_tagged_parameters(object[\\\'filter_parmeters\\\'])\\n                # Retrieve the object to update\\n                self.tagged_objects[tag] = klass.objects.filter(**object[\\\'filter_parameters\\\']).first()\\n                if self.tagged_objects[tag] is not None:\\n                    # Update the object\\\'s attributes\\n                    for key in object[\\\'parameters\\\']:\\n                        setattr(self.tagged_objects[tag], key, object[\\\'parameters\\\'][key])\\n\\n                    # Cache the action with object primary key\\n                    cache.set(\\n                        threading.get_ident(),\\n                        f"{object[\\\'class\\\']}_{action}_{self.tagged_objects[tag].pk}"\\n                    )\\n                    # Save the updated object\\n                    self.tagged_objects[tag].save()\\n\\n            elif action == \\\'delete\\\':\\n                # Delete objects matching the filter parameters\\n                klass.objects.filter(**object[\\\'filter_parameters\\\']).delete()\\n\\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n        # Purpose: Determines the path to the test data JSON file and loads it.\\n        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json in the test class directory.\\n        #   If test_path is provided, constructs the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path() to load and return the test data.\\n\\n        if self.test_path is None:\\n            # Use default \\\'test_data.json\\\' in the test class directory\\n            file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n            clean_test_path = str(path) + os.sep + "test_data.json"\\n        else:\\n            # Use the provided \\\'test_path\\\'\\n            clean_test_path = self.test_path.replace(\\\'/\\\', os.sep)\\n            clean_test_path = os.getenv("PROJECT_ROOT") + os.sep + clean_test_path\\n\\n        # Load test data from the JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n        # Purpose: Loads test data from the specified path, handling any nested subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON file at the given path.\\n        #   Checks for any "subprocess" keys in the data and recursively loads additional test data.\\n        #   Flattens the potentially nested lists of test data into a single list.\\n        #   Returns the combined test data.\\n\\n        with open(str(path), \\\'r\\\') as f:\\n            test_data = json.loads(f.read())\\n            # Process any \\\'subprocess\\\' entries recursively\\n            for index, object in enumerate(test_data):\\n                if "subprocess" in object:\\n                    subprocess_path = object[\\\'subprocess\\\'].replace(\\\'/\\\', os.sep)\\n                    subprocess_path = os.getenv("PROJECT_ROOT") + os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n                    test_data[index] = sublist\\n\\n        # Flatten the list in case of nested lists from subprocesses\\n        flat_list = []\\n        for sublist in test_data:\\n            if isinstance(sublist, list):\\n                flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n        # Purpose: Retrieves a set of all model classes involved in the test data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #   Uses a set comprehension to collect unique model classes based on the "class" key in each test data object.\\n\\n        test_data = self.get_test_data()\\n        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\\\'class\\\']] for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n        # Purpose: Checks whether all involved model classes have no instances in the database.\\n        # Functionality:\\n        #   Iterates over each model class obtained from get_classes().\\n        #   Returns False immediately if any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count() > 0:\\n                # If any model has objects, return False\\n                return False\\n        # All models are empty\\n        return True\\n\\n    def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose: Provides a summary of models that have instances in the database.\\n        # Functionality:\\n        #   Iterates over each model class from get_classes().\\n        #   Counts the number of instances for each class.\\n        #   If a class has instances, adds it to the result dictionary with the count.\\n        #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models = {}\\n        for klass in self.get_classes(generic_app_models):\\n            c = klass.objects.all().count()\\n            if c > 0:\\n                # Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)] = c\\n        return count_of_objects_in_non_empty_models\\n\'}}\n\nFile content and their explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nProject Structure:\n{\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n\nProject Models and Fields:\n{\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n\nAccumulated Functionalities:\n# Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback.\n\n[START INSTRUCTIONS]\n\n1. Given the project structure, enhance the basic pseudocode provided in the functionalities step by adding detailed business logic for each model’s calculate method. Incorporate specifics on data processing, complex calculations, and any conditional steps.\n2. Include validation checks for each calculate method:\n    - Confirm that each step in the pseudocode aligns with the intended functionality and business logic.\n    - Cross-check that calculations, logging with LexLogger, and any necessary data transformations are included and correctly ordered.\n    - Highlight any potential gaps or issues within the calculation logic.\n3. Log any key operations as required by project guidelines using LexLogger.\n4. Avoid including fields from CalculationModel or LexModel unless they are specific to this project.\n5. Present the business logic in markdown format with only descriptive text (no code snippets).\n6. Please add a user story or scenario that demonstrates the use of the business logic in a real-world context.\n\n[STOP INSTRUCTIONS]\nStart of markdown: \n    '}]
2024-11-29 13:51:57.818 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:51:57.827 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:51:57.829 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 10370, completion_tokens: 1895
2024-11-29 13:51:57.830 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:52:00.385 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n   Lex_app context:...']
2024-11-29 13:52:00.385 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:52:00.385 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:52:00.385 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '\nYou are Josh, a highly capable and analytical AI Software Engineer. Your task is to provide an accurate, detailed, and comprehensive description of the business logic for the given project. You will use the provided project context, structure, models, and any user feedback to inform your response. \n**Incorporate a clear chain-of-thought** in your reasoning to ensure transparency and comprehensiveness in your explanations. When describing the `calculate()` methods, provide **detailed and well-documented pseudo-code** that outlines each step of the calculation process.\n    '}, {'role': 'user', 'content': '\n   Lex_app context:\n{\'CalculationModel\': {\'Import Path\': \'from lex.lex_app.lex_models.CalculationModel import CalculationModel\', \'source code\': "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50, choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract = True\\n    \\n    # This is the only thing that should be implemented, DON\'T USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE, on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self, \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct type\\n          # update_calculation_status(self)\\n          self.calculate()\\n          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n      except Exception as e:\\n        self.is_calculated = self.ERROR\\n        raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n", \'LexModel\': {\'Import Path\': \'from lex.lex_app.lex_models.LexModel import LexModel\', \'source code\': \'class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True, blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True, editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n  def update_edited_by(self):\\n    context = context_id.get()\\n    if context and hasattr(context[\\\'request_obj\\\'], \\\'auth\\\'):\\n      self.edited_by = f"{context[\\\'request_obj\\\'].auth[\\\'name\\\']} ({context[\\\'request_obj\\\'].auth[\\\'sub\\\']})"\\n    else:\\n      self.edited_by = \\\'Initial Data Upload\\\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n    context = context_id.get()\\n    if context and hasattr(context[\\\'request_obj\\\'], \\\'auth\\\'):\\n      self.created_by = f"{context[\\\'request_obj\\\'].auth[\\\'name\\\']} ({context[\\\'request_obj\\\'].auth[\\\'sub\\\']})"\\n    else:\\n      self.created_by = \\\'Initial Data Upload\\\'\\n\'}}, \'LexLogger\': {\'Import Path\': \'from lex.lex_app.LexLogger.LexLogger import LexLogger\', \'source code\': \'@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n      self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n      self.kwargs = {**{key: value for key, value in kwargs.items()\\n        if key != "flushing" and key != "level" and key not in self.kwargs.keys()}, **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self, text: str, level: int = 1):\\n                                              if 1 <= level <= 6:\\n                                                self.content.append(f"{\\\'#\\\' * level} {text}\\\\n")\\n                                              return self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n                                      """Add a paragraph."""\\n                                        self.content.append(f"{text}\\\\n\\\\n")\\n                                        return self._check_flush()\\n    \\n    def sleep(self, seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self, text, color="black"):\\n      self.content.append(f"<span style=\\\'color:{color}\\\'>{text}</span>\\\\n\\\\n")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text: str):\\n                                 """Add bold text."""\\n                                   self.content.append(f"**{text}** ")\\n                                   return self._check_flush()\\n    \\n    def add_table(self, data: dict):\\n                                  """Add a table from a dictionary. Keys are the headers, values are lists of column data."""\\n                                    headers = list(data.keys())\\n                                    rows = list(zip(*data.values()))\\n                                \\n                                # Add header row\\n                                    self.content.append(f"| {\\\' | \\\'.join(headers)} |\\\\n")\\n                                    self.content.append(f"|{\\\'|\\\'.join([\\\' --- \\\' for _ in headers])}|\\\\n")\\n                                \\n                                # Add rows\\n                                    for row in rows:\\n                                      self.content.append(f"| {\\\' | \\\'.join(map(str, row))} |\\\\n")\\n                                    self.content.append("\\\\n")\\n                                    return self._check_flush()\\n    \\n    def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n        return self.add_paragraph("No data available")._check_flush()\\n      \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n      # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number of columns dynamically from the first row of the data\\n        num_columns = len(data[0])\\n        columns = [f"Column {i + 1}" for i in range(num_columns)]\\n        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return self.add_paragraph("Invalid data format")._check_flush()\\n    \\n    def add_italic(self, text: str):\\n                                   """Add italic text."""\\n                                     self.content.append(f"*{text}*")\\n                                     return self._check_flush()\\n    \\n    def add_link(self, text: str, url: str):\\n                                           """Add a link."""\\n                                             self.content.append(f"[{text}]({url})")\\n                                             return self._check_flush()\\n    \\n    def add_list(self, items: list, ordered: bool = False):\\n                                                 """Add a list, either ordered (numbered) or unordered (bullets)."""\\n                                                   if ordered:\\n                                                     self.content.extend([f"{i + 1}. {item}" for i, item in enumerate(items)])\\n                                                   else:\\n                                                     self.content.extend([f"- {item}" for item in items])\\n                                                   self.content.append("\\\\n")\\n                                                   return self._check_flush()\\n    \\n    def add_code_block(self, code: str, language: str = ""):\\n                                                      """Add a code block with optional language syntax highlighting."""\\n                                                        self.content.append(f"```{language}\\\\n{code}\\\\n```\\\\n")\\n                                                        return self._check_flush()\\n    \\n    def add_horizontal_rule(self):\\n        """Add a horizontal rule."""\\n          self.content.append("---\\\\n")\\n          return self._check_flush()\\n    \\n    def add_blockquote(self, text: str):\\n                                       """Add blockquote."""\\n                                         self.content.append(f"> {text}\\\\n\\\\n")\\n                                         return self._check_flush()\\n    \\n    def add_image(self, alt_text: str, url: str):\\n                                                """Add an image."""\\n                                                  self.content.append(f"![{alt_text}]({url})\\\\n\\\\n")\\n                                                  return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n                           message = self.__str__()\\n                           if not message:\\n                             return\\n                           self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n                           if self.content is self.parts:\\n                             self.parts = []\\n                             self.det = []\\n                             self.content = self.parts\\n                           else:\\n                             self.parts = []\\n                             self.det = []\\n                             self.content = self.det\\n                           \\n                           return self\\n    \\n    def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n        """Return the entire Markdown text as a string."""\\n          return "".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return "".join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]: value for key, value in self.kwargs.items() if key != "flushing"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'details\\\']: self.details_to_str(),\\n                                                                 WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'message\\\']: self.__str__(), WebSocketHandler.DJANGO_TO_REACT_MAPPER[\\\'level\\\']: \\\'INFO\\\'}\\n  \\n  def is_valid_markdown(self, message: str) -> bool:\\n                                         try:\\n                                           self.parser(message)\\n                                           return True\\n                                         except Exception as e:\\n                                           print(e)\\n                                           return False\\n  \\n  def __init__(self):\\n    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE, "VERBOSE")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\\\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\\\')\\n    formatter = logging.Formatter(\\\'%(message)s\\\')\\n    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self, message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n    message=message,\\n    message_type=kwargs.get(\\\'message_type\\\', "Progress"),\\n    trigger_name=kwargs.get(\\\'trigger_name\\\', None),\\n    is_notification=kwargs.get(\\\'is_notification\\\', False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \\\'log_id\\\': obj.id, \\\'calculation_id\\\': obj.calculationId, \\\'class_name\\\': obj.calculation_record, "trigger_name": obj.trigger_name, "method": obj.method})\\n  \\n  def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level, flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self, message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n\'}, \'XLSXField\': {\'Import Path\': \'from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\', \'source code\': \'class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format = \\\'#,##0.00 ;[Red]-#,##0.00 ;_-* "-"??_-\\\'\\n  cell_format_without_color = \\\'#,##0.00 ;-#,##0.00 ;_-* "-"??_-\\\'\\n  boolean_format = \\\'[Green]"TRUE";[Red]"FALSE";[Red]"FALSE";[Red]"FALSE"\\\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self, sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)  # Adjust the row number to the row where you want to start splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(".")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1, amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows, index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1, column=column_num)  # Adjust the row number to the row where you want to start splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(".")  # Split the entry at every "."\\n        first_row = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num, value=split_value)\\n          # Bold the cell and add outside borders\\n          cell_to_format = sheet.cell(row=row_num, column=column_num)\\n          cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\\\'thin\\\'),\\n          bottom=Side(border_style=\\\'thin\\\'),\\n          left=Side(border_style=\\\'thin\\\'),\\n          right=Side(border_style=\\\'thin\\\'))\\n  \\n  def create_pivotable_row(self, sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value = ""\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num, column=column_num)\\n        if cell.value:\\n          concatenated_value += cell.value + " "\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num, value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self, path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={}, index=True, ranges_of_pivot_concatenation={\\\'default\\\': None}):\\n                                                                                                                                                                                  """\\n        :param path: file_path including file_name; if relative, will be saved under self.to+path\\n        :param data_frames: list of dataframes that will be inserted into an excel tab each\\n        :param sheet_names: list of sheet names corresponding to the data_frames\\n        :rtype: None\\n        """\\n    if sheet_names is None:\\n        sheet_names = [\\\'Sheet\\\']\\n    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\\\'xlsxwriter\\\')\\n    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames, sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n                df = df.append(pd.Series(), ignore_index=True)\\n            if index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer, sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]  # pull worksheet object\\n            worksheet_comment = comments[sheet_name] if sheet_name in comments else None\\n            cell_formats = {}\\n            for format in formats:\\n                cell_formats[format] = writer.book.add_format({\\\'num_format\\\': formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n                for idx, col in enumerate(index_frame):  # loop through all columns\\n                    series = index_frame[col]\\n                    max_len = max((\\n                        series.astype(str).map(len).max(),  # len of largest item\\n                        len(str(series.name))  # len of column name/header\\n                    )) + 1  # adding a little extra space\\n                    if is_datetime(series):\\n                        max_len = 22\\n                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\n            for idx, col in enumerate(df):  # loop through all columns\\n                series = df[col]\\n                if worksheet_comment is not None:\\n                    comment = worksheet_comment[col] if col in worksheet_comment else None\\n                    if comment is not None:\\n                        worksheet.write_comment(0, idx + idx_length, comment)\\n\\n                max_len = max((\\n                    series.astype(str).map(len).max(),  # len of largest item\\n                    len(str(series.name))  # len of column name/header\\n                )) + 1  # adding a little extra space\\n                worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  # set column width\\n                # set Cell format\\n                if col in formats:\\n                    worksheet.set_column(idx + idx_length, idx + idx_length, max_len, cell_format=cell_formats[col])\\n                elif is_datetime(df[col]):\\n                    pass\\n                else:\\n                    worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n                                         cell_format=writer.book.add_format({\\\'num_format\\\': XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns) > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path, content=File(excel_file), save=False)\\n\\n    return excel_file\\n\', \'Example Usage\': \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames, sheet_names)\\n\'}, \'LexLogLevel\': {\'Import Path\': \'from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\', \'source code\': \'class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n\'}, \'PrcoessAdminTest\': {\'Import Path\': \'from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\', \'source code\': \'class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase class is a custom test case class designed to facilitate testing in a Django application. It extends from unittest.TestCase and automates the setup and teardown of test data based on a JSON configuration. This class reads a JSON structure defining actions like create, update, and delete on Django models and executes these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase` to create a custom test case class that automates test data setup.\\n\\n    def replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes parameters by replacing special tagged values with actual objects or parsed data.\\n       # * Functionality:\\n       #   - Iterates over each key-value pair in object_parameters.\\n       #   - If the value is a string and starts with:\\n       #     "tag:": Replaces it with an object from self.tagged_objects.\\n       #     "datetime:": Parses it into a datetime object.\\n       #   - Updates the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n          value: str = object_parameters[key]\\n          if isinstance(value, str):\\n              parsed_value = value\\n              if value.startswith("tag:"):\\n                  # Replace \\\'tag:<tag_name>\\\' with the actual object from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace("tag:", "")]\\n              elif value.startswith("datetime:"):\\n                  # Parse \\\'datetime:<date_string>\\\' into a datetime object\\n                  parsed_value = dateutil.parser.parse(value.replace("datetime:", ""))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\n\\n    # test_path: An attribute that specifies the path to the test data JSON file. If not provided, it defaults to test_data.json in the same directory as the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n       # Purpose: Sets up the test environment by performing actions defined in the test data.\\n       # Functionality:\\n       #   Retrieves all models from the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp and an empty dictionary for tagged objects.\\n       #   Loads test data using get_test_data().\\n       #   Iterates over each action in the test data and performs create, update, or delete operations.\\n       #     Create:\\n       #       Processes parameters and creates a new object.\\n       #       Stores the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n       #       Processes filter parameters to find the object.\\n       #       Updates the object\\\'s attributes and saves it.\\n       #     Delete:\\n       #       Deletes objects matching the filter parameters.\\n\\n        from datetime import datetime\\n\\n        # Retrieve all models from the specified Django app\\n        generic_app_models = {\\n            f"{model.__name__}": model for model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n        self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n        # Iterate over each action in the test data\\n        for object in test_data:\\n            klass = generic_app_models[object[\\\'class\\\']]  # Get the model class\\n            action = object[\\\'action\\\']\\n            tag = object.get(\\\'tag\\\', \\\'instance\\\')  # Default tag if not provided\\n\\n            if action == \\\'create\\\':\\n                # Replace tagged parameters\\n                object[\\\'parameters\\\'] = self.replace_tagged_parameters(object[\\\'parameters\\\'])\\n                # Create an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\\\'parameters\\\'])\\n                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(), f"{object[\\\'class\\\']}_{action}")\\n                # Save the object to the database\\n                self.tagged_objects[tag].save()\\n\\n            elif action == \\\'update\\\':\\n                # Replace tagged filter parameters\\n                object[\\\'filter_parameters\\\'] = self.replace_tagged_parameters(object[\\\'filter_parmeters\\\'])\\n                # Retrieve the object to update\\n                self.tagged_objects[tag] = klass.objects.filter(**object[\\\'filter_parameters\\\']).first()\\n                if self.tagged_objects[tag] is not None:\\n                    # Update the object\\\'s attributes\\n                    for key in object[\\\'parameters\\\']:\\n                        setattr(self.tagged_objects[tag], key, object[\\\'parameters\\\'][key])\\n\\n                    # Cache the action with object primary key\\n                    cache.set(\\n                        threading.get_ident(),\\n                        f"{object[\\\'class\\\']}_{action}_{self.tagged_objects[tag].pk}"\\n                    )\\n                    # Save the updated object\\n                    self.tagged_objects[tag].save()\\n\\n            elif action == \\\'delete\\\':\\n                # Delete objects matching the filter parameters\\n                klass.objects.filter(**object[\\\'filter_parameters\\\']).delete()\\n\\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n        # Purpose: Determines the path to the test data JSON file and loads it.\\n        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json in the test class directory.\\n        #   If test_path is provided, constructs the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path() to load and return the test data.\\n\\n        if self.test_path is None:\\n            # Use default \\\'test_data.json\\\' in the test class directory\\n            file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n            clean_test_path = str(path) + os.sep + "test_data.json"\\n        else:\\n            # Use the provided \\\'test_path\\\'\\n            clean_test_path = self.test_path.replace(\\\'/\\\', os.sep)\\n            clean_test_path = os.getenv("PROJECT_ROOT") + os.sep + clean_test_path\\n\\n        # Load test data from the JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n        # Purpose: Loads test data from the specified path, handling any nested subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON file at the given path.\\n        #   Checks for any "subprocess" keys in the data and recursively loads additional test data.\\n        #   Flattens the potentially nested lists of test data into a single list.\\n        #   Returns the combined test data.\\n\\n        with open(str(path), \\\'r\\\') as f:\\n            test_data = json.loads(f.read())\\n            # Process any \\\'subprocess\\\' entries recursively\\n            for index, object in enumerate(test_data):\\n                if "subprocess" in object:\\n                    subprocess_path = object[\\\'subprocess\\\'].replace(\\\'/\\\', os.sep)\\n                    subprocess_path = os.getenv("PROJECT_ROOT") + os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n                    test_data[index] = sublist\\n\\n        # Flatten the list in case of nested lists from subprocesses\\n        flat_list = []\\n        for sublist in test_data:\\n            if isinstance(sublist, list):\\n                flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n        # Purpose: Retrieves a set of all model classes involved in the test data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #   Uses a set comprehension to collect unique model classes based on the "class" key in each test data object.\\n\\n        test_data = self.get_test_data()\\n        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\\\'class\\\']] for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n        # Purpose: Checks whether all involved model classes have no instances in the database.\\n        # Functionality:\\n        #   Iterates over each model class obtained from get_classes().\\n        #   Returns False immediately if any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count() > 0:\\n                # If any model has objects, return False\\n                return False\\n        # All models are empty\\n        return True\\n\\n    def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose: Provides a summary of models that have instances in the database.\\n        # Functionality:\\n        #   Iterates over each model class from get_classes().\\n        #   Counts the number of instances for each class.\\n        #   If a class has instances, adds it to the result dictionary with the count.\\n        #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models = {}\\n        for klass in self.get_classes(generic_app_models):\\n            c = klass.objects.all().count()\\n            if c > 0:\\n                # Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)] = c\\n        return count_of_objects_in_non_empty_models\\n\'}}\n\nFile content and their explanations:\n{"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n\nProject Structure:\n{\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n\nProject Models and Fields:\n{\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n\nAccumulated Functionalities:\n# Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\nUser Feedback:\nThis is the first time for this query, there is no user feedback.\n\n[START INSTRUCTIONS]\n\n1. Given the project structure, enhance the basic pseudocode provided in the functionalities step by adding detailed business logic for each model’s calculate method. Incorporate specifics on data processing, complex calculations, and any conditional steps.\n2. Include validation checks for each calculate method:\n    - Confirm that each step in the pseudocode aligns with the intended functionality and business logic.\n    - Cross-check that calculations, logging with LexLogger, and any necessary data transformations are included and correctly ordered.\n    - Highlight any potential gaps or issues within the calculation logic.\n3. Log any key operations as required by project guidelines using LexLogger.\n4. Avoid including fields from CalculationModel or LexModel unless they are specific to this project.\n5. Present the business logic in markdown format with only descriptive text (no code snippets).\n6. Please add a user story or scenario that demonstrates the use of the business logic in a real-world context.\n\n[STOP INSTRUCTIONS]\nStart of markdown: \n    '}]
2024-11-29 13:52:23.553 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:52:23.570 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:52:23.572 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 10370, completion_tokens: 1893
2024-11-29 13:52:23.572 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:52:40.040 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 13:52:40.041 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 13:52:40.042 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 13:52:51.496 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=-1
2024-11-29 13:53:29.129 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 13:53:29.129 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 13:53:29.130 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 13:53:31.841 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        \n     \n    The next class to generate is: Windpark\n    The class path is: model_fields/Inputs/Windpark.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:53:34.891 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:34.914 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:34.914 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.077 | Max budget: $10.000 | Current cost: $0.077, prompt_tokens: 15153, completion_tokens: 112
2024-11-29 13:53:34.992 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n     \n    The next class to generate is: CashflowReport\n    The class path is: model_fields/Reports/CashflowReport.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:53:45.042 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:45.056 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:45.057 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.163 | Max budget: $10.000 | Current cost: $0.086, prompt_tokens: 15279, completion_tokens: 633
2024-11-29 13:53:45.124 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n     \n    The next class to generate is: WindparkUpload\n    The class path is: model_fields/Uploads/WindparkUpload.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:53:50.998 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:51.012 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:51.013 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.250 | Max budget: $10.000 | Current cost: $0.086, prompt_tokens: 15925, completion_tokens: 453
2024-11-29 13:53:51.076 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n     \n    The next class to generate is: USCashflowReport\n    The class path is: model_fields/Inputs/USCashflowReport.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:53:54.293 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:54.306 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:54.307 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.333 | Max budget: $10.000 | Current cost: $0.084, prompt_tokens: 16392, completion_tokens: 106
2024-11-29 13:53:54.369 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n     \n    The next class to generate is: ChinaCashflowReport\n    The class path is: model_fields/Inputs/ChinaCashflowReport.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:53:57.214 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:57.245 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:53:57.245 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.418 | Max budget: $10.000 | Current cost: $0.084, prompt_tokens: 16511, completion_tokens: 124
2024-11-29 13:53:57.353 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n     \n    The next class to generate is: GermanyCashflowReport\n    The class path is: model_fields/Inputs/GermanyCashflowReport.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:00.239 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:00.252 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:00.252 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.503 | Max budget: $10.000 | Current cost: $0.085, prompt_tokens: 16649, completion_tokens: 150
2024-11-29 13:54:00.323 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/GermanyCashflowReport.py\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n     \n    The next class to generate is: USCashflowReportUpload\n    The class path is: model_fields/Uploads/USCashflowReportUpload.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:06.838 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:06.852 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:06.853 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.594 | Max budget: $10.000 | Current cost: $0.091, prompt_tokens: 16815, completion_tokens: 452
2024-11-29 13:54:06.919 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/GermanyCashflowReport.py\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Uploads/USCashflowReportUpload.py\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n     \n    The next class to generate is: WindparkOperatorOverview\n    The class path is: model_fields/Inputs/WindparkOperatorOverview.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:09.659 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:09.672 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:09.673 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.682 | Max budget: $10.000 | Current cost: $0.088, prompt_tokens: 17279, completion_tokens: 103
2024-11-29 13:54:09.733 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/GermanyCashflowReport.py\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Uploads/USCashflowReportUpload.py\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/WindparkOperatorOverview.py\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n\n     \n    The next class to generate is: ChinaCashflowReportUpload\n    The class path is: model_fields/Uploads/ChinaCashflowReportUpload.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:16.464 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:16.478 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:16.479 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.776 | Max budget: $10.000 | Current cost: $0.094, prompt_tokens: 17398, completion_tokens: 488
2024-11-29 13:54:16.544 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/GermanyCashflowReport.py\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Uploads/USCashflowReportUpload.py\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/WindparkOperatorOverview.py\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n     \n    The next class to generate is: GermanyCashflowReportUpload\n    The class path is: model_fields/Uploads/GermanyCashflowReportUpload.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:24.475 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:24.494 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:24.495 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.874 | Max budget: $10.000 | Current cost: $0.098, prompt_tokens: 17902, completion_tokens: 541
2024-11-29 13:54:24.623 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n    \n    \n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n    \n\n         \n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n    \n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n        \n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n        \n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n        \n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkUpload import WindparkUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n    \n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    \n    \n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n    \n    \n    \n    \n    Already Generated Code: \n        ### model_fields/Inputs/Windpark.py\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n\n\n### model_fields/Reports/CashflowReport.py\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/WindparkUpload.py\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/USCashflowReport.py\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/ChinaCashflowReport.py\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Inputs/GermanyCashflowReport.py\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n\n\n### model_fields/Uploads/USCashflowReportUpload.py\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Inputs/WindparkOperatorOverview.py\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass GermanyCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Germany Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark\', \'Year\', \'Quarter\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the GermanyCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                windpark = Windpark.objects.filter(windpark=row[\'Windpark\']).first()\n                if not windpark:\n                    raise ValueError(f"Windpark \'{row[\'Windpark\']}\' not found.")\n                \n                GermanyCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    windpark=windpark,\n                    defaults={\n                        \'year\': row[\'Year\'],\n                        \'quarter\': row[\'Quarter\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("Germany Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Germany Cashflow Report data: {str(e)}")\n            raise e\n```\n\n\n     \n    The next class to generate is: WindparkOperatorOverviewUpload\n    The class path is: model_fields/Uploads/WindparkOperatorOverviewUpload.py\n\n    \n    Please just regenerate the class and stop according to the test and error code and then stop:\n    \n    [START GENERATING CODE]\n    '}]
2024-11-29 13:54:30.698 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:30.719 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:54:30.719 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.973 | Max budget: $10.000 | Current cost: $0.099, prompt_tokens: 18458, completion_tokens: 446
2024-11-29 13:54:30.847 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 13:54:30.847 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 13:54:30.847 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 13:54:30.847 | WARNING  | metagpt.utils.common:wrapper:663 - There is a exception in role's execution, in order to resume, we delete the newest role communication message in the role's memory.
2024-11-29 13:54:30.856 | WARNING  | metagpt.utils.common:wrapper:663 - There is a exception in role's execution, in order to resume, we delete the newest role communication message in the role's memory.
2024-11-29 13:55:28.592 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 13:55:28.592 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 13:55:28.593 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 13:55:32.512 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 13:55:32.512 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 13:55:32.513 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 13:55:32.564 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    \n\n### Tests/test.py\n```python\n\nfrom django.core.management import call_command\nfrom lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\nfrom lex_app import settings\n\nclass Test(ProcessAdminTestCase):\n    test_path = "DemoWindparkConsolidation/Tests/test_data/test.json"\n       \n    def test(self):\n        pass\n        \n    \n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: test\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 13:55:40.436 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:55:40.449 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:55:40.450 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.082 | Max budget: $10.000 | Current cost: $0.082, prompt_tokens: 14653, completion_tokens: 565
2024-11-29 13:55:40.450 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 13:55:50.805 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 13:55:50.805 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:55:50.805 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:55:50.806 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        \n\n### Tests/test.py\n```python\n\nfrom django.core.management import call_command\nfrom lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\nfrom lex_app import settings\n\nclass Test(ProcessAdminTestCase):\n    test_path = "DemoWindparkConsolidation/Tests/test_data/test.json"\n       \n    def test(self):\n        pass\n        \n    \n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "Windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "USCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "ChinaCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "GermanyCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_1",\n      "file": "DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### Tests/test.py\n```python\n\nfrom django.core.management import call_command\nfrom lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\nfrom lex_app import settings\n\nclass Test(ProcessAdminTestCase):\n    test_path = "DemoWindparkConsolidation/Tests/test_data/test.json"\n       \n    def test(self):\n        pass\n        \n    \n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_testTest.testTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_testTest.testTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 109, in setUp\n    self.tagged_objects[tag].save()\n  File "/usr/lib/python3.10/contextlib.py", line 78, in inner\n    with self._recreate_cm():\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/transaction.py", line 307, in __exit__\n    connection.set_autocommit(True)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 491, in set_autocommit\n    self.run_and_clear_commit_hooks()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 769, in run_and_clear_commit_hooks\n    func()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/decorators.py", line 184, in func\n    hooked_method(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 52, in calculate_hook\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 48, in calculate_hook\n    self.calculate()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 42, in calculate\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 31, in calculate\n    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\nValueError: Windpark Operator \'Deutschland\' not found.\n\n----------------------------------------------------------------------\nRan 1 test in 1.678s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 13:55:56.768 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:55:56.779 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:55:56.780 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.089 | Max budget: $10.000 | Current cost: $0.089, prompt_tokens: 16444, completion_tokens: 425
2024-11-29 13:55:56.780 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:55:56.833 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.Tests.test import test\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        \n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\' not found.` This suggests that the `WindparkOperatorOverview` model does not contain an entry for \'Deutschland\' when the `WindparkUpload` model attempts to reference it.\\n\\n2. **Potential Issue in Code**:\\n   - The `calculate` method in the `WindparkUpload` class is likely not handling the foreign key relationship correctly. Specifically, it is not ensuring that the `WindparkOperatorOverview` entries are created or exist before attempting to reference them.\\n\\n3. **Data Alignment**:\\n   - The fields in the `WindparkUpload` model and the columns in the uploaded file are aligned correctly. The issue is not with the data but with the code logic handling the foreign key relationship.\\n\\n4. **Imports**:\\n   - The imports appear to be correct as per the provided context and specifications.\\n\\n### Detailed Analysis\\n\\n1. **Foreign Key Handling**:\\n   - The `calculate` method in `WindparkUpload` should ensure that the `WindparkOperatorOverview` entries are present before referencing them. This can be done by checking the existence of the `WindparkOperatorOverview` entry for \'Deutschland\' and creating it if it does not exist.\\n\\n2. **Order of Operations**:\\n   - The `WindparkOperatorOverviewUpload` should be processed and saved before the `WindparkUpload` to ensure that all necessary foreign key references are available.\\n\\n3. **Error Handling**:\\n   - The error handling in the `calculate` method should be improved to provide more context-specific information and ensure that the foreign key relationships are validated before proceeding with data insertion.\\n\\n### Conclusion\\n\\nThe primary issue is the lack of validation and creation of foreign key references in the `calculate` method of the `WindparkUpload` class. Ensuring that the `WindparkOperatorOverview` entries are present before referencing them will resolve the error. The order of operations in the test setup should also be verified to ensure that dependencies are correctly handled."}\n\n\n    \n    The class to regenerate is: test\n    The class path is: Tests/test.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 13:56:09.516 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:09.560 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:09.564 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.090 | Max budget: $10.000 | Current cost: $0.090, prompt_tokens: 15330, completion_tokens: 894
2024-11-29 13:56:21.412 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 13:56:21.412 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:56:21.413 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:56:21.413 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        \n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "Windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "USCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "ChinaCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "GermanyCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_1",\n      "file": "DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### Tests/test.py\n\n```python\nfrom django.test import TestCase\nfrom django.core.cache import cache\nfrom django.apps import apps\nfrom django.conf import settings\nimport threading\nimport json\nimport os\nimport inspect\nfrom pathlib import Path\nimport dateutil.parser\n\nclass ProcessAdminTestCase(TestCase):\n    def replace_tagged_parameters(self, object_parameters):\n        for key in object_parameters:\n            value: str = object_parameters[key]\n            if isinstance(value, str):\n                parsed_value = value\n                if value.startswith("tag:"):\n                    parsed_value = self.tagged_objects[value.replace("tag:", "")]\n                elif value.startswith("datetime:"):\n                    parsed_value = dateutil.parser.parse(value.replace("datetime:", ""))\n                object_parameters[key] = parsed_value\n        return object_parameters\n\n    test_path = None\n\n    def setUp(self) -> None:\n        from datetime import datetime\n\n        generic_app_models = {\n            f"{model.__name__}": model for model in\n            set(apps.get_app_config(settings.repo_name).models.values())\n        }\n\n        self.t0 = datetime.now()\n        self.tagged_objects = {}\n\n        test_data = self.get_test_data()\n\n        for object in test_data:\n            klass = generic_app_models[object[\'class\']]\n            action = object[\'action\']\n            tag = object.get(\'tag\', \'instance\')\n\n            if action == \'create\':\n                object[\'parameters\'] = self.replace_tagged_parameters(object[\'parameters\'])\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\n                cache.set(threading.get_ident(), f"{object[\'class\']}_{action}")\n                self.tagged_objects[tag].save()\n\n            elif action == \'update\':\n                object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parameters\'])\n                self.tagged_objects[tag] = klass.objects.filter(**object[\'filter_parameters\']).first()\n                if self.tagged_objects[tag] is not None:\n                    for key in object[\'parameters\']:\n                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\n                    cache.set(\n                        threading.get_ident(),\n                        f"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}"\n                    )\n                    self.tagged_objects[tag].save()\n\n            elif action == \'delete\':\n                klass.objects.filter(**object[\'filter_parameters\']).delete()\n\n    def tearDown(self) -> None:\n        pass\n\n    def get_test_data(self):\n        if self.test_path is None:\n            file = inspect.getfile(self.__class__)\n            path = Path(file).parent\n            clean_test_path = str(path) + os.sep + "test_data.json"\n        else:\n            clean_test_path = self.test_path.replace(\'/\', os.sep)\n            clean_test_path = os.getenv("PROJECT_ROOT") + os.sep + clean_test_path\n\n        test_data = self.get_test_data_from_path(clean_test_path)\n        return test_data\n\n    def get_test_data_from_path(self, path):\n        with open(str(path), \'r\') as f:\n            test_data = json.loads(f.read())\n            for index, object in enumerate(test_data):\n                if "subprocess" in object:\n                    subprocess_path = object[\'subprocess\'].replace(\'/\', os.sep)\n                    subprocess_path = os.getenv("PROJECT_ROOT") + os.sep + subprocess_path\n                    sublist = self.get_test_data_from_path(subprocess_path)\n                    test_data[index] = sublist\n\n        flat_list = []\n        for sublist in test_data:\n            if isinstance(sublist, list):\n                flat_list.extend(sublist)\n            else:\n                flat_list.append(sublist)\n        return flat_list\n\n    def get_classes(self, generic_app_models):\n        test_data = self.get_test_data()\n        return set(generic_app_models[object[\'class\']] for object in test_data)\n\n    def check_if_all_models_are_empty(self, generic_app_models):\n        for klass in self.get_classes(generic_app_models):\n            if klass.objects.all().count() > 0:\n                return False\n        return True\n\n    def get_list_of_non_empty_models(self, generic_app_models):\n        count_of_objects_in_non_empty_models = {}\n        for klass in self.get_classes(generic_app_models):\n            c = klass.objects.all().count()\n            if c > 0:\n                count_of_objects_in_non_empty_models[str(klass)] = c\n        return count_of_objects_in_non_empty_models\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_testTest.testTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_testTest.testTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 109, in setUp\n    self.tagged_objects[tag].save()\n  File "/usr/lib/python3.10/contextlib.py", line 78, in inner\n    with self._recreate_cm():\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/transaction.py", line 307, in __exit__\n    connection.set_autocommit(True)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 491, in set_autocommit\n    self.run_and_clear_commit_hooks()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 769, in run_and_clear_commit_hooks\n    func()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/decorators.py", line 184, in func\n    hooked_method(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 52, in calculate_hook\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 48, in calculate_hook\n    self.calculate()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 42, in calculate\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 31, in calculate\n    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\nValueError: Windpark Operator \'Deutschland\' not found.\n\n----------------------------------------------------------------------\nRan 1 test in 2.328s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 13:56:28.929 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:28.942 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:28.943 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.094 | Max budget: $10.000 | Current cost: $0.094, prompt_tokens: 17196, completion_tokens: 511
2024-11-29 13:56:28.943 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:56:28.999 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.Tests.test import test\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        \n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\' not found.` This suggests that the `WindparkOperatorOverview` model does not contain an entry for \'Deutschland\' when the `WindparkUpload` model attempts to reference it.\\n\\n2. **Potential Issue in Code**:\\n   - The `calculate` method in the `WindparkUpload` class is likely not handling the foreign key relationship correctly. Specifically, it is not ensuring that the `WindparkOperatorOverview` entries are created or exist before attempting to reference them.\\n\\n3. **Data Alignment**:\\n   - The fields in the `WindparkUpload` model and the columns in the uploaded file are aligned correctly. The issue is not with the data but with the code logic handling the foreign key relationship.\\n\\n4. **Imports**:\\n   - The imports appear to be correct as per the provided context and specifications.\\n\\n### Detailed Analysis\\n\\n1. **Foreign Key Handling**:\\n   - The `calculate` method in `WindparkUpload` should ensure that the `WindparkOperatorOverview` entries are present before referencing them. This can be done by checking the existence of the `WindparkOperatorOverview` entry for \'Deutschland\' and creating it if it does not exist.\\n\\n2. **Order of Operations**:\\n   - The `WindparkOperatorOverviewUpload` should be processed and saved before the `WindparkUpload` to ensure that all necessary foreign key references are available.\\n\\n3. **Error Handling**:\\n   - The error handling in the `calculate` method should be improved to provide more context-specific information and ensure that the foreign key relationships are validated before proceeding with data insertion.\\n\\n### Conclusion\\n\\nThe primary issue is the lack of validation and creation of foreign key references in the `calculate` method of the `WindparkUpload` class. Ensuring that the `WindparkOperatorOverview` entries are present before referencing them will resolve the error. The order of operations in the test setup should also be verified to ensure that dependencies are correctly handled."}\n\n------------------Reflection 2---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\'` is not found during the `calculate` method execution in the `WindparkUpload` class.\\n   - This suggests that the `WindparkOperatorOverview` model does not have an entry for \'Deutschland\' when the `WindparkUpload` calculation is performed.\\n\\n2. **Potential Issues**:\\n   - **Data Dependency**: The `WindparkUpload` calculation depends on the existence of `WindparkOperatorOverview` entries. If the `WindparkOperatorOverviewUpload` is not processed before `WindparkUpload`, the required data will be missing.\\n   - **Order of Execution**: The test setup might not ensure that `WindparkOperatorOverviewUpload` is processed before `WindparkUpload`.\\n\\n3. **Model Fields Alignment**:\\n   - The fields in the `Windpark` model and the columns in the uploaded file are aligned correctly as per the provided context.\\n\\n4. **Imports**:\\n   - The imports in the provided test code and class code are correct and align with the context and specifications.\\n\\n### Structured Analysis\\n\\n1. **Data Dependency Issue**:\\n   - Ensure that the `WindparkOperatorOverviewUpload` is processed and saved before the `WindparkUpload` calculation starts. This can be achieved by verifying the order of actions in the test setup.\\n\\n2. **Order of Execution**:\\n   - The test setup should be modified to ensure that `WindparkOperatorOverviewUpload` is created and saved before `WindparkUpload`. This ensures that the necessary `WindparkOperatorOverview` entries exist when `WindparkUpload` is processed.\\n\\n3. **Model Fields Alignment**:\\n   - The fields in the `Windpark` model (`windpark`, `windparkOperator`, `productionMioKwhPerYear`) match the columns in the `Windparks.xlsx` file (`Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`).\\n\\n4. **Imports**:\\n   - The imports in the test code and class code are correct and align with the context and specifications provided.\\n\\n### Conclusion\\n\\nThe primary issue is the order of execution in the test setup. The `WindparkOperatorOverviewUpload` must be processed before the `WindparkUpload` to ensure that the necessary `WindparkOperatorOverview` entries exist. Adjusting the order of actions in the test setup will resolve the error."}\n\n\n    \n    The class to regenerate is: test\n    The class path is: Tests/test.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 13:56:40.324 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:40.337 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:40.338 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.183 | Max budget: $10.000 | Current cost: $0.093, prompt_tokens: 15883, completion_tokens: 894
2024-11-29 13:56:50.930 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 13:56:50.930 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 13:56:50.931 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 13:56:50.931 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        \n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "Windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "USCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "ChinaCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "GermanyCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_1",\n      "file": "DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### Tests/test.py\n```python\nfrom django.test import TestCase\nfrom django.apps import apps\nfrom django.conf import settings\nfrom django.core.cache import cache\nimport threading\nimport json\nimport os\nimport inspect\nfrom pathlib import Path\nimport dateutil.parser\n\nclass ProcessAdminTestCase(TestCase):\n    def replace_tagged_parameters(self, object_parameters):\n        for key in object_parameters:\n            value: str = object_parameters[key]\n            if isinstance(value, str):\n                parsed_value = value\n                if value.startswith("tag:"):\n                    parsed_value = self.tagged_objects[value.replace("tag:", "")]\n                elif value.startswith("datetime:"):\n                    parsed_value = dateutil.parser.parse(value.replace("datetime:", ""))\n                object_parameters[key] = parsed_value\n        return object_parameters\n\n    test_path = None\n\n    def setUp(self) -> None:\n        from datetime import datetime\n\n        generic_app_models = {\n            f"{model.__name__}": model for model in\n            set(apps.get_app_config(settings.repo_name).models.values())\n        }\n\n        self.t0 = datetime.now()\n        self.tagged_objects = {}\n\n        test_data = self.get_test_data()\n\n        for object in test_data:\n            klass = generic_app_models[object[\'class\']]\n            action = object[\'action\']\n            tag = object.get(\'tag\', \'instance\')\n\n            if action == \'create\':\n                object[\'parameters\'] = self.replace_tagged_parameters(object[\'parameters\'])\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\n                cache.set(threading.get_ident(), f"{object[\'class\']}_{action}")\n                self.tagged_objects[tag].save()\n\n            elif action == \'update\':\n                object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parameters\'])\n                self.tagged_objects[tag] = klass.objects.filter(**object[\'filter_parameters\']).first()\n                if self.tagged_objects[tag] is not None:\n                    for key in object[\'parameters\']:\n                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\n                    cache.set(\n                        threading.get_ident(),\n                        f"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}"\n                    )\n                    self.tagged_objects[tag].save()\n\n            elif action == \'delete\':\n                klass.objects.filter(**object[\'filter_parameters\']).delete()\n\n    def tearDown(self) -> None:\n        pass\n\n    def get_test_data(self):\n        if self.test_path is None:\n            file = inspect.getfile(self.__class__)\n            path = Path(file).parent\n            clean_test_path = str(path) + os.sep + "test_data.json"\n        else:\n            clean_test_path = self.test_path.replace(\'/\', os.sep)\n            clean_test_path = os.getenv("PROJECT_ROOT") + os.sep + clean_test_path\n\n        test_data = self.get_test_data_from_path(clean_test_path)\n        return test_data\n\n    def get_test_data_from_path(self, path):\n        with open(str(path), \'r\') as f:\n            test_data = json.loads(f.read())\n            for index, object in enumerate(test_data):\n                if "subprocess" in object:\n                    subprocess_path = object[\'subprocess\'].replace(\'/\', os.sep)\n                    subprocess_path = os.getenv("PROJECT_ROOT") + os.sep + subprocess_path\n                    sublist = self.get_test_data_from_path(subprocess_path)\n                    test_data[index] = sublist\n\n        flat_list = []\n        for sublist in test_data:\n            if isinstance(sublist, list):\n                flat_list.extend(sublist)\n            else:\n                flat_list.append(sublist)\n        return flat_list\n\n    def get_classes(self, generic_app_models):\n        test_data = self.get_test_data()\n        return set(generic_app_models[object[\'class\']] for object in test_data)\n\n    def check_if_all_models_are_empty(self, generic_app_models):\n        for klass in self.get_classes(generic_app_models):\n            if klass.objects.all().count() > 0:\n                return False\n        return True\n\n    def get_list_of_non_empty_models(self, generic_app_models):\n        count_of_objects_in_non_empty_models = {}\n        for klass in self.get_classes(generic_app_models):\n            c = klass.objects.all().count()\n            if c > 0:\n                count_of_objects_in_non_empty_models[str(klass)] = c\n        return count_of_objects_in_non_empty_models\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_testTest.testTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_testTest.testTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 109, in setUp\n    self.tagged_objects[tag].save()\n  File "/usr/lib/python3.10/contextlib.py", line 78, in inner\n    with self._recreate_cm():\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/transaction.py", line 307, in __exit__\n    connection.set_autocommit(True)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 491, in set_autocommit\n    self.run_and_clear_commit_hooks()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/backends/base/base.py", line 769, in run_and_clear_commit_hooks\n    func()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/decorators.py", line 184, in func\n    hooked_method(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 52, in calculate_hook\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/lex_models/CalculationModel.py", line 48, in calculate_hook\n    self.calculate()\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 42, in calculate\n    raise e\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/asgiref/sync.py", line 327, in main_wrap\n    raise exc_info[1]\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/DemoWindparkConsolidation/model_fields/Uploads/WindparkUpload.py", line 31, in calculate\n    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\nValueError: Windpark Operator \'Deutschland\' not found.\n\n----------------------------------------------------------------------\nRan 1 test in 1.476s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 13:56:58.328 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:58.340 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 13:56:58.341 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.094 | Max budget: $10.000 | Current cost: $0.094, prompt_tokens: 17196, completion_tokens: 560
2024-11-29 13:56:58.341 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 13:56:58.411 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.Tests.test import test\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        \n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\' not found.` This suggests that the `WindparkOperatorOverview` model does not contain an entry for \'Deutschland\' when the `WindparkUpload` model attempts to reference it.\\n\\n2. **Potential Issue in Code**:\\n   - The `calculate` method in the `WindparkUpload` class is likely not handling the foreign key relationship correctly. Specifically, it is not ensuring that the `WindparkOperatorOverview` entries are created or exist before attempting to reference them.\\n\\n3. **Data Alignment**:\\n   - The fields in the `WindparkUpload` model and the columns in the uploaded file are aligned correctly. The issue is not with the data but with the code logic handling the foreign key relationship.\\n\\n4. **Imports**:\\n   - The imports appear to be correct as per the provided context and specifications.\\n\\n### Detailed Analysis\\n\\n1. **Foreign Key Handling**:\\n   - The `calculate` method in `WindparkUpload` should ensure that the `WindparkOperatorOverview` entries are present before referencing them. This can be done by checking the existence of the `WindparkOperatorOverview` entry for \'Deutschland\' and creating it if it does not exist.\\n\\n2. **Order of Operations**:\\n   - The `WindparkOperatorOverviewUpload` should be processed and saved before the `WindparkUpload` to ensure that all necessary foreign key references are available.\\n\\n3. **Error Handling**:\\n   - The error handling in the `calculate` method should be improved to provide more context-specific information and ensure that the foreign key relationships are validated before proceeding with data insertion.\\n\\n### Conclusion\\n\\nThe primary issue is the lack of validation and creation of foreign key references in the `calculate` method of the `WindparkUpload` class. Ensuring that the `WindparkOperatorOverview` entries are present before referencing them will resolve the error. The order of operations in the test setup should also be verified to ensure that dependencies are correctly handled."}\n\n------------------Reflection 2---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\'` is not found during the `calculate` method execution in the `WindparkUpload` class.\\n   - This suggests that the `WindparkOperatorOverview` model does not have an entry for \'Deutschland\' when the `WindparkUpload` calculation is performed.\\n\\n2. **Potential Issues**:\\n   - **Data Dependency**: The `WindparkUpload` calculation depends on the existence of `WindparkOperatorOverview` entries. If the `WindparkOperatorOverviewUpload` is not processed before `WindparkUpload`, the required data will be missing.\\n   - **Order of Execution**: The test setup might not ensure that `WindparkOperatorOverviewUpload` is processed before `WindparkUpload`.\\n\\n3. **Model Fields Alignment**:\\n   - The fields in the `Windpark` model and the columns in the uploaded file are aligned correctly as per the provided context.\\n\\n4. **Imports**:\\n   - The imports in the provided test code and class code are correct and align with the context and specifications.\\n\\n### Structured Analysis\\n\\n1. **Data Dependency Issue**:\\n   - Ensure that the `WindparkOperatorOverviewUpload` is processed and saved before the `WindparkUpload` calculation starts. This can be achieved by verifying the order of actions in the test setup.\\n\\n2. **Order of Execution**:\\n   - The test setup should be modified to ensure that `WindparkOperatorOverviewUpload` is created and saved before `WindparkUpload`. This ensures that the necessary `WindparkOperatorOverview` entries exist when `WindparkUpload` is processed.\\n\\n3. **Model Fields Alignment**:\\n   - The fields in the `Windpark` model (`windpark`, `windparkOperator`, `productionMioKwhPerYear`) match the columns in the `Windparks.xlsx` file (`Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`).\\n\\n4. **Imports**:\\n   - The imports in the test code and class code are correct and align with the context and specifications provided.\\n\\n### Conclusion\\n\\nThe primary issue is the order of execution in the test setup. The `WindparkOperatorOverviewUpload` must be processed before the `WindparkUpload` to ensure that the necessary `WindparkOperatorOverview` entries exist. Adjusting the order of actions in the test setup will resolve the error."}\n\n------------------Reflection 3---------------------\n\n{\'analysis\': "### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `Windpark Operator \'Deutschland\' not found.` This suggests that the `WindparkOperatorOverview` model does not contain an entry for \'Deutschland\' when the `WindparkUpload` model attempts to reference it.\\n\\n2. **Data/Files Alignment**:\\n   - The columns in the uploaded file for `Windpark` include `Windpark`, `Windparkbetreiber`, and `Erzeugung [Mio. khW/Jahr]`. The `Windparkbetreiber` column should match the `windparkOperator` field in the `WindparkOperatorOverview` model.\\n   - The `WindparkOperatorOverview` model should have an entry for \'Deutschland\' as per the provided data samples.\\n\\n3. **Imports**:\\n   - The imports appear to be correct and aligned with the context provided.\\n\\n### Detailed Analysis\\n\\n1. **Data Validation**:\\n   - Ensure that the `WindparkOperatorOverview` model is populated with the correct data before the `WindparkUpload` process begins. The error suggests that the `WindparkOperatorOverview` model might not have the \'Deutschland\' entry at the time of the `WindparkUpload`.\\n\\n2. **Order of Operations**:\\n   - The `WindparkOperatorOverviewUpload` should be processed and saved before the `WindparkUpload` to ensure that all necessary foreign key references are available.\\n\\n3. **Foreign Key Relationship**:\\n   - Verify that the `WindparkUpload` model correctly references the `windparkOperator` field in the `WindparkOperatorOverview` model. The error indicates a missing foreign key reference.\\n\\n4. **Test Data Setup**:\\n   - Ensure that the test data setup in the `ProcessAdminTestCase` correctly creates and saves the `WindparkOperatorOverview` entries before attempting to create `Windpark` entries.\\n\\n### Potential Fix\\n\\n1. **Ensure Correct Data Insertion Order**:\\n   - Modify the test setup to ensure that `WindparkOperatorOverviewUpload` entries are created and saved before `WindparkUpload` entries.\\n\\n2. **Check Data Consistency**:\\n   - Verify that the data in the `WindparkOperatorOverview` model includes all necessary entries (e.g., \'Deutschland\') before running the `WindparkUpload` process.\\n\\n3. **Logging and Debugging**:\\n   - Add logging to the `calculate` method of `WindparkUpload` to verify the presence of the `WindparkOperator` entries before attempting to reference them.\\n\\nBy addressing these points, the error can be resolved, ensuring that the `WindparkUpload` process correctly references existing `WindparkOperatorOverview` entries."}\n\n\n    \n    The class to regenerate is: test\n    The class path is: Tests/test.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 14:01:53.459 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 14:01:53.459 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 14:01:53.462 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 14:04:19.651 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:04:19.652 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:04:19.654 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:04:19.828 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkOperatorOverviewUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkOperatorOverviewUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Operator Overview Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windparkbetreiber\', \'number of Windparks\', \'Year\', \'Investment [MM €]\', \'Mitarbeiter\', \'Fläche [m2]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the WindparkOperatorOverview model\n            for _, row in df.iterrows():\n                WindparkOperatorOverview.objects.update_or_create(\n                    windparkOperator=row[\'Windparkbetreiber\'],\n                    defaults={\n                        \'numberOfWindparks\': row[\'number of Windparks\'],\n                        \'year\': row[\'Year\'],\n                        \'investmentMMEur\': row[\'Investment [MM €]\'],\n                        \'employees\': row[\'Mitarbeiter\'],\n                        \'areaM2\': row[\'Fläche [m2]\']\n                    }\n                )\n            \n            logger.add_paragraph("Windpark Operator Overview data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark Operator Overview data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkOperatorOverviewUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:04:22.066 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:04:22.080 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:04:22.081 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.077 | Max budget: $10.000 | Current cost: $0.077, prompt_tokens: 15137, completion_tokens: 100
2024-11-29 14:04:22.081 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:06:27.670 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:06:27.670 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:06:27.673 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:06:27.899 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: USCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:08:55.706 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 14:08:55.707 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 14:08:55.709 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 14:08:59.004 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:08:59.005 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:08:59.005 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:08:59.057 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkOperatorOverviewUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkOperatorOverviewUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Operator Overview Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windparkbetreiber\', \'number of Windparks\', \'Year\', \'Investment [MM €]\', \'Mitarbeiter\', \'Fläche [m2]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the WindparkOperatorOverview model\n            for _, row in df.iterrows():\n                WindparkOperatorOverview.objects.update_or_create(\n                    windparkOperator=row[\'Windparkbetreiber\'],\n                    defaults={\n                        \'numberOfWindparks\': row[\'number of Windparks\'],\n                        \'year\': row[\'Year\'],\n                        \'investmentMMEur\': row[\'Investment [MM €]\'],\n                        \'employees\': row[\'Mitarbeiter\'],\n                        \'areaM2\': row[\'Fläche [m2]\']\n                    }\n                )\n            \n            logger.add_paragraph("Windpark Operator Overview data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark Operator Overview data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkOperatorOverviewUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:09:01.310 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:01.323 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:01.324 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.077 | Max budget: $10.000 | Current cost: $0.077, prompt_tokens: 15137, completion_tokens: 100
2024-11-29 14:09:01.324 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:09:12.531 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:09:12.531 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:09:12.532 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:09:12.600 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: USCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:09:14.671 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:14.683 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:14.684 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15366, completion_tokens: 97
2024-11-29 14:09:14.684 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:09:27.586 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:09:27.586 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:09:27.587 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:09:27.638 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:09:29.849 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:29.860 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:29.860 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15356, completion_tokens: 87
2024-11-29 14:09:29.861 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:09:42.614 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:09:42.614 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:09:42.615 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:09:42.672 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass GermanyCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Germany Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark\', \'Year\', \'Quarter\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the GermanyCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                windpark = Windpark.objects.filter(windpark=row[\'Windpark\']).first()\n                if not windpark:\n                    raise ValueError(f"Windpark \'{row[\'Windpark\']}\' not found.")\n                \n                GermanyCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    windpark=windpark,\n                    defaults={\n                        \'year\': row[\'Year\'],\n                        \'quarter\': row[\'Quarter\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("Germany Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Germany Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "Windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: GermanyCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:09:44.921 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:44.942 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:09:44.942 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 15699, completion_tokens: 98
2024-11-29 14:09:44.943 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:09:58.706 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:09:58.707 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:09:58.707 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:09:58.764 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: ChinaCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:10:00.834 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:00.844 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:00.845 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.079 | Max budget: $10.000 | Current cost: $0.079, prompt_tokens: 15420, completion_tokens: 96
2024-11-29 14:10:00.845 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:10:10.830 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:10:10.830 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:10:10.831 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:10:10.925 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "USCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "GermanyCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "ChinaCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "<class>_<name>_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: CashflowReport\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:10:13.264 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:13.275 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:13.276 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.082 | Max budget: $10.000 | Current cost: $0.082, prompt_tokens: 16112, completion_tokens: 100
2024-11-29 14:10:13.276 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:10:27.704 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 14:10:27.705 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 14:10:27.705 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 14:10:27.705 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_upload_1",\n      "file": "DemoWindparkConsolidation/Tests/output_files/Cashflow_Report.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Generating Cashflow Report", level=2)\n        \n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n            \n            us_cashflows = USCashflowReport.objects.all()\n            china_cashflows = ChinaCashflowReport.objects.all()\n            germany_cashflows = GermanyCashflowReport.objects.all()\n            \n            # Summarize the aggregated data to calculate total cashflows for each windpark operator\n            us_df = pd.DataFrame(list(us_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            china_df = pd.DataFrame(list(china_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            germany_df = pd.DataFrame(list(germany_cashflows.values(\'windparkOperator__windparkOperator\', \'cashflow\')))\n            \n            us_summary = us_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            china_summary = china_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            germany_summary = germany_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            \n            # Merge the summaries\n            summary_df = pd.concat([us_summary, china_summary, germany_summary])\n            summary_df = summary_df.groupby(\'windparkOperator__windparkOperator\').sum().reset_index()\n            summary_df.columns = [\'Windpark Operator\', \'Cashflow Sum\']\n            \n            # Generate a consolidated cashflow report\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [summary_df], sheet_names=[\'Cashflow Report\'])\n            \n            logger.add_paragraph("Cashflow report generated successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 107, in setUp\n    self.tagged_objects[tag] = klass(**object[\'parameters\'])\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/mixins.py", line 82, in __init__\n    super().__init__(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/base.py", line 543, in __init__\n    _setattr(self, field.name, rel_obj)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/fields/related_descriptors.py", line 287, in __set__\n    raise ValueError(\nValueError: Cannot assign "<WindparkOperatorOverviewUpload: WindparkOperatorOverviewUpload object (1)>": "CashflowReport.windparkOperator" must be a "WindparkOperatorOverview" instance.\n\n----------------------------------------------------------------------\nRan 1 test in 5.173s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 14:10:32.651 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:32.664 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:32.664 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.091 | Max budget: $10.000 | Current cost: $0.091, prompt_tokens: 17196, completion_tokens: 339
2024-11-29 14:10:32.665 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 14:10:32.721 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': \'### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `windparkOperator` field in the `CashflowReport` model is being assigned an instance of `WindparkOperatorOverviewUpload` instead of `WindparkOperatorOverview`.\\n\\n2. **Detailed Analysis**:\\n   - The `windparkOperator` field in the `CashflowReport` model expects an instance of `WindparkOperatorOverview`.\\n   - The test setup is incorrectly assigning a `WindparkOperatorOverviewUpload` instance to this field.\\n   - This mismatch occurs because the test data uses a tag (`tag:windpark_operator_overview_upload_1`) that refers to an upload model instance instead of the actual model instance.\\n\\n3. **Potential Fix**:\\n   - Ensure that the test setup assigns the correct instance type to the `windparkOperator` field.\\n   - Modify the test data to use a tag that refers to a `WindparkOperatorOverview` instance, not `WindparkOperatorOverviewUpload`.\\n\\n4. **Model Fields Alignment**:\\n   - The model fields are aligned with the columns in the data files as per the provided context and specifications.\\n\\n5. **Imports**:\\n   - The imports in the provided code are correct and align with the project structure and context.\\n\\n### Summary\\n- The error is due to assigning an incorrect instance type (`WindparkOperatorOverviewUpload` instead of `WindparkOperatorOverview`) to the `windparkOperator` field in the `CashflowReport` model.\\n- The test data should be corrected to ensure the proper instance type is assigned.\\n- The model fields and imports are correctly aligned with the project specifications and data files.\'}\n\n\n    \n    The class to regenerate is: CashflowReport\n    The class path is: model_fields/Reports/CashflowReport.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 14:10:42.802 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:42.815 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:10:42.816 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.088 | Max budget: $10.000 | Current cost: $0.088, prompt_tokens: 15863, completion_tokens: 563
2024-11-29 14:10:58.124 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 14:10:58.124 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 14:10:58.125 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 14:10:58.125 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_upload_1",\n      "file": "DemoWindparkConsolidation/Tests/output_files/Cashflow_Report.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 107, in setUp\n    self.tagged_objects[tag] = klass(**object[\'parameters\'])\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/mixins.py", line 82, in __init__\n    super().__init__(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/base.py", line 543, in __init__\n    _setattr(self, field.name, rel_obj)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/fields/related_descriptors.py", line 287, in __set__\n    raise ValueError(\nValueError: Cannot assign "<WindparkOperatorOverviewUpload: WindparkOperatorOverviewUpload object (1)>": "CashflowReport.windparkOperator" must be a "WindparkOperatorOverview" instance.\n\n----------------------------------------------------------------------\nRan 1 test in 5.727s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 14:15:25.654 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 14:15:25.654 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 14:15:25.654 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 14:15:28.326 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:15:28.326 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:15:28.327 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:15:28.376 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkOperatorOverviewUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkOperatorOverviewUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Operator Overview Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windparkbetreiber\', \'number of Windparks\', \'Year\', \'Investment [MM €]\', \'Mitarbeiter\', \'Fläche [m2]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the WindparkOperatorOverview model\n            for _, row in df.iterrows():\n                WindparkOperatorOverview.objects.update_or_create(\n                    windparkOperator=row[\'Windparkbetreiber\'],\n                    defaults={\n                        \'numberOfWindparks\': row[\'number of Windparks\'],\n                        \'year\': row[\'Year\'],\n                        \'investmentMMEur\': row[\'Investment [MM €]\'],\n                        \'employees\': row[\'Mitarbeiter\'],\n                        \'areaM2\': row[\'Fläche [m2]\']\n                    }\n                )\n            \n            logger.add_paragraph("Windpark Operator Overview data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark Operator Overview data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkOperatorOverviewUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:15:30.637 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:30.658 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:30.659 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.077 | Max budget: $10.000 | Current cost: $0.077, prompt_tokens: 15157, completion_tokens: 100
2024-11-29 14:15:30.659 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:15:41.538 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:15:41.538 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:15:41.539 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:15:41.592 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: USCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:15:43.674 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:43.686 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:43.686 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15386, completion_tokens: 97
2024-11-29 14:15:43.686 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:15:56.064 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:15:56.064 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:15:56.065 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:15:56.116 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:15:58.390 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:58.401 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:15:58.401 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15376, completion_tokens: 87
2024-11-29 14:15:58.401 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:16:11.590 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:16:11.590 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:16:11.591 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:16:11.654 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass GermanyCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Germany Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark\', \'Year\', \'Quarter\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the GermanyCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                windpark = Windpark.objects.filter(windpark=row[\'Windpark\']).first()\n                if not windpark:\n                    raise ValueError(f"Windpark \'{row[\'Windpark\']}\' not found.")\n                \n                GermanyCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    windpark=windpark,\n                    defaults={\n                        \'year\': row[\'Year\'],\n                        \'quarter\': row[\'Quarter\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("Germany Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Germany Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "Windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: GermanyCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:16:13.937 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:13.948 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:13.949 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 15719, completion_tokens: 98
2024-11-29 14:16:13.949 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:16:28.398 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:16:28.398 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:16:28.399 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:16:28.448 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: ChinaCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:16:30.938 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:30.954 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:30.954 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.079 | Max budget: $10.000 | Current cost: $0.079, prompt_tokens: 15440, completion_tokens: 96
2024-11-29 14:16:30.955 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:16:40.981 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:16:40.981 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:16:40.982 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:16:41.066 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "USCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "GermanyCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "WindparkOperatorOverview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "ChinaCashflowReport",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<Class>" // Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<Class2>" // Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: CashflowReport\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:16:57.166 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:57.177 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:16:57.178 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.082 | Max budget: $10.000 | Current cost: $0.082, prompt_tokens: 16062, completion_tokens: 109
2024-11-29 14:16:57.178 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:17:11.400 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 14:17:11.400 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 14:17:11.401 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 14:17:11.401 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "type": "Report",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_upload_1",\n      "file": "DemoWindparkConsolidation/Tests/output_files/Cashflow_Report_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 107, in setUp\n    self.tagged_objects[tag] = klass(**object[\'parameters\'])\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/mixins.py", line 82, in __init__\n    super().__init__(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/base.py", line 543, in __init__\n    _setattr(self, field.name, rel_obj)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/fields/related_descriptors.py", line 287, in __set__\n    raise ValueError(\nValueError: Cannot assign "<WindparkOperatorOverviewUpload: WindparkOperatorOverviewUpload object (1)>": "CashflowReport.windparkOperator" must be a "WindparkOperatorOverview" instance.\n\n----------------------------------------------------------------------\nRan 1 test in 5.670s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 14:19:12.326 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 14:19:12.326 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 14:19:12.327 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 14:19:15.577 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:19:15.577 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:19:15.578 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:19:15.629 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkOperatorOverviewUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkOperatorOverviewUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Operator Overview Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windparkbetreiber\', \'number of Windparks\', \'Year\', \'Investment [MM €]\', \'Mitarbeiter\', \'Fläche [m2]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the WindparkOperatorOverview model\n            for _, row in df.iterrows():\n                WindparkOperatorOverview.objects.update_or_create(\n                    windparkOperator=row[\'Windparkbetreiber\'],\n                    defaults={\n                        \'numberOfWindparks\': row[\'number of Windparks\'],\n                        \'year\': row[\'Year\'],\n                        \'investmentMMEur\': row[\'Investment [MM €]\'],\n                        \'employees\': row[\'Mitarbeiter\'],\n                        \'areaM2\': row[\'Fläche [m2]\']\n                    }\n                )\n            \n            logger.add_paragraph("Windpark Operator Overview data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark Operator Overview data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkOperatorOverviewUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:19:17.251 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:17.264 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:17.264 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.077 | Max budget: $10.000 | Current cost: $0.077, prompt_tokens: 15174, completion_tokens: 101
2024-11-29 14:19:17.264 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:19:28.129 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:19:28.129 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:19:28.130 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:19:28.184 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: USCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:19:29.885 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:29.895 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:29.896 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15404, completion_tokens: 97
2024-11-29 14:19:29.896 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:19:42.237 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:19:42.237 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:19:42.237 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:19:42.289 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:19:44.285 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:44.296 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:44.296 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15394, completion_tokens: 87
2024-11-29 14:19:44.296 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:19:57.448 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:19:57.448 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:19:57.449 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:19:57.499 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass GermanyCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Germany Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark\', \'Year\', \'Quarter\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the GermanyCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                windpark = Windpark.objects.filter(windpark=row[\'Windpark\']).first()\n                if not windpark:\n                    raise ValueError(f"Windpark \'{row[\'Windpark\']}\' not found.")\n                \n                GermanyCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    windpark=windpark,\n                    defaults={\n                        \'year\': row[\'Year\'],\n                        \'quarter\': row[\'Quarter\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("Germany Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Germany Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: GermanyCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:19:59.818 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:59.829 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:19:59.829 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 15737, completion_tokens: 99
2024-11-29 14:19:59.829 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:20:13.693 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:20:13.693 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:20:13.694 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:20:13.755 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: ChinaCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:20:15.377 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:15.388 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:15.388 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.079 | Max budget: $10.000 | Current cost: $0.079, prompt_tokens: 15458, completion_tokens: 96
2024-11-29 14:20:15.388 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:20:25.204 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:20:25.204 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:20:25.205 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:20:25.274 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "us_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "germany_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "china_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4",\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: CashflowReport\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:20:27.812 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:27.823 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:27.824 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.082 | Max budget: $10.000 | Current cost: $0.082, prompt_tokens: 16081, completion_tokens: 109
2024-11-29 14:20:27.824 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:20:41.949 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 14:20:41.949 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 14:20:41.950 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 14:20:41.950 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "type": "Report",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_upload_1",\n      "file": "DemoWindparkConsolidation/Tests/output_files/Cashflow_Report_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined data to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 107, in setUp\n    self.tagged_objects[tag] = klass(**object[\'parameters\'])\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/mixins.py", line 82, in __init__\n    super().__init__(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/base.py", line 543, in __init__\n    _setattr(self, field.name, rel_obj)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/fields/related_descriptors.py", line 287, in __set__\n    raise ValueError(\nValueError: Cannot assign "<WindparkOperatorOverviewUpload: WindparkOperatorOverviewUpload object (1)>": "CashflowReport.windparkOperator" must be a "WindparkOperatorOverview" instance.\n\n----------------------------------------------------------------------\nRan 1 test in 5.290s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 14:20:48.105 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:48.118 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:48.119 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.090 | Max budget: $10.000 | Current cost: $0.090, prompt_tokens: 17065, completion_tokens: 323
2024-11-29 14:20:48.119 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 14:20:48.189 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': \'### Error Analysis\\n\\n1. **Root Cause**: The error occurs because the `windparkOperator` field in the `CashflowReport` model expects an instance of `WindparkOperatorOverview`, but the test code is assigning an instance of `WindparkOperatorOverviewUpload`.\\n\\n2. **Detailed Issue**: \\n   - The `windparkOperator` field in the `CashflowReport` model is defined as a ForeignKey to `WindparkOperatorOverview`.\\n   - In the test setup, the `windparkOperator` parameter is being assigned a tagged object `windpark_operator_overview_upload_1`, which is an instance of `WindparkOperatorOverviewUpload` instead of `WindparkOperatorOverview`.\\n\\n3. **Potential Fix**: \\n   - Ensure that the `windparkOperator` field in the test setup is assigned an instance of `WindparkOperatorOverview` and not `WindparkOperatorOverviewUpload`.\\n   - This can be achieved by correctly tagging and referencing the appropriate instance in the test data setup.\\n\\n4. **Model Fields Alignment**: The model fields are aligned with the columns in the data files as per the provided context and specifications.\\n\\n5. **Imports**: The imports in the provided code are correct and align with the project structure and context.\\n\\n### Summary\\n- The error is due to assigning the wrong instance type to the `windparkOperator` field in the `CashflowReport` model.\\n- Ensure the test setup assigns an instance of `WindparkOperatorOverview` to the `windparkOperator` field.\\n- The model fields and imports are correct as per the provided context and specifications.\'}\n\n\n    \n    The class to regenerate is: CashflowReport\n    The class path is: model_fields/Reports/CashflowReport.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 14:20:55.927 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:55.940 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:20:55.941 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.088 | Max budget: $10.000 | Current cost: $0.088, prompt_tokens: 15844, completion_tokens: 563
2024-11-29 14:21:10.360 | DEBUG    | metagpt.roles.role:_observe:443 - LLM(Gets the result of the LLM output) observed: ['user: \n        [ORIGINAL S...']
2024-11-29 14:21:10.360 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=0
2024-11-29 14:21:10.361 | DEBUG    | metagpt.roles.role:_react:474 - LLM(Gets the result of the LLM output): self.rc.state=0, will do AskLLM
2024-11-29 14:21:10.361 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Gets the result of the LLM output, named LLM, your goal is . '}, {'role': 'user', 'content': '\n        [ORIGINAL SPEFICIATION]\n        \n       Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n \n    \n        \n        [CONTEXT]\n        \n\n        Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.update_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.update_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.update_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n    \n        \n        [CODE CONTEXT]\n        ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n        \n        \n        [FILES AND THEIR COLUMNS]\n        {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n        \n        Analyze the following test error and provide insights:\n        \n        [ERROR ANALYSIS]\n        Test Code: [\n  {\n    "class": "CashflowReport",\n    "action": "create",\n    "tag": "cashflow_report_1",\n    "type": "Report",\n    "parameters": {\n      "cashflowSum": 0.0,\n      "windparkOperator": "tag:windpark_operator_overview_upload_1",\n      "file": "DemoWindparkConsolidation/Tests/output_files/Cashflow_Report_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n        Class Code: ### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined DataFrame to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n```\n        Error Message: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest) ... ERROR\n\n======================================================================\nERROR: test (DemoWindparkConsolidation.Tests.test_CashflowReportTest.CashflowReportTest)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/src/lex-app/lex/lex_app/tests/ProcessAdminTestCase.py", line 107, in setUp\n    self.tagged_objects[tag] = klass(**object[\'parameters\'])\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django_lifecycle/mixins.py", line 82, in __init__\n    super().__init__(*args, **kwargs)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/base.py", line 543, in __init__\n    _setattr(self, field.name, rel_obj)\n  File "/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/venv/lib/python3.10/site-packages/django/db/models/fields/related_descriptors.py", line 287, in __set__\n    raise ValueError(\nValueError: Cannot assign "<WindparkOperatorOverviewUpload: WindparkOperatorOverviewUpload object (1)>": "CashflowReport.windparkOperator" must be a "WindparkOperatorOverview" instance.\n\n----------------------------------------------------------------------\nRan 1 test in 5.202s\n\nFAILED (errors=1)\n\n        Corrected Code (Not tested yet):     \n\n        Please analyze:\n        0. The data/files are always correct (columns/rows) and the error is always due to the code.\n        1. What is the root cause of this error assuming the code is incorrect somewhere?\n        2. Don\'t provide any code, just give a comprehensive analysis of the error and the potential fix (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        3. Be structured and detailed and don\'t give generic answers (Adhering to the **Context** and **Original Prompt\'s Specification**)\n        4. Keep it really short and direct (Write the reason and where is the problem in the code)\n        \n        Questions to anseer:\n        1. Are the models field aligned with the columns in the data in the uploaded file (According to [FILES AND THEIR COLUMNS]? (If it\'s a data upload error)\n        2. Are imports correct ?\n\n        Provide a structured analysis of maximum 10 sentences that can guide code regeneration.\n        '}]
2024-11-29 14:21:16.249 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:21:16.284 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:21:16.285 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.088 | Max budget: $10.000 | Current cost: $0.088, prompt_tokens: 16501, completion_tokens: 372
2024-11-29 14:21:16.287 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[AskLLM], state=-1
2024-11-29 14:21:16.395 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': '** [YOU ARE A SOFTWARE ENGINEER AND YOU ARE TASKED TO COMPLETE EVERYTHING IN THE PROJECT] **'}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n\n        Usage example:\n            ```\n            from <ProjectName>.<Folder>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n            ClassModel(LexModel):\n                # Implement fields here\n                # Foreign key relationship should be used according to the data sample from: **Project Input and Output Files**\n                classId = models.ForeignKey(ClassModel2, on_delete=models.CASCADE)\n                # Example of wrong foreign key:  models.ForeignKey(\'<ProjectName>.<Folder>.ClassModel2\', on_delete=models.CASCADE) Write only the class name\n                # Example of wrong foreign key:  models.ForeignKey(\'ClassModel2\', on_delete=models.CASCADE) It should be the class itself not a string\n                \n                # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                #    - Do not use: ModelClass.objects.get_or_create()\n                #    - Do not use: ModelClass.objects.get()\n                #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                \n            ```\n            ClassModelUpload(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file upload (Mandatory field in every CalculationModel)\n                \n                def calculate(self):\n                    logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n                    logger.add_heading("ClassModel Data Upload", level=2)\n                    try:\n                        # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                        # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                        #    - Do not use: ModelClass.objects.get_or_create()\n                        #    - Do not use: ModelClass.objects.get()\n                        #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    except Exception as e:\n                        logger.add_paragraph(f"Error processing ClassModel data: str(e)")\n                        raise e\n                        \n            ```\n            \n            ```\n            OutputReport(CalculationModel):\n                # Implement fields here\n                # XLSXField here for file report (Please don\'t forget this field in every CalculationModel)\n                \n                def calculate(self):\n                    # logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True) \n                    # Implement the logic here (Should always be implemented, never forget the logic anywhere)\n                \n                    # DO NOT USE THE FOLLOWING FUNCTIONS FROM ModelClass.objects (because if they are used with a Non-Unique key-value they return more than one object and it will cause an error):\n                    #    - Do not use: ModelClass.objects.get_or_create()\n                    #    - Do not use: ModelClass.objects.get()\n                    #    for example instead you can use ModelClass.objects.filter(<key>=<Value>).first() and to check wheter the object is created or not you can use if not ModelClass.objects.filter(<key>=<Value>).exists():\n                    \n                    from <ProjectName>.<Folder>.ClassModel import ClassModel # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class_model_objects = ClassModel.objects.all() # Example of using the foreign key relationship (ClassModel should therefor be imported)\n                    \n                    from <ProjectName>.<Folder2>.ClassModel2 import ClassModel2 # Since there is a foreign key relationship (or it\'s used somewhere)\n                    class2_model_objects = ClassModel2.objects.all() # Example of using the foreign key relationship (ClassModel2 should therefor be imported)\n                    \n            ``` \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n\n    Import Pool (Double check the implementation against the imports at the end):\n    Inner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.USCashflowReportUpload import USCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.ChinaCashflowReportUpload import ChinaCashflowReportUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.WindparkOperatorOverviewUpload import WindparkOperatorOverviewUpload\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Reports.CashflowReport import CashflowReport\nInner ImporPath: from DemoWindparkConsolidation.model_fields.Uploads.GermanyCashflowReportUpload import GermanyCashflowReportUpload\nLex App Import: from lex.lex_app.rest_api.fields.XLSX_field\nLex App Import: from lex.lex_app.LexLogger.LexLogLevel\nLex App Import: from lex.lex_app.LexLogger.LexLogger import LexLogger\nLex App Import: from lex.lex_app.lex_models.LexModel import LexModel\nLex App Import: from lex.lex_app.lex_models.CalculationModel import CalculationModel\nExternal ImportPath: from django.db import models\nExternal ImportPath: import pandas as pd\nExternal ImportPath: import numpy as np \n\n\n    Project Requirement: \n        1. Write a project which is explained in the given context as project description\n2. Read and understand the lex-app library source code which is again included fully inside the given context.\n3. In cases where you have to extend Django models class, instead you have to extend the lex-app library LexModel class.\n4. DO NOT forget to import everything necessary for the code you will write and everytime you change something double check this requirement.\n5. In the places which requires business logic calculations, use the lex-app library CalculationModel class where you extend the calculate method.\n6. In the places which requires uploads or downloads, use the lex-app library CalculationModel class where you extend the calculate method because it also works for post upload operations and to create the files will be downloaded.\n7. When an excel creation or update is needed, use the lex-app library XLSXField and its create_excel_file_from_dfs function. (Only one line instruction)\n    ```python\n        output_path = \'reports/<Report>.xlsx\'\n        XLSXField.create_excel_file_from_dfs(self.file_field, output_path, [df], sheet_names=[\'<ExampleColumnName>\'])\n    ```\n8. In the case of Excel FileField usage for any Django Model classes you will write, use the lex-app library XLSXField class.\n9. Put logs in meaningful places in your code such as calculations and use the lex-app library MarkdownBuilder class from the LexLogger class.\n10. Add primary keys to the Django Model classes you will create in below format:\n     id = models.AutoField(primary_key=True)\n11. Use ForeignKey fields for the relationships between the models when it is applicable according to the project design and PLEASE USE THE CORRECT COLUMN NAMES FROM DATA SAMPLE DON"T USE SOMETHING THAT DOESN\'T EXIST.\n12. For every code file, write relative file paths which includes meaningful folders in this format and remember the path for imports later\n    ### <example_dir1>/<example_dir2>/<example_file>.py\n    ```python\n13. Don\'t forget to implement the necessary buisness logic in the calculate method of the CalculationModel class.\n14. LexLogger shall only be used for CalculationModel classes and CalculationModel class\'s calculate\n15. Double check the imports from the lex_app context or if you use any foreign key relationship\n\n \n\n\n\n    Generation requirement:\n        Before starting to generate the code, please read the following requirements:\n            1. Only generate the next class and then stop generating.\n            2. Use foreign key relationship for according to the data sample from: **Project Input and Output Files**\n            2. No class Meta is allowed\n            3. Don\'t use self.is_calculated or any implementation detail of LexApp class\n            4. Use imports <Projectname>.<InBetweenFolders>.<class_name> or <Projectname>.<InBetweenFolders>.<function_name>\n            5. Implement every method\n            6. Use python convention for class names\n            7. ONLY USE COLUMN NAMES FROM THE FILES INPUT AND OUTPUT CONTENT (THIS IS EXTREMELY IMPORTANT)\n            8. You will get the folder hierarchy and the file names from the project structure (KEEP THAT IN MIND!!)\n            9. start with and no other than ### path/to/class.py\nclass ClassName:\n            10. Use import pool for importing classes of the project\n            11. Calculation logic should be filled and implemented even if not provided in the project structure.\n\n    \n    \n\n    **Already Generated Code Context**: \n        ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n    Reflection Context: \n    ------------------Reflection 1---------------------\n\n{\'analysis\': \'### Error Analysis\\n\\n1. **Root Cause**: The error occurs because the `windparkOperator` field in the `CashflowReport` model expects an instance of `WindparkOperatorOverview`, but the test code is assigning an instance of `WindparkOperatorOverviewUpload`.\\n\\n2. **Detailed Issue**: \\n   - The `windparkOperator` field in the `CashflowReport` model is defined as a ForeignKey to `WindparkOperatorOverview`.\\n   - In the test setup, the `windparkOperator` parameter is being assigned a tagged object `windpark_operator_overview_upload_1`, which is an instance of `WindparkOperatorOverviewUpload` instead of `WindparkOperatorOverview`.\\n\\n3. **Potential Fix**: \\n   - Ensure that the `windparkOperator` field in the test setup is assigned an instance of `WindparkOperatorOverview` and not `WindparkOperatorOverviewUpload`.\\n   - This can be achieved by correctly tagging and referencing the appropriate instance in the test data setup.\\n\\n4. **Model Fields Alignment**: The model fields are aligned with the columns in the data files as per the provided context and specifications.\\n\\n5. **Imports**: The imports in the provided code are correct and align with the project structure and context.\\n\\n### Summary\\n- The error is due to assigning the wrong instance type to the `windparkOperator` field in the `CashflowReport` model.\\n- Ensure the test setup assigns an instance of `WindparkOperatorOverview` to the `windparkOperator` field.\\n- The model fields and imports are correct as per the provided context and specifications.\'}\n\n------------------Reflection 2---------------------\n\n{\'analysis\': \'### Error Analysis\\n\\n1. **Root Cause**:\\n   - The error message indicates that the `windparkOperator` field in the `CashflowReport` model is being assigned an instance of `WindparkOperatorOverviewUpload` instead of `WindparkOperatorOverview`.\\n   - This mismatch occurs because the test setup is using a tagged object (`windpark_operator_overview_upload_1`) which is an instance of `WindparkOperatorOverviewUpload`, not `WindparkOperatorOverview`.\\n\\n2. **Potential Fix**:\\n   - Ensure that the `windparkOperator` field in the `CashflowReport` model is assigned an instance of `WindparkOperatorOverview`.\\n   - Modify the test setup to correctly reference an instance of `WindparkOperatorOverview` instead of `WindparkOperatorOverviewUpload`.\\n\\n### Structured Analysis\\n\\n1. **Model Field Alignment**:\\n   - The model fields are aligned with the columns in the data files as per the provided context and specifications.\\n\\n2. **Imports**:\\n   - The imports in the provided code are correct and align with the project structure and context.\\n\\n### Detailed Steps to Fix\\n\\n1. **Test Setup Correction**:\\n   - Update the test data to ensure that the `windparkOperator` field in the `CashflowReport` model is assigned a valid `WindparkOperatorOverview` instance.\\n\\n2. **Code Review**:\\n   - Review the test setup in `ProcessAdminTestCase` to ensure that the correct instances are being created and assigned.\\n\\n3. **Validation**:\\n   - Validate that all foreign key assignments in the test setup are using the correct model instances.\\n\\nBy addressing the assignment issue in the test setup, the error can be resolved, ensuring that the `windparkOperator` field in the `CashflowReport` model is correctly assigned an instance of `WindparkOperatorOverview`.\'}\n\n\n    \n    The class to regenerate is: CashflowReport\n    The class path is: model_fields/Reports/CashflowReport.py\n\n    Please just regenerate the class and stop according to the test and error code and then stop:\n\n    [START REGENERATING CODE]\n    '}]
2024-11-29 14:35:08.075 | DEBUG    | metagpt.roles.role:_observe:443 - CodeGenerator(Expert in generating code based on architecture and specifications) observed: ['user: START...']
2024-11-29 14:35:08.075 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=0
2024-11-29 14:35:08.076 | DEBUG    | metagpt.roles.role:_react:474 - CodeGenerator(Expert in generating code based on architecture and specifications): self.rc.state=0, will do GenerateCode
2024-11-29 14:35:10.667 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:35:10.667 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:35:10.667 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:35:10.726 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/WindparkOperatorOverviewUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkOperatorOverviewUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Operator Overview Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windparkbetreiber\', \'number of Windparks\', \'Year\', \'Investment [MM €]\', \'Mitarbeiter\', \'Fläche [m2]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the WindparkOperatorOverview model\n            for _, row in df.iterrows():\n                WindparkOperatorOverview.objects.update_or_create(\n                    windparkOperator=row[\'Windparkbetreiber\'],\n                    defaults={\n                        \'numberOfWindparks\': row[\'number of Windparks\'],\n                        \'year\': row[\'Year\'],\n                        \'investmentMMEur\': row[\'Investment [MM €]\'],\n                        \'employees\': row[\'Mitarbeiter\'],\n                        \'areaM2\': row[\'Fläche [m2]\']\n                    }\n                )\n            \n            logger.add_paragraph("Windpark Operator Overview data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark Operator Overview data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    \n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkOperatorOverviewUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:35:12.895 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:12.907 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:12.908 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15202, completion_tokens: 101
2024-11-29 14:35:12.908 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:35:24.351 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:35:24.352 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:35:24.352 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:35:24.408 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/USCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass USCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("US Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Year\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the USCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                USCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'year\': row[\'Year\'], \'cashflow\': row[\'Cashflow\']}\n                )\n            \n            logger.add_paragraph("US Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing US Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: USCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:35:31.893 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:31.907 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:31.907 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.079 | Max budget: $10.000 | Current cost: $0.079, prompt_tokens: 15432, completion_tokens: 97
2024-11-29 14:35:31.907 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:35:44.988 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:35:44.988 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:35:44.988 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:35:45.071 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n\n### model_fields/Uploads/WindparkUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass WindparkUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Windpark Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'Windpark\', \'Windparkbetreiber\', \'Erzeugung [Mio. khW/Jahr]\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the Windpark model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                Windpark.objects.update_or_create(\n                    windpark=row[\'Windpark\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\'productionMioKwhPerYear\': row[\'Erzeugung [Mio. khW/Jahr]\']}\n                )\n            \n            logger.add_paragraph("Windpark data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Windpark data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: WindparkUpload\n    \n    \n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:35:47.364 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:47.384 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:35:47.385 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.078 | Max budget: $10.000 | Current cost: $0.078, prompt_tokens: 15422, completion_tokens: 87
2024-11-29 14:35:47.386 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:36:01.554 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:36:01.554 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:36:01.554 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:36:01.618 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/Windpark.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass Windpark(LexModel):\n    id = models.AutoField(primary_key=True)\n    windpark = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n    productionMioKwhPerYear = models.FloatField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Uploads/GermanyCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass GermanyCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Germany Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark\', \'Year\', \'Quarter\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the GermanyCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                windpark = Windpark.objects.filter(windpark=row[\'Windpark\']).first()\n                if not windpark:\n                    raise ValueError(f"Windpark \'{row[\'Windpark\']}\' not found.")\n                \n                GermanyCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    windpark=windpark,\n                    defaults={\n                        \'year\': row[\'Year\'],\n                        \'quarter\': row[\'Quarter\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("Germany Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing Germany Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkUpload",\n    "action": "create",\n    "tag": "windpark_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparks.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: GermanyCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:36:03.779 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:03.790 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:03.791 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.080 | Max budget: $10.000 | Current cost: $0.080, prompt_tokens: 15765, completion_tokens: 99
2024-11-29 14:36:03.791 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:36:19.048 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:36:19.048 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:36:19.049 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:36:19.120 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n\n### model_fields/Uploads/ChinaCashflowReportUpload.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\nfrom DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\n\nclass ChinaCashflowReportUpload(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("China Cashflow Report Data Upload", level=2)\n        \n        try:\n            # Read the uploaded file\n            df = pd.read_excel(self.file.path)\n            \n            # Validate the required columns\n            required_columns = [\'ID\', \'Windparkbetreiber\', \'Windpark Name\', \'Datum\', \'Jahr\', \'Cashflow\']\n            if not all(column in df.columns for column in required_columns):\n                raise ValueError(f"Missing required columns: {\', \'.join(set(required_columns) - set(df.columns))}")\n            \n            # Iterate over the DataFrame rows and insert data into the ChinaCashflowReport model\n            for _, row in df.iterrows():\n                windpark_operator = WindparkOperatorOverview.objects.filter(windparkOperator=row[\'Windparkbetreiber\']).first()\n                if not windpark_operator:\n                    raise ValueError(f"Windpark Operator \'{row[\'Windparkbetreiber\']}\' not found.")\n                \n                ChinaCashflowReport.objects.update_or_create(\n                    id=row[\'ID\'],\n                    windparkOperator=windpark_operator,\n                    defaults={\n                        \'windparkName\': row[\'Windpark Name\'],\n                        \'date\': row[\'Datum\'],\n                        \'year\': row[\'Jahr\'],\n                        \'cashflow\': row[\'Cashflow\']\n                    }\n                )\n            \n            logger.add_paragraph("China Cashflow Report data uploaded successfully.")\n        except Exception as e:\n            logger.add_paragraph(f"Error processing China Cashflow Report data: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: ChinaCashflowReportUpload\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:36:21.293 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:21.309 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:21.310 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.079 | Max budget: $10.000 | Current cost: $0.079, prompt_tokens: 15486, completion_tokens: 96
2024-11-29 14:36:21.310 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:36:32.586 | DEBUG    | metagpt.roles.role:_observe:443 - GenerateJsonRole(Expert in generating test jsons) observed: ['user: START...']
2024-11-29 14:36:32.587 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=0
2024-11-29 14:36:32.588 | DEBUG    | metagpt.roles.role:_react:474 - GenerateJsonRole(Expert in generating test jsons): self.rc.state=0, will do GenerateJson
2024-11-29 14:36:32.666 | DEBUG    | metagpt.provider.base_llm:aask:151 - [{'role': 'system', 'content': 'You are a Expert in generating test jsons, named GenerateJsonRole, your goal is . '}, {'role': 'user', 'content': '\n    Lex App Context:\n        Key classes and their Required Imports:\n            CalculationModel:\n  Import Path: from lex.lex_app.lex_models.CalculationModel import CalculationModel\n  LexModel:\n    Import Path: from lex.lex_app.lex_models.LexModel import LexModel\n    source code: "class LexModel(LifecycleModel):\\n  \\n  created_by = models.TextField(null=True,\\\n      \\ blank=True, editable=False)\\n  edited_by = models.TextField(null=True, blank=True,\\\n      \\ editable=False)\\n  \\n  class Meta:\\n    abstract = True\\n  \\n  @hook(AFTER_UPDATE)\\n\\\n      \\  def update_edited_by(self):\\n    context = context_id.get()\\n    if context\\\n      \\ and hasattr(context[\'request_obj\'], \'auth\'):\\n      self.edited_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.edited_by\\\n      \\ = \'Initial Data Upload\'\\n  \\n  @hook(AFTER_CREATE)\\n  def update_created_by(self):\\n\\\n      \\    context = context_id.get()\\n    if context and hasattr(context[\'request_obj\'],\\\n      \\ \'auth\'):\\n      self.created_by = f\\"{context[\'request_obj\'].auth[\'name\']}\\\n      \\ ({context[\'request_obj\'].auth[\'sub\']})\\"\\n    else:\\n      self.created_by\\\n      \\ = \'Initial Data Upload\'\\n"\n  source code: "class CalculationModel(LexModel):\\n    IN_PROGRESS = \'IN_PROGRESS\'\\n\\\n    \\    ERROR = \'ERROR\'\\n    SUCCESS = \'SUCCESS\'\\n    NOT_CALCULATED = \'NOT_CALCULATED\'\\n\\\n    \\    ABORTED = \'ABORTED\'\\n    STATUSES = [\\n    (IN_PROGRESS, \'IN_PROGRESS\'),\\n\\\n    \\    (ERROR, \'ERROR\'),\\n    (SUCCESS, \'SUCCESS\'),\\n    (NOT_CALCULATED, \'NOT_CALCULATED\'),\\n\\\n    \\    (ABORTED, \'ABORTED\')\\n    ]\\n    \\n    is_calculated =  models.CharField(max_length=50,\\\n    \\ choices=STATUSES, default=NOT_CALCULATED)\\n    \\n    class Meta:\\n      abstract\\\n    \\ = True\\n    \\n    # This is the only thing that should be implemented, DON\'T\\\n    \\ USE the implementation details\\n    @abstractmethod\\n    def calculate(self):\\n\\\n    \\      pass\\n    \\n    # TODO: For the Celery task cases, this hook should be\\\n    \\ updated\\n    \\n    @hook(AFTER_UPDATE, on_commit=True)\\n    @hook(AFTER_CREATE,\\\n    \\ on_commit=True)\\n    def calculate_hook(self):\\n      try:\\n        if hasattr(self,\\\n    \\ \'is_atomic\') and not self.is_atomic:\\n          # TODO: To fix with the correct\\\n    \\ type\\n          # update_calculation_status(self)\\n          self.calculate()\\n\\\n    \\          self.is_calculated = self.SUCCESS\\n        else:\\n          with transaction.atomic():\\n\\\n    \\            self.calculate()\\n            self.is_calculated = self.SUCCESS\\n\\\n    \\      except Exception as e:\\n        self.is_calculated = self.ERROR\\n     \\\n    \\   raise e\\n      finally:\\n        self.save(skip_hooks=True)\\n        update_calculation_status(self)\\n"\nLexLogLevel:\n  Import Path: from lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\n  source code: "class LexLogLevel:\\n  VERBOSE = 5\\n  DEBUG = 10\\n  INFO = 20\\n  WARNING\\\n    \\ = 30\\n  ERROR = 40\\n  CRITICAL = 50\\n"\nLexLogger:\n  Import Path: from lex.lex_app.LexLogger.LexLogger import LexLogger\n  source code: "@LexSingleton\\nclass LexLogger:\\n  class MarkdownBuilder:\\n    lexLogger\\\n    \\ = None\\n    \\n    def __init__(self, level, flushing=True, **kwargs):\\n    \\\n    \\  self.kwargs = kwargs\\n      self.flushing = flushing\\n      self.level = level\\n\\\n    \\      self.parts = []\\n      self.det = []\\n      self.content = self.parts\\n\\\n    \\    \\n    def builder(self, level=LexLogLevel.INFO, flushing=True, **kwargs):\\n\\\n    \\      self.kwargs = {**{key: value for key, value in kwargs.items()\\n       \\\n    \\ if key != \\"flushing\\" and key != \\"level\\" and key not in self.kwargs.keys()},\\\n    \\ **self.kwargs}\\n      return self\\n    \\n    def details(self):\\n      self.content\\\n    \\ = self.det\\n      return self\\n    \\n    def normal(self):\\n      self.content\\\n    \\ = self.parts\\n      return self\\n    \\n    def _check_flush(self):\\n      if\\\n    \\ self.flushing:\\n        self.log()\\n      return self\\n    \\n    def add_heading(self,\\\n    \\ text: str, level: int = 1):\\n                                              if\\\n    \\ 1 <= level <= 6:\\n                                                self.content.append(f\\"\\\n    {\'#\' * level} {text}\\\\n\\")\\n                                              return\\\n    \\ self._check_flush()\\n    \\n    def add_paragraph(self, text: str):\\n       \\\n    \\                               \\"\\"\\"Add a paragraph.\\"\\"\\"\\n               \\\n    \\                         self.content.append(f\\"{text}\\\\n\\\\n\\")\\n           \\\n    \\                             return self._check_flush()\\n    \\n    def sleep(self,\\\n    \\ seconds):\\n      time.sleep(seconds)\\n      return self\\n    \\n    def add_colored_text(self,\\\n    \\ text, color=\\"black\\"):\\n      self.content.append(f\\"<span style=\'color:{color}\'>{text}</span>\\\\\\\n    n\\\\n\\")\\n      return self._check_flush()\\n    \\n    def add_bold(self, text:\\\n    \\ str):\\n                                 \\"\\"\\"Add bold text.\\"\\"\\"\\n       \\\n    \\                            self.content.append(f\\"**{text}** \\")\\n         \\\n    \\                          return self._check_flush()\\n    \\n    def add_table(self,\\\n    \\ data: dict):\\n                                  \\"\\"\\"Add a table from a dictionary.\\\n    \\ Keys are the headers, values are lists of column data.\\"\\"\\"\\n             \\\n    \\                       headers = list(data.keys())\\n                        \\\n    \\            rows = list(zip(*data.values()))\\n                              \\\n    \\  \\n                                # Add header row\\n                      \\\n    \\              self.content.append(f\\"| {\' | \'.join(headers)} |\\\\n\\")\\n      \\\n    \\                              self.content.append(f\\"|{\'|\'.join([\' --- \' for\\\n    \\ _ in headers])}|\\\\n\\")\\n                                \\n                 \\\n    \\               # Add rows\\n                                    for row in rows:\\n\\\n    \\                                      self.content.append(f\\"| {\' | \'.join(map(str,\\\n    \\ row))} |\\\\n\\")\\n                                    self.content.append(\\"\\\\\\\n    n\\")\\n                                    return self._check_flush()\\n    \\n \\\n    \\   def add_df(self, dataframe, with_borders=True):\\n      if dataframe.empty:\\n\\\n    \\        return self.add_paragraph(\\"No data available\\")._check_flush()\\n   \\\n    \\   \\n      if with_borders:\\n        table_md = dataframe.to_markdown(index=False)\\n\\\n    \\      else:\\n        table_md = dataframe.to_string(index=False)\\n      \\n  \\\n    \\    # Add to the content\\n      return self.add_paragraph(table_md)._check_flush()\\n\\\n    \\    \\n    def add_df_from_string(self, string_data):\\n      data = ast.literal_eval(string_data)\\n\\\n    \\      \\n      # If the data is a list of tuples/lists, infer the number of columns\\n\\\n    \\      if isinstance(data, list) and len(data) > 0:\\n        # Infer the number\\\n    \\ of columns dynamically from the first row of the data\\n        num_columns =\\\n    \\ len(data[0])\\n        columns = [f\\"Column {i + 1}\\" for i in range(num_columns)]\\n\\\n    \\        \\n        # Create a DataFrame\\n        df = pd.DataFrame(data, columns=columns)\\n\\\n    \\        return self.add_table(df.to_dict())._check_flush()\\n      \\n      return\\\n    \\ self.add_paragraph(\\"Invalid data format\\")._check_flush()\\n    \\n    def add_italic(self,\\\n    \\ text: str):\\n                                   \\"\\"\\"Add italic text.\\"\\"\\"\\\n    \\n                                     self.content.append(f\\"*{text}*\\")\\n  \\\n    \\                                   return self._check_flush()\\n    \\n    def\\\n    \\ add_link(self, text: str, url: str):\\n                                     \\\n    \\      \\"\\"\\"Add a link.\\"\\"\\"\\n                                             self.content.append(f\\"\\\n    [{text}]({url})\\")\\n                                             return self._check_flush()\\n\\\n    \\    \\n    def add_list(self, items: list, ordered: bool = False):\\n         \\\n    \\                                        \\"\\"\\"Add a list, either ordered (numbered)\\\n    \\ or unordered (bullets).\\"\\"\\"\\n                                            \\\n    \\       if ordered:\\n                                                     self.content.extend([f\\"\\\n    {i + 1}. {item}\\" for i, item in enumerate(items)])\\n                        \\\n    \\                           else:\\n                                          \\\n    \\           self.content.extend([f\\"- {item}\\" for item in items])\\n         \\\n    \\                                          self.content.append(\\"\\\\n\\")\\n    \\\n    \\                                               return self._check_flush()\\n \\\n    \\   \\n    def add_code_block(self, code: str, language: str = \\"\\"):\\n       \\\n    \\                                               \\"\\"\\"Add a code block with optional\\\n    \\ language syntax highlighting.\\"\\"\\"\\n                                      \\\n    \\                  self.content.append(f\\"```{language}\\\\n{code}\\\\n```\\\\n\\")\\n\\\n    \\                                                        return self._check_flush()\\n\\\n    \\    \\n    def add_horizontal_rule(self):\\n        \\"\\"\\"Add a horizontal rule.\\"\\\n    \\"\\"\\n          self.content.append(\\"---\\\\n\\")\\n          return self._check_flush()\\n\\\n    \\    \\n    def add_blockquote(self, text: str):\\n                            \\\n    \\           \\"\\"\\"Add blockquote.\\"\\"\\"\\n                                    \\\n    \\     self.content.append(f\\"> {text}\\\\n\\\\n\\")\\n                             \\\n    \\            return self._check_flush()\\n    \\n    def add_image(self, alt_text:\\\n    \\ str, url: str):\\n                                                \\"\\"\\"Add an\\\n    \\ image.\\"\\"\\"\\n                                                  self.content.append(f\\"\\\n    ![{alt_text}]({url})\\\\n\\\\n\\")\\n                                              \\\n    \\    return self._check_flush()\\n    \\n    def log(self, level: int = LexLogLevel.INFO):\\n\\\n    \\                           message = self.__str__()\\n                       \\\n    \\    if not message:\\n                             return\\n                  \\\n    \\         self.lexLogger.markdown(self.level, self.__str__(), **self.kwargs)\\n\\\n    \\                           if self.content is self.parts:\\n                 \\\n    \\            self.parts = []\\n                             self.det = []\\n   \\\n    \\                          self.content = self.parts\\n                       \\\n    \\    else:\\n                             self.parts = []\\n                   \\\n    \\          self.det = []\\n                             self.content = self.det\\n\\\n    \\                           \\n                           return self\\n    \\n \\\n    \\   def __del__(self, **kwargs):\\n      self.log()\\n    \\n    def __str__(self):\\n\\\n    \\        \\"\\"\\"Return the entire Markdown text as a string.\\"\\"\\"\\n          return\\\n    \\ \\"\\".join(self.parts)\\n    \\n    def details_to_str(self):\\n      return \\"\\"\\\n    .join(self.det)\\n    \\n    def return_markdown(self):\\n      return {**{WebSocketHandler.DJANGO_TO_REACT_MAPPER[key]:\\\n    \\ value for key, value in self.kwargs.items() if key != \\"flushing\\"}, WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'details\']:\\\n    \\ self.details_to_str(),\\n                                                   \\\n    \\              WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'message\']: self.__str__(),\\\n    \\ WebSocketHandler.DJANGO_TO_REACT_MAPPER[\'level\']: \'INFO\'}\\n  \\n  def is_valid_markdown(self,\\\n    \\ message: str) -> bool:\\n                                         try:\\n    \\\n    \\                                       self.parser(message)\\n               \\\n    \\                            return True\\n                                   \\\n    \\      except Exception as e:\\n                                           print(e)\\n\\\n    \\                                           return False\\n  \\n  def __init__(self):\\n\\\n    \\    self.logger = None\\n    self._initialize_logger()\\n    self.parser = mistune.create_markdown()\\n\\\n    \\    self.MarkdownBuilder.lexLogger = self\\n  \\n  def _initialize_logger(self):\\n\\\n    \\    self.logger = logging.getLogger(__name__)\\n    self.logger.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Add custom log level\\n    logging.addLevelName(LexLogLevel.VERBOSE,\\\n    \\ \\"VERBOSE\\")\\n    \\n    # Create handlers\\n    console_handler = logging.StreamHandler()\\n\\\n    \\    file_handler = logging.FileHandler(settings.LOG_FILE_PATH)\\n    websocket_handler\\\n    \\ = WebSocketHandler()\\n    \\n    # Set levels for handlers\\n    console_handler.setLevel(LexLogLevel.WARNING)\\n\\\n    \\    file_handler.setLevel(LexLogLevel.VERBOSE)\\n    websocket_handler.setLevel(LexLogLevel.VERBOSE)\\n\\\n    \\    \\n    # Create formatters and add them to handlers\\n    # formatter = logging.Formatter(\'%(asctime)s\\\n    \\ - %(name)s - %(levelname)s - %(message)s\')\\n    formatter = logging.Formatter(\'%(message)s\')\\n\\\n    \\    console_handler.setFormatter(formatter)\\n    file_handler.setFormatter(formatter)\\n\\\n    \\    websocket_handler.setFormatter(formatter)\\n    \\n    # Add handlers to the\\\n    \\ logger\\n    self.logger.addHandler(console_handler)\\n    self.logger.addHandler(file_handler)\\n\\\n    \\    self.logger.addHandler(websocket_handler)\\n  \\n  def markdown_error(self,\\\n    \\ message):\\n    if not self.is_valid_markdown(message):\\n      return\\n    \\n\\\n    \\    self.logger.error(message)\\n  \\n  def markdown(self, level, message, **kwargs):\\n\\\n    \\    if not self.is_valid_markdown(message):\\n      return\\n    obj = CalculationLog.create(\\n\\\n    \\    message=message,\\n    message_type=kwargs.get(\'message_type\', \\"Progress\\"\\\n    ),\\n    trigger_name=kwargs.get(\'trigger_name\', None),\\n    is_notification=kwargs.get(\'is_notification\',\\\n    \\ False),\\n    )\\n    self.logger.log(level, message, extra={**kwargs, \'log_id\':\\\n    \\ obj.id, \'calculation_id\': obj.calculationId, \'class_name\': obj.calculation_record,\\\n    \\ \\"trigger_name\\": obj.trigger_name, \\"method\\": obj.method})\\n  \\n  def builder(self,\\\n    \\ level=LexLogLevel.INFO, flushing=True, **kwargs):\\n    return self.MarkdownBuilder(level=level,\\\n    \\ flushing=flushing, **kwargs)\\n  \\n  def markdown_warning(self, message):\\n \\\n    \\   if not self.is_valid_markdown(message):\\n      return\\n    \\n    self.logger.warning(message)\\n\\\n    \\  \\n  def verbose(self, message):\\n    self.logger.log(LexLogLevel.VERBOSE, message)\\n\\\n    \\  \\n  def debug(self, message):\\n    self.logger.debug(message)\\n  \\n  def info(self,\\\n    \\ message):\\n    self.logger.info(message)\\n  \\n  def warning(self, message):\\n\\\n    \\    self.logger.warning(message)\\n  \\n  def error(self, message):\\n    self.logger.error(message)\\n\\\n    \\  \\n  def critical(self, message):\\n    self.logger.critical(message)\\n"\nPrcoessAdminTest:\n  Import Path: from lex.lex_app.tests.ProcessAdminTestCase import ProcessAdminTestCase\n  source code: "class ProcessAdminTestCase(TestCase):\\n# The ProcessAdminTestCase\\\n    \\ class is a custom test case class designed to facilitate testing in a Django\\\n    \\ application. It extends from unittest.TestCase and automates the setup and teardown\\\n    \\ of test data based on a JSON configuration. This class reads a JSON structure\\\n    \\ defining actions like create, update, and delete on Django models and executes\\\n    \\ these actions during the test setup phase.\\n# Purpose: Extends `unittest.TestCase`\\\n    \\ to create a custom test case class that automates test data setup.\\n\\n    def\\\n    \\ replace_tagged_parameters(self, object_parameters):\\n       # * Purpose: Processes\\\n    \\ parameters by replacing special tagged values with actual objects or parsed\\\n    \\ data.\\n       # * Functionality:\\n       #   - Iterates over each key-value\\\n    \\ pair in object_parameters.\\n       #   - If the value is a string and starts\\\n    \\ with:\\n       #     \\"tag:\\": Replaces it with an object from self.tagged_objects.\\n\\\n    \\       #     \\"datetime:\\": Parses it into a datetime object.\\n       #   - Updates\\\n    \\ the parameter with the parsed value.\\n\\n      for key in object_parameters:\\n\\\n    \\          value: str = object_parameters[key]\\n          if isinstance(value,\\\n    \\ str):\\n              parsed_value = value\\n              if value.startswith(\\"\\\n    tag:\\"):\\n                  # Replace \'tag:<tag_name>\' with the actual object\\\n    \\ from tagged_objects\\n                  parsed_value = self.tagged_objects[value.replace(\\"\\\n    tag:\\", \\"\\")]\\n              elif value.startswith(\\"datetime:\\"):\\n        \\\n    \\          # Parse \'datetime:<date_string>\' into a datetime object\\n         \\\n    \\         parsed_value = dateutil.parser.parse(value.replace(\\"datetime:\\", \\"\\\n    \\"))\\n              object_parameters[key] = parsed_value\\n      return object_parameters\\n\\\n    \\n\\n    # test_path: An attribute that specifies the path to the test data JSON\\\n    \\ file. If not provided, it defaults to test_data.json in the same directory as\\\n    \\ the test class.\\n    test_path = None\\n\\n \\n    def setUp(self) -> None:\\n \\\n    \\      # Purpose: Sets up the test environment by performing actions defined in\\\n    \\ the test data.\\n       # Functionality:\\n       #   Retrieves all models from\\\n    \\ the Django app specified in settings.repo_name.\\n       #   Initializes a timestamp\\\n    \\ and an empty dictionary for tagged objects.\\n       #   Loads test data using\\\n    \\ get_test_data().\\n       #   Iterates over each action in the test data and\\\n    \\ performs create, update, or delete operations.\\n       #     Create:\\n     \\\n    \\  #       Processes parameters and creates a new object.\\n       #       Stores\\\n    \\ the object in self.tagged_objects with the specified tag.\\n       #     Update:\\n\\\n    \\       #       Processes filter parameters to find the object.\\n       #    \\\n    \\   Updates the object\'s attributes and saves it.\\n       #     Delete:\\n    \\\n    \\   #       Deletes objects matching the filter parameters.\\n\\n        from datetime\\\n    \\ import datetime\\n\\n        # Retrieve all models from the specified Django app\\n\\\n    \\        generic_app_models = {\\n            f\\"{model.__name__}\\": model for\\\n    \\ model in\\n            set(apps.get_app_config(settings.repo_name).models.values())\\n\\\n    \\        }\\n\\n        self.t0 = datetime.now()  # Record the start time\\n    \\\n    \\    self.tagged_objects = {}  # Initialize a dictionary to store created objects\\n\\\n    \\n        test_data = self.get_test_data()  # Load test data from JSON\\n\\n   \\\n    \\     # Iterate over each action in the test data\\n        for object in test_data:\\n\\\n    \\            klass = generic_app_models[object[\'class\']]  # Get the model class\\n\\\n    \\            action = object[\'action\']\\n            tag = object.get(\'tag\', \'instance\')\\\n    \\  # Default tag if not provided\\n\\n            if action == \'create\':\\n     \\\n    \\           # Replace tagged parameters\\n                object[\'parameters\']\\\n    \\ = self.replace_tagged_parameters(object[\'parameters\'])\\n                # Create\\\n    \\ an instance of the model\\n                self.tagged_objects[tag] = klass(**object[\'parameters\'])\\n\\\n    \\                # Cache the action (thread-safe)\\n                cache.set(threading.get_ident(),\\\n    \\ f\\"{object[\'class\']}_{action}\\")\\n                # Save the object to the database\\n\\\n    \\                self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'update\':\\n                # Replace tagged filter parameters\\n            \\\n    \\    object[\'filter_parameters\'] = self.replace_tagged_parameters(object[\'filter_parmeters\'])\\n\\\n    \\                # Retrieve the object to update\\n                self.tagged_objects[tag]\\\n    \\ = klass.objects.filter(**object[\'filter_parameters\']).first()\\n            \\\n    \\    if self.tagged_objects[tag] is not None:\\n                    # Update the\\\n    \\ object\'s attributes\\n                    for key in object[\'parameters\']:\\n\\\n    \\                        setattr(self.tagged_objects[tag], key, object[\'parameters\'][key])\\n\\\n    \\n                    # Cache the action with object primary key\\n           \\\n    \\         cache.set(\\n                        threading.get_ident(),\\n       \\\n    \\                 f\\"{object[\'class\']}_{action}_{self.tagged_objects[tag].pk}\\"\\\n    \\n                    )\\n                    # Save the updated object\\n     \\\n    \\               self.tagged_objects[tag].save()\\n\\n            elif action ==\\\n    \\ \'delete\':\\n                # Delete objects matching the filter parameters\\n\\\n    \\                klass.objects.filter(**object[\'filter_parameters\']).delete()\\n\\\n    \\n    def tearDown(self) -> None:\\n        pass\\n\\n    def get_test_data(self):\\n\\\n    \\        # Purpose: Determines the path to the test data JSON file and loads it.\\n\\\n    \\        # Functionality:\\n        #   If test_path is not set, defaults to test_data.json\\\n    \\ in the test class directory.\\n        #   If test_path is provided, constructs\\\n    \\ the path relative to PROJECT_ROOT.\\n        #   Calls get_test_data_from_path()\\\n    \\ to load and return the test data.\\n\\n        if self.test_path is None:\\n  \\\n    \\          # Use default \'test_data.json\' in the test class directory\\n      \\\n    \\      file = inspect.getfile(self.__class__)\\n            path = Path(file).parent\\n\\\n    \\            clean_test_path = str(path) + os.sep + \\"test_data.json\\"\\n     \\\n    \\   else:\\n            # Use the provided \'test_path\'\\n            clean_test_path\\\n    \\ = self.test_path.replace(\'/\', os.sep)\\n            clean_test_path = os.getenv(\\"\\\n    PROJECT_ROOT\\") + os.sep + clean_test_path\\n\\n        # Load test data from the\\\n    \\ JSON file\\n        test_data = self.get_test_data_from_path(clean_test_path)\\n\\\n    \\        return test_data\\n\\n    def get_test_data_from_path(self, path):\\n  \\\n    \\      # Purpose: Loads test data from the specified path, handling any nested\\\n    \\ subprocesses.\\n        # Functionality:\\n        #   Opens and reads the JSON\\\n    \\ file at the given path.\\n        #   Checks for any \\"subprocess\\" keys in the\\\n    \\ data and recursively loads additional test data.\\n        #   Flattens the potentially\\\n    \\ nested lists of test data into a single list.\\n        #   Returns the combined\\\n    \\ test data.\\n\\n        with open(str(path), \'r\') as f:\\n            test_data\\\n    \\ = json.loads(f.read())\\n            # Process any \'subprocess\' entries recursively\\n\\\n    \\            for index, object in enumerate(test_data):\\n                if \\"\\\n    subprocess\\" in object:\\n                    subprocess_path = object[\'subprocess\'].replace(\'/\',\\\n    \\ os.sep)\\n                    subprocess_path = os.getenv(\\"PROJECT_ROOT\\") +\\\n    \\ os.sep + subprocess_path\\n                    sublist = self.get_test_data_from_path(subprocess_path)\\n\\\n    \\                    test_data[index] = sublist\\n\\n        # Flatten the list\\\n    \\ in case of nested lists from subprocesses\\n        flat_list = []\\n        for\\\n    \\ sublist in test_data:\\n            if isinstance(sublist, list):\\n         \\\n    \\       flat_list.extend(sublist)\\n            else:\\n                flat_list.append(sublist)\\n\\\n    \\        return flat_list\\n\\n    def get_classes(self, generic_app_models):\\n\\\n    \\        # Purpose: Retrieves a set of all model classes involved in the test\\\n    \\ data.\\n        # Functionality:\\n        #   Loads the test data.\\n        #\\\n    \\   Uses a set comprehension to collect unique model classes based on the \\"class\\"\\\n    \\ key in each test data object.\\n\\n        test_data = self.get_test_data()\\n\\\n    \\        # Collects all model classes used in the test data\\n        return set(generic_app_models[object[\'class\']]\\\n    \\ for object in test_data)\\n\\n    def check_if_all_models_are_empty(self, generic_app_models):\\n\\\n    \\        # Purpose: Checks whether all involved model classes have no instances\\\n    \\ in the database.\\n        # Functionality:\\n        #   Iterates over each model\\\n    \\ class obtained from get_classes().\\n        #   Returns False immediately if\\\n    \\ any class has objects.\\n        #   Returns True only if all classes are empty.\\n\\\n    \\n        for klass in self.get_classes(generic_app_models):\\n            if klass.objects.all().count()\\\n    \\ > 0:\\n                # If any model has objects, return False\\n           \\\n    \\     return False\\n        # All models are empty\\n        return True\\n\\n  \\\n    \\  def get_list_of_non_empty_models(self, generic_app_models):\\n        # Purpose:\\\n    \\ Provides a summary of models that have instances in the database.\\n        #\\\n    \\ Functionality:\\n        #   Iterates over each model class from get_classes().\\n\\\n    \\        #   Counts the number of instances for each class.\\n        #   If a\\\n    \\ class has instances, adds it to the result dictionary with the count.\\n    \\\n    \\    #   Returns the dictionary of non-empty models.\\n\\n        count_of_objects_in_non_empty_models\\\n    \\ = {}\\n        for klass in self.get_classes(generic_app_models):\\n         \\\n    \\   c = klass.objects.all().count()\\n            if c > 0:\\n                #\\\n    \\ Add the class and count to the dictionary if not empty\\n                count_of_objects_in_non_empty_models[str(klass)]\\\n    \\ = c\\n        return count_of_objects_in_non_empty_models\\n"\nXLSXField:\n  Example Usage: \'XLSXField.create_excel_file_from_dfs(<FileField Object>, path, data_frames,\n    sheet_names)\n\n    \'\n  Import Path: from lex.lex_app.rest_api.fields.XLSX_field import XLSXField\n  source code: "class XLSXField(FileField):\\n  max_length = 300\\n  \\n  cell_format\\\n    \\ = \'#,##0.00 ;[Red]-#,##0.00 ;_-* \\"-\\"??_-\'\\n  cell_format_without_color = \'#,##0.00\\\n    \\ ;-#,##0.00 ;_-* \\"-\\"??_-\'\\n  boolean_format = \'[Green]\\"TRUE\\";[Red]\\"FALSE\\"\\\n    ;[Red]\\"FALSE\\";[Red]\\"FALSE\\"\'\\n  \\n  \\n  def get_number_of_rows_to_insert(self,\\\n    \\ sheet, index_len):\\n    max_len = 0\\n    for column_num in range(index_len +\\\n    \\ 1, sheet.max_column + 1):\\n      cell = sheet.cell(row=1, column=column_num)\\\n    \\  # Adjust the row number to the row where you want to start splitting\\n    \\\n    \\  if cell.value:  # Check if the cell is not empty\\n        split_values = cell.value.split(\\"\\\n    .\\")\\n        max_len = max(max_len, len(split_values))\\n    \\n    return max_len\\n\\\n    \\  \\n  def insert_rows_before_first_row(self, sheet, num_rows):\\n    sheet.insert_rows(1,\\\n    \\ amount=num_rows)\\n  \\n  def split_entries_in_sheet(self, sheet, number_of_inserted_rows,\\\n    \\ index_len):\\n    for column_num in range(index_len + 1, sheet.max_column + 1):\\\n    \\  # Column F starts at index 6 (1-indexed)\\n      cell = sheet.cell(row=number_of_inserted_rows+1,\\\n    \\ column=column_num)  # Adjust the row number to the row where you want to start\\\n    \\ splitting\\n      if cell.value:  # Check if the cell is not empty\\n        split_values\\\n    \\ = cell.value.split(\\".\\")  # Split the entry at every \\".\\"\\n        first_row\\\n    \\ = 1  # First row to fill the split values\\n        for idx, split_value in enumerate(split_values):\\n\\\n    \\          row_num = first_row + idx\\n          sheet.cell(row=row_num, column=column_num,\\\n    \\ value=split_value)\\n          # Bold the cell and add outside borders\\n    \\\n    \\      cell_to_format = sheet.cell(row=row_num, column=column_num)\\n         \\\n    \\ cell_to_format.font = Font(bold=True)\\n          cell_to_format.border = Border(top=Side(border_style=\'thin\'),\\n\\\n    \\          bottom=Side(border_style=\'thin\'),\\n          left=Side(border_style=\'thin\'),\\n\\\n    \\          right=Side(border_style=\'thin\'))\\n  \\n  def create_pivotable_row(self,\\\n    \\ sheet, index_len, number_of_rows_to_be_inserted, range_of_pivot_concatenation=None):\\n\\\n    \\    for column_num in range(index_len + 1, sheet.max_column + 1):\\n      concatenated_value\\\n    \\ = \\"\\"\\n      for row_num in range_of_pivot_concatenation:\\n        cell = sheet.cell(row=row_num,\\\n    \\ column=column_num)\\n        if cell.value:\\n          concatenated_value +=\\\n    \\ cell.value + \\" \\"\\n      sheet.cell(row=number_of_rows_to_be_inserted+1, column=column_num,\\\n    \\ value=concatenated_value.strip())\\n  \\n  def create_excel_file_from_dfs(self,\\\n    \\ path, data_frames, sheet_names=None, merge_cells=False, formats={}, comments={},\\\n    \\ index=True, ranges_of_pivot_concatenation={\'default\': None}):\\n            \\\n    \\                                                                            \\\n    \\                                                                            \\\n    \\              \\"\\"\\"\\n        :param path: file_path including file_name; if\\\n    \\ relative, will be saved under self.to+path\\n        :param data_frames: list\\\n    \\ of dataframes that will be inserted into an excel tab each\\n        :param sheet_names:\\\n    \\ list of sheet names corresponding to the data_frames\\n        :rtype: None\\n\\\n    \\        \\"\\"\\"\\n    if sheet_names is None:\\n        sheet_names = [\'Sheet\']\\n\\\n    \\    excel_file = BytesIO()\\n    writer = pd.ExcelWriter(excel_file, engine=\'xlsxwriter\')\\n\\\n    \\    df: pd.DataFrame\\n    idx_length = 0\\n    for df, sheet_name in zip(data_frames,\\\n    \\ sheet_names):\\n        if df is not None:\\n            if len(df) == 0:\\n  \\\n    \\              df = df.append(pd.Series(), ignore_index=True)\\n            if\\\n    \\ index:\\n                idx_length = df.index.nlevels\\n            df.to_excel(writer,\\\n    \\ sheet_name=sheet_name, merge_cells=merge_cells, freeze_panes=(1, idx_length),\\n\\\n    \\                        index=index)\\n\\n            worksheet = writer.sheets[sheet_name]\\\n    \\  # pull worksheet object\\n            worksheet_comment = comments[sheet_name]\\\n    \\ if sheet_name in comments else None\\n            cell_formats = {}\\n       \\\n    \\     for format in formats:\\n                cell_formats[format] = writer.book.add_format({\'num_format\':\\\n    \\ formats[format]})\\n\\n            if index:\\n                index_frame = df.index.to_frame()\\n\\\n    \\                for idx, col in enumerate(index_frame):  # loop through all columns\\n\\\n    \\                    series = index_frame[col]\\n                    max_len =\\\n    \\ max((\\n                        series.astype(str).map(len).max(),  # len of\\\n    \\ largest item\\n                        len(str(series.name))  # len of column\\\n    \\ name/header\\n                    )) + 1  # adding a little extra space\\n   \\\n    \\                 if is_datetime(series):\\n                        max_len = 22\\n\\\n    \\                    worksheet.set_column(idx, idx, max_len)  # set column width\\n\\\n    \\n            for idx, col in enumerate(df):  # loop through all columns\\n   \\\n    \\             series = df[col]\\n                if worksheet_comment is not None:\\n\\\n    \\                    comment = worksheet_comment[col] if col in worksheet_comment\\\n    \\ else None\\n                    if comment is not None:\\n                   \\\n    \\     worksheet.write_comment(0, idx + idx_length, comment)\\n\\n              \\\n    \\  max_len = max((\\n                    series.astype(str).map(len).max(),  #\\\n    \\ len of largest item\\n                    len(str(series.name))  # len of column\\\n    \\ name/header\\n                )) + 1  # adding a little extra space\\n       \\\n    \\         worksheet.set_column(idx + idx_length, idx + idx_length, max_len)  #\\\n    \\ set column width\\n                # set Cell format\\n                if col\\\n    \\ in formats:\\n                    worksheet.set_column(idx + idx_length, idx\\\n    \\ + idx_length, max_len, cell_format=cell_formats[col])\\n                elif\\\n    \\ is_datetime(df[col]):\\n                    pass\\n                else:\\n   \\\n    \\                 worksheet.set_column(idx + idx_length, idx + idx_length, max_len,\\n\\\n    \\                                         cell_format=writer.book.add_format({\'num_format\':\\\n    \\ XLSXField.cell_format}))\\n            # Add autofilter:\\n            if len(df.columns)\\\n    \\ > 0:\\n                worksheet.autofilter(0, 0, len(df), idx_length + len(df.columns)-1)\\n\\\n    \\n    writer.save()\\n    writer.close()\\n    excel_file.seek(0)\\n    self.save(path,\\\n    \\ content=File(excel_file), save=False)\\n\\n    return excel_file\\n"\n \n\n    SPECIFICATIONS:\n        Project Overview:\n            The Demo Windpark Consolidation project is designed to manage and analyze data related to windparks and their operators across different regions (China, Germany, and the USA). The project involves uploading data from Excel files, processing and storing this data in a database, and generating reports based on the consolidated data. The primary functionalities include data upload, data processing, and report generation.\n\n\n\n        Project Functionalities:\n            # Project Functionalities Overview\n\n## Main Functionalities\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Pseudocode Outline for Each Model’s Calculate Method\n\n### WindparkUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark data.\n  3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### USCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract US cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### ChinaCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract China cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### GermanyCashflowReportUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n  3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### WindparkOperatorOverviewUpload\n- **Primary Steps**:\n  1. **Data Validation**: Validate the uploaded file format and content.\n  2. **Data Parsing**: Parse the file to extract windpark operator data.\n  3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n  4. **Logging**: Log the upload process and any errors encountered.\n\n### CashflowReport\n- **Primary Steps**:\n  1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n  2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n  3. **Report Generation**: Generate a consolidated cashflow report.\n  4. **Logging**: Log the report generation process and any errors encountered.\n\nThis high-level overview and pseudocode outline provide a structured approach to understanding and implementing the functionalities of the project.\n\n        Project Models and Fields:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'id\': \'AutoField (Primary Key)\', \'windpark\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\', \'productionMioKwhPerYear\': \'FloatField\'}, \'USCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'date\': \'DateTimeField\', \'year\': \'IntegerField\', \'cashflow\': \'FloatField\', \'windparkName\': \'CharField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'quarter\': \'CharField\', \'cashflow\': \'FloatField\', \'windpark\': \'ForeignKey (to Windpark.windpark)\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}, \'WindparkOperatorOverview\': {\'id\': \'AutoField (Primary Key)\', \'year\': \'IntegerField\', \'areaM2\': \'FloatField\', \'employees\': \'IntegerField\', \'investmentMMEur\': \'FloatField\', \'windparkOperator\': \'CharField\', \'numberOfWindparks\': \'IntegerField\'}}, \'Reports\': {\'CashflowReport\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField (Optional)\', \'cashflowSum\': \'FloatField\', \'windparkOperator\': \'ForeignKey (to WindparkOperatorOverview.windparkOperator)\'}}, \'Uploads\': {\'WindparkUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'USCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'ChinaCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'GermanyCashflowReportUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}, \'WindparkOperatorOverviewUpload\': {\'id\': \'AutoField (Primary Key)\', \'file\': \'FileField\'}}}}\n            ```\n        Project Business Logic Calculations:\n            # Business Logic for Windpark Data Management and Reporting\n\n## Overview\nThe project is designed to handle data related to windparks and their operators, including cashflow reports from different countries. It consolidates this data to generate comprehensive reports. The key models and their roles are as follows:\n\n### Models and Their Roles\n\n1. **Windpark**\n   - **Description**: Represents individual windparks and their annual energy production.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n2. **WindparkOperatorOverview**\n   - **Description**: Represents windpark operators, their investments, and other details.\n   - **Key Interactions**: Acts as a central entity connected to `Windpark`, `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n3. **USCashflowReport**\n   - **Description**: Represents cashflow data for US windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n4. **ChinaCashflowReport**\n   - **Description**: Represents cashflow data for Chinese windpark operators.\n   - **Key Interactions**: Linked to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n5. **GermanyCashflowReport**\n   - **Description**: Represents cashflow data for German windpark operators.\n   - **Key Interactions**: Linked to `Windpark` through the `windpark` foreign key and to `WindparkOperatorOverview` through the `windparkOperator` foreign key.\n\n6. **CashflowReport**\n   - **Description**: Generates consolidated cashflow reports.\n   - **Key Interactions**: Aggregates data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n\n### Upload Models\n- **WindparkUpload**\n  - **Description**: Handles data uploads for windparks.\n- **USCashflowReportUpload**\n  - **Description**: Handles data uploads for US cashflow reports.\n- **ChinaCashflowReportUpload**\n  - **Description**: Handles data uploads for China cashflow reports.\n- **GermanyCashflowReportUpload**\n  - **Description**: Handles data uploads for Germany cashflow reports.\n- **WindparkOperatorOverviewUpload**\n  - **Description**: Handles data uploads for windpark operators.\n\n## Detailed Business Logic for Each Model’s Calculate Method\n\n### WindparkUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windpark`, `Windparkbetreiber`, `Erzeugung [Mio. khW/Jahr]`.\n2. **Data Parsing**: Parse the file to extract windpark data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `Windpark` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `Windpark` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### USCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Year`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract US cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `USCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `USCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### ChinaCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark Name`, `Datum`, `Jahr`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract China cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `ChinaCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `ChinaCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### GermanyCashflowReportUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `ID`, `Windparkbetreiber`, `Windpark`, `Year`, `Quarter`, `Cashflow`.\n2. **Data Parsing**: Parse the file to extract Germany cashflow data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `GermanyCashflowReport` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `GermanyCashflowReport` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### WindparkOperatorOverviewUpload\n**Primary Steps**:\n1. **Data Validation**: Validate the uploaded file format and content.\n   - Ensure the file is in the correct format (e.g., XLSX).\n   - Check for required columns: `Windparkbetreiber`, `number of Windparks`, `Year`, `Investment [MM €]`, `Mitarbeiter`, `Fläche [m2]`.\n2. **Data Parsing**: Parse the file to extract windpark operator data.\n   - Read the file using a library like pandas.\n   - Extract data into a DataFrame.\n3. **Data Insertion**: Insert the parsed data into the `WindparkOperatorOverview` model.\n   - Iterate over the DataFrame rows.\n   - For each row, create or update a `WindparkOperatorOverview` instance.\n4. **Logging**: Log the upload process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the upload process.\n   - Log any validation errors or exceptions.\n\n### CashflowReport\n**Primary Steps**:\n1. **Data Aggregation**: Aggregate cashflow data from `USCashflowReport`, `ChinaCashflowReport`, and `GermanyCashflowReport`.\n   - Query each cashflow report model to retrieve relevant data.\n   - Aggregate the cashflows by `windparkOperator`.\n2. **Data Summarization**: Summarize the aggregated data to calculate total cashflows for each windpark operator.\n   - Sum the cashflows for each operator.\n3. **Report Generation**: Generate a consolidated cashflow report.\n   - Create a new `CashflowReport` instance.\n   - Populate the instance with the summarized data.\n4. **Logging**: Log the report generation process and any errors encountered.\n   - Use `LexLogger` to log the start and end of the report generation process.\n   - Log any aggregation errors or exceptions.\n\n## User Story\n\n### Scenario: Generating a Consolidated Cashflow Report\n\n**User**: Financial Analyst at a Renewable Energy Company\n\n**Goal**: To generate a consolidated cashflow report for all windpark operators.\n\n**Steps**:\n1. The analyst uploads the latest cashflow data for US, China, and Germany windpark operators using the respective upload interfaces.\n2. Each upload triggers the `calculate()` method for the respective upload model, validating and parsing the data, and inserting it into the database.\n3. The analyst then initiates the generation of a consolidated cashflow report.\n4. The `calculate()` method for the `CashflowReport` model is triggered, aggregating and summarizing the cashflow data from all countries.\n5. The consolidated report is generated and saved, providing a comprehensive view of the cashflows for each windpark operator.\n6. Throughout the process, `LexLogger` logs key operations, ensuring transparency and traceability.\n\n**Outcome**: The analyst successfully generates a detailed cashflow report, enabling informed decision-making and financial analysis for the company\'s windpark operations.\n\n        Project Structure:\n            ```\n            {\'Folders\': {\'Inputs\': {\'Windpark\': {\'fields\': {\'windpark\': \'String\', \'windparkOperator\': \'String\', \'productionMioKwhPerYear\': \'Float\'}, \'description\': \'Represents individual windparks and their annual energy production.\'}, \'USCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for US windpark operators.\'}, \'ChinaCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'date\': \'DateTime\', \'year\': \'Integer\', \'cashflow\': \'Float\', \'windparkName\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for Chinese windpark operators.\'}, \'GermanyCashflowReport\': {\'fields\': {\'id\': \'Integer\', \'year\': \'Integer\', \'quarter\': \'String\', \'cashflow\': \'Float\', \'windpark\': \'String\', \'windparkOperator\': \'String\'}, \'description\': \'Represents cashflow data for German windpark operators.\'}, \'WindparkOperatorOverview\': {\'fields\': {\'year\': \'Integer\', \'areaM2\': \'Float\', \'employees\': \'Integer\', \'investmentMMEur\': \'Float\', \'windparkOperator\': \'String\', \'numberOfWindparks\': \'Integer\'}, \'description\': \'Represents windpark operators, their investments, and other details.\'}}, \'Reports\': {\'CashflowReport\': {\'fields\': {\'file\': \'File (Optional)\', \'cashflowSum\': \'Float\', \'windparkOperator\': \'String\'}, \'description\': \'Generates consolidated cashflow reports.\'}}, \'Uploads\': {\'WindparkUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windparks.\'}, \'USCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for US cashflow reports.\'}, \'ChinaCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for China cashflow reports.\'}, \'GermanyCashflowReportUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for Germany cashflow reports.\'}, \'WindparkOperatorOverviewUpload\': {\'fields\': {\'file\': \'File\'}, \'description\': \'Handles data uploads for windpark operators.\'}}}, \'Relationships\': {\'Windpark\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'USCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'ChinaCashflowReport\': {\'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}, \'GermanyCashflowReport\': {\'windpark\': \'ForeignKey(Windpark.windpark)\', \'windparkOperator\': \'ForeignKey(WindparkOperatorOverview.windparkOperator)\'}}}\n            ```\n\n        Project Input and Output Files:\n            ```\n            {"components":[{"name":"Windparks","type":"Input","explanation":"Windparks","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparks.xlsx","columns":["Windpark","Windparkbetreiber","Erzeugung [Mio. khW/Jahr]"],"data_samples":[{"Windpark":"DE1","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":5},{"Windpark":"DE2","Windparkbetreiber":"Deutschland","Erzeugung [Mio. khW/Jahr]":3},{"Windpark":"US1","Windparkbetreiber":"USA","Erzeugung [Mio. khW/Jahr]":7}],"row_count":6},{"name":"Windparkbetreiber_\\u00dcbersicht","type":"Input","explanation":"Windpark Operators","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/Windparkbetreiber_\\u00dcbersicht.xlsx","columns":["Windparkbetreiber","number of Windparks","Year","Investment [MM \\u20ac]","Mitarbeiter","Fl\\u00e4che [m2]"],"data_samples":[{"Windparkbetreiber":"Deutschland","number of Windparks":2,"Year":2019,"Investment [MM \\u20ac]":250,"Mitarbeiter":50,"Fl\\u00e4che [m2]":70000},{"Windparkbetreiber":"USA","number of Windparks":1,"Year":2019,"Investment [MM \\u20ac]":200,"Mitarbeiter":20,"Fl\\u00e4che [m2]":20000},{"Windparkbetreiber":"China","number of Windparks":3,"Year":2019,"Investment [MM \\u20ac]":300,"Mitarbeiter":75,"Fl\\u00e4che [m2]":100000}],"row_count":3},{"name":"US_CF_Report_20-22","type":"Input","explanation":"US Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/US_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Year","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"USA","Year":2020,"Cashflow":50},{"ID":2,"Windparkbetreiber":"USA","Year":2021,"Cashflow":90},{"ID":3,"Windparkbetreiber":"USA","Year":2022,"Cashflow":80}],"row_count":3},{"name":"C_CF_Report_20-22","type":"Input","explanation":"China Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/C_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark Name","Datum","Jahr","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-03-31 00:00:00","Jahr":2020,"Cashflow":15},{"ID":2,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-06-30 00:00:00","Jahr":2020,"Cashflow":14},{"ID":3,"Windparkbetreiber":"China","Windpark Name":"CH1","Datum":"2020-09-30 00:00:00","Jahr":2020,"Cashflow":4}],"row_count":36},{"name":"DE_CF_Report_20-22","type":"Input","explanation":"Germany Cashflows","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/input_files/DE_CF_Report_20-22.xlsx","columns":["ID","Windparkbetreiber","Windpark","Year","Quarter","Cashflow"],"data_samples":[{"ID":1,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q1","Cashflow":15},{"ID":2,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q2","Cashflow":15},{"ID":3,"Windparkbetreiber":"Deutschland","Windpark":"DE1","Year":2020,"Quarter":"Q3","Cashflow":9}],"row_count":24},{"name":"Cashflow_Report","type":"Output","explanation":"Cashflow Report Example","path":"/home/hazem/LUND_IT/lex_app_submodels/lex_ai/DemoWindparkConsolidation/output_files/Cashflow_Report.xlsx","columns":["Unnamed: 0","Windpark Operator","Cashflow Sum"],"data_samples":[{"Unnamed: 0":0,"Windpark Operator":"China","Cashflow Sum":379},{"Unnamed: 0":1,"Windpark Operator":"US","Cashflow Sum":220},{"Unnamed: 0":2,"Windpark Operator":"GER","Cashflow Sum":306}],"row_count":3}]}\n\n            ```\n    \n    Model Code to generate the test json for it:\n    ### model_fields/Inputs/ChinaCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass ChinaCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    date = models.DateTimeField()\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkName = models.CharField(max_length=255)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/USCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass USCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    cashflow = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n### model_fields/Inputs/WindparkOperatorOverview.py\n```python\nfrom django.db import models\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass WindparkOperatorOverview(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    areaM2 = models.FloatField()\n    employees = models.IntegerField()\n    investmentMMEur = models.FloatField()\n    windparkOperator = models.CharField(max_length=255)\n    numberOfWindparks = models.IntegerField()\n```\n### model_fields/Inputs/GermanyCashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.Windpark import Windpark\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.LexModel import LexModel\n\nclass GermanyCashflowReport(LexModel):\n    id = models.AutoField(primary_key=True)\n    year = models.IntegerField()\n    quarter = models.CharField(max_length=2)\n    cashflow = models.FloatField()\n    windpark = models.ForeignKey(Windpark, on_delete=models.CASCADE)\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n```\n\n### model_fields/Reports/CashflowReport.py\n```python\nfrom django.db import models\nfrom DemoWindparkConsolidation.model_fields.Inputs.WindparkOperatorOverview import WindparkOperatorOverview\nfrom lex.lex_app.lex_models.CalculationModel import CalculationModel\nfrom lex.lex_app.rest_api.fields.XLSX_field import XLSXField\nfrom lex.lex_app.LexLogger.LexLogger import LexLogger\nfrom lex.lex_app.LexLogger.LexLogLevel import LexLogLevel\nimport pandas as pd\n\nclass CashflowReport(CalculationModel):\n    id = models.AutoField(primary_key=True)\n    file = XLSXField()\n    cashflowSum = models.FloatField()\n    windparkOperator = models.ForeignKey(WindparkOperatorOverview, on_delete=models.CASCADE)\n\n    def calculate(self):\n        logger = LexLogger().builder(level=LexLogLevel.INFO, flushing=True)\n        logger.add_heading("Cashflow Report Generation", level=2)\n        try:\n            # Aggregate cashflow data from USCashflowReport, ChinaCashflowReport, and GermanyCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.USCashflowReport import USCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.ChinaCashflowReport import ChinaCashflowReport\n            from DemoWindparkConsolidation.model_fields.Inputs.GermanyCashflowReport import GermanyCashflowReport\n\n            us_cashflows = USCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            china_cashflows = ChinaCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n            germany_cashflows = GermanyCashflowReport.objects.values(\'windparkOperator\').annotate(total_cashflow=models.Sum(\'cashflow\'))\n\n            # Combine the cashflows into a single DataFrame\n            us_df = pd.DataFrame(list(us_cashflows))\n            china_df = pd.DataFrame(list(china_cashflows))\n            germany_df = pd.DataFrame(list(germany_cashflows))\n\n            combined_df = pd.concat([us_df, china_df, germany_df]).groupby(\'windparkOperator\').sum().reset_index()\n\n            # Save the combined DataFrame to an Excel file\n            output_path = \'reports/Cashflow_Report.xlsx\'\n            XLSXField.create_excel_file_from_dfs(self.file, output_path, [combined_df], sheet_names=[\'Cashflow Report\'])\n\n            # Update the cashflowSum field\n            self.cashflowSum = combined_df[\'total_cashflow\'].sum()\n            logger.add_paragraph(f"Total Cashflow Sum: {self.cashflowSum}")\n\n        except Exception as e:\n            logger.add_paragraph(f"Error generating cashflow report: {str(e)}")\n            raise e\n``` \n\n\n    Already Generated Test Jsons (Test were executed and the data is already available in the database, please reuse the data for the next json test):\n    [\n  {\n    "class": "GermanyCashflowReportUpload",\n    "action": "create",\n    "tag": "germany_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "germany_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/DE_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "WindparkOperatorOverviewUpload",\n    "action": "create",\n    "tag": "windpark_operator_overview_upload_1",\n    "type": "Upload",\n    "input_tag": "windpark_operator_overview",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/Windparkbetreiber_Übersicht.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "ChinaCashflowReportUpload",\n    "action": "create",\n    "tag": "china_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "china_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/C_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n[\n  {\n    "class": "USCashflowReportUpload",\n    "action": "create",\n    "tag": "us_cashflow_report_upload_1",\n    "type": "Upload",\n    "input_tag": "us_cashflow_report",\n    "parameters": {\n      "file": "DemoWindparkConsolidation/Tests/input_files/US_CF_Report_20-22.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n\n\n    \n    **Generation requirement**:\n    Before starting to generate the test json, please read the following requirements:\n        1. Only generate the next test json and then stop generating for the model code and the classes name given to you.\n        2. While generating test jsons take the example test json into consideration.\n        3. Be aware that the test json that you wrote will be used by a test class which is inhereting ProcessAdminTestCase from lex_app\n        4. Please create the test json for the cases of the User Story mentioned in the business logic \n        5. Do not use any upload model as a foreign key in the test json object, instead use their corresponding input model as a foreign key (e.g. use <Class> instead of <Class>Upload: "<class>_id" : "<object_pk>"):\n            Example for class <Class>:\n             class <ClassUpload>(CalculationModel):\n                file_field = models.FileField(upload_to=\'Tests/input_files/\')\n                class2 = models.ForeignKey(<Class>, on_delete=models.CASCADE) \n            Example Output: \n             \n     {\n        "class": "<ClassUpload>",\n        "action": "create",\n        "tag": "<class>_upload_<object_pk>",\n        "type": "Upload",\n        "input_tag": "<class>" // (always lowercase and seperated by "_") Since the <ClassUpload> is the Upload class of the <Class> model, we need to define the input tag to be able to use the <Class> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        }\n    }\n  \n    The above test creates <Class> objects by using <ClassUpload> and below test should use <Class> instead of <ClassUpload> because in the actual logic it is actually depending on <Class> objects and not <ClassUpload> objects: \n     {\n        "class": "<Class2Upload>",\n        "action": "create",\n        "tag": "<class2>_upload_<object_id>", // object_id is an int\n        "type": "Upload",\n        "input_tag": "<class2>" //(always lowercase and seperated by "_") Since the <Class2Upload> is the Upload class of the <Class2> model, we need to define the input tag to be able to use the <Class2> object in the next tests\n        "parameters": {\n          "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n          "is_calculated": "IN_PROGRESS",\n        },\n    }\n    \n    Report Example (Knowing that there is 4 objects of ExampleDependency in the database):\n[\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_1",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_1", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_1.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n  },\n  {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_2",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_2", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_2.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_3",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_3", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_3.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n    {\n    "class": "ExampleReport",\n    "action": "create",\n    "tag": "example_report_4",\n    "type": "Report",\n    "parameters": {\n      "sum": 0.0,\n      "exampleDependency": "example_dependency_4", // Never use upload models here\n      "file": "<ProjectName>/<Folder>/../<OutputName>_4.xlsx",\n      "is_calculated": "IN_PROGRESS"\n    }\n  },\n]\n \n     \n        \n        6. Use is_calculated field in the test json object if the class of that object is inhererting from CalculationModel, in our case those models are Report and Upload models\n        7. When you upload a file with a create test by assigning is_calculated to IN_PROGRESS, There is no need for an extra update test for that calculation which is being triggered by the file upload\n        8. Build upon the test jsons that were already provided to you \n        9. No delete test cases\n        10. Use the data available from previous tests, you don\'t have to create new data for the tests (build on top of the existing data)\n        11. If a test has file_input field, you can just used that to upload the data, and use the data for the next tests so don\'t define any data of your own\n        12. Please no ```json\n        \n        \n    **Example Test Json**:\n     \n    \n    [\n  {\n    "class": "<ClassName>",\n    "action": "create",\n    "tag": "class_name_<object_pk>",\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n    }\n  },\n  {\n    "class": "<ClassName>",\n    "action": "update",\n    "tag": "class_name_<object_pk>",\n    "filter_parameters": {\n      "period": "tag:class_name_<object_pk>"\n    },\n    "parameters": {\n      "date_time_field": "datetime:2023-12-12",\n      "text_field": "2024-Q4",\n      "foreign_key_field": "tag:class_name_<object_pk>",\n      "file_field": "<ProjectName>/Tests/input_files/<file_name>.xlsx",\n      "integer_field": 1,\n      "float_field": 1.0,\n      "boolean_field": true,\n      "is_calculated": "IN_PROGRESS"\n    }\n  }\n]\n    \n\n    The next test json to generate is: CashflowReport\n    \n    Number of objects for the report dependency (From the database): 1\n\n    Please just regenerate the classes mentioned and stop according to the test and error code and then stop:\n\n    **Output**:\n    Return only the json without ```json:\n    \n    json =    \n    '}]
2024-11-29 14:36:35.076 | INFO     | metagpt.utils.token_counter:count_input_tokens:385 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:35.088 | INFO     | metagpt.utils.token_counter:count_output_tokens:472 - Warning: model lund-gpt-4o not found in tiktoken. Using cl100k_base encoding.
2024-11-29 14:36:35.088 | INFO     | metagpt.utils.cost_manager:update_cost:57 - Total running cost: $0.082 | Max budget: $10.000 | Current cost: $0.082, prompt_tokens: 16109, completion_tokens: 108
2024-11-29 14:36:35.088 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateJson], state=-1
2024-11-29 14:37:05.072 | DEBUG    | metagpt.roles.role:_set_state:326 - actions=[GenerateCode], state=-1
